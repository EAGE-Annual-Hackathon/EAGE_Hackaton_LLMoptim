{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset creation for SFT and continued pretraining\n",
    "From EAGE abstracts released for the Annual Hackathon in 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gdown --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!pip install langchain --quiet\n",
    "!pip install langchain_nvidia_ai_endpoints --quiet\n",
    "!pip install pypdf --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the relevant libraries\n",
    "import json\n",
    "import os\n",
    "\n",
    "import tqdm\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "from langchain.text_splitter import  RecursiveCharacterTextSplitter\n",
    "from multiprocessing import Pool\n",
    "from pypdf import PdfReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext cudf.pandas\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pprint import PrettyPrinter\n",
    "pprint = PrettyPrinter(indent=4).pprint\n",
    "# os.environ['NVIDIA_API_KEY'] = \"<YOUR NVIDIA API KEY HERE>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and extract documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !gdown 1HmxAZerbIQfHo3evhys1nGX2lspda1RX -O /workspace/data/documents.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !unzip -o /workspace/data/documents.zip -d /workspace/data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract raw texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('workspace/Norway - Diskos reports.csv', sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Populate .jsonl from extracted .pdf files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"/workspace/local_data/raw\", exist_ok=True)\n",
    "\n",
    "for document in df['filename'].unique():\n",
    "    with open(f\"/workspace/local_data/raw/{document}.jsonl\", \"w\") as f:\n",
    "        for raw in df[df['filename'] == document]['content']:\n",
    "            f.write(json.dumps({\"text\": raw}) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head /workspace/data/raw/.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning raw documents with NeMo Curator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!text_cleaning --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!text_cleaning --input-data-dir /workspace/local_data/raw --output-clean-dir /workspace/local_data/clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --extra-index-url https://pypi.nvidia.com nemo-curator[cuda12x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nemo_curator as nc\n",
    "from nemo_curator.datasets import DocumentDataset\n",
    "from nemo_curator.utils.file_utils import get_all_files_paths_under\n",
    "from nemo_curator.filters import WordCountFilter\n",
    "from nemo_curator.modifiers import UnicodeReformatter\n",
    "\n",
    "files = get_all_files_paths_under(\"/workspace/local_data/clean/\")\n",
    "documents = DocumentDataset.read_json(files, add_filename=True)\n",
    "\n",
    "filter_step = nc.ScoreFilter(\n",
    "                WordCountFilter(min_words=80),\n",
    "                text_field=\"text\",\n",
    "                score_field=\"word_count\",\n",
    "            )\n",
    "\n",
    "filtered_documents = filter_step(documents)\n",
    "\n",
    "cleaner = nc.Modify(UnicodeReformatter())\n",
    "filtered_documents = cleaner(filtered_documents)\n",
    "\n",
    "filtered_documents.to_json(\"/workspace/local_data/curator/\", write_to_filename=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing data for continuous pretraining\n",
    "Creating `*.idx` and `*.bin` files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python /opt/NeMo/scripts/nlp_language_modeling/preprocess_data_for_megatron.py \\\n",
    "    --input /workspace/local_data/clean/* \\\n",
    "    --json-keys text \\\n",
    "    --tokenizer-library sentencepiece \\\n",
    "    --tokenizer-model /workspace/models/Llama-2-7b-chat-hf/tokenizer.model \\\n",
    "    --output-prefix /workspace/local_data/clean/hackathon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -l /workspace/local_data/clean | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing data for LLM tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split text into overlapping chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=3000, chunk_overlap=1500)\n",
    "\n",
    "with open(\"/workspace/local_data/clean/documents.jsonl\", \"r\") as f:\n",
    "    documents = [json.loads(line)[\"text\"] for line in f.readlines()]\n",
    "document_chunks = [text_splitter.split_text(document) for document in documents]\n",
    "document_chunks_flat = [chunk for chunks in document_chunks for chunk in chunks]\n",
    "print(f'{len(document_chunks_flat)} chunks extracted out of {len(documents)} pdf documents')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the paragraph after <INPUT_START> tag, create a very good geoscience-related question and answer pair. Your output should be in a .json format containing the following fields: ['question', 'answer']\n",
    "Restrict the question to the context information provided. The questions should use information from passage, but should not refer to the originating text implicitly (you can not use 'according to', 'based on', and similar).\n",
    "Respond only with .json output, add no other comments. If generating a good question and answer pair is not possible, output <skip> instead.\n",
    "<INPUT_START>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define LLM and prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INSTRUCTION_PROMPT = \"\"\"Given the paragraph after <INPUT_START> tag, create a very good geoscience-related question and answer pair. Your output should be in a .json format containing the following fields: ['question', 'answer']\n",
    "Restrict the question to the context information provided. The questions should use information from passage, but should not refer to the originating text implicitly (you can not use 'according to', 'based on', and similar).\n",
    "Respond only with .json output, add no other comments. If generating a good question and answer pair is not possible, output <skip> instead.\n",
    "<INPUT_START>\"\"\"\n",
    "# CHUNKS_TO_PROCESS = 10\n",
    "CHUNKS_TO_PROCESS = None # means all\n",
    "\n",
    "llm = ChatNVIDIA(\n",
    "    model=\"ai-llama3-70b\",\n",
    "    temperature=0.2,\n",
    "    max_tokens=256\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submit batched requests to the LLM\n",
    "\n",
    "**WARNING! It will take Â±30 min to generate QA paris**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# qa_pairs = await llm.abatch(['\\n'.join([INSTRUCTION_PROMPT, chunk]) for chunk in document_chunks_flat[:CHUNKS_TO_PROCESS]], \n",
    "#                             config={\"max_concurrency\": 10})\n",
    "# qa_pairs = [qa_pair.content for qa_pair in qa_pairs if qa_pair.content != \"<skip>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(qa_pairs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse generated QA pairs in tovalid document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failed_count = 0\n",
    "warning_count = 0\n",
    "with open(\"/workspace/data/clean/documents_sft.jsonl\", \"w\") as f:\n",
    "    for qa_pair in qa_pairs:\n",
    "        # Checking if json is correct\n",
    "        try:\n",
    "            json.loads(qa_pair)\n",
    "        except json.JSONDecodeError:\n",
    "            print(f'Failed to read {qa_pair} as a valid JSON')\n",
    "            failed_count += 1\n",
    "            continue\n",
    "        jsonl_line = qa_pair.replace(\"\\n\", \"\").replace('\"question\":', '\"input\":').replace('\"answer\":', '\"output\":').strip()\n",
    "        json_line_obj = json.loads(jsonl_line)\n",
    "        if isinstance(json_line_obj, list):\n",
    "            print(f'WARNING: {jsonl_line}')\n",
    "            jsonl_line = json.dumps(json_line_obj[0])\n",
    "            warning_count += 1\n",
    "        f.write(jsonl_line + \"\\n\")\n",
    "\n",
    "print('Done')\n",
    "print(f'Failed\\t{failed_count} / {len(qa_pairs)}')\n",
    "print(f'Warnings\\t{warning_count} / {len(qa_pairs)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(jsonl_line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split dataset into train / val / test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "\n",
    "def read_and_split(fname: str, out_dir: str):\n",
    "    # Open the original file\n",
    "    with open(fname, 'r') as original_file:\n",
    "        lines = original_file.readlines()\n",
    "\n",
    "    # Calculate partition sizes\n",
    "    total_lines = len(lines)\n",
    "    test_size = int(total_lines * 0.1)\n",
    "    val_size = int(total_lines * 0.1)\n",
    "    # The rest goes to the train partition\n",
    "\n",
    "    print(f'There are {total_lines}--> {test_size}, {val_size}, {total_lines - test_size - val_size}')\n",
    "    print(f'Iterate over {len(lines)} lines in {fname}')\n",
    "\n",
    "    with open(os.path.join(out_dir, 'data_test.jsonl'), 'w') as test_file, \\\n",
    "         open(os.path.join(out_dir, 'data_val.jsonl'), 'w') as val_file, \\\n",
    "         open(os.path.join(out_dir, 'data_train.jsonl'), 'w') as train_file:\n",
    "\n",
    "        # Iterate over each line in the original file\n",
    "        for i, line in enumerate(lines):\n",
    "            # Parse JSON data (optional, if you need to manipulate the data)\n",
    "            json_data = json.loads(line)\n",
    "\n",
    "            # Convert JSON back to string (if manipulated) or use original line\n",
    "            # json_line = json.dumps(json_data) if 'manipulate' in locals() else line\n",
    "            # json_line = str(json.dumps(json_data))\n",
    "            json_line = line\n",
    "\n",
    "            # Write to appropriate file based on index\n",
    "            if i < test_size:\n",
    "                test_file.write(json_line)\n",
    "            elif i < test_size + val_size:\n",
    "                val_file.write(json_line)\n",
    "            else:\n",
    "                train_file.write(json_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_and_split('/workspace/data/clean/documents_sft.jsonl', '/workspace/data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head /workspace/data/data_train.jsonl"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
