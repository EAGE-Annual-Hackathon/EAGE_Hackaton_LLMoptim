{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset creation for SFT and continued pretraining\n",
    "From EAGE abstracts released for the Annual Hackathon in 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gdown --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# !pip install langchain --quiet\n",
    "# !pip install langchain_nvidia_ai_endpoints --quiet\n",
    "# !pip install pypdf --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jun  3 18:26:16 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.3     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA H100 PCIe               On  | 00000000:01:00.0 Off |                    0 |\n",
      "| N/A   45C    P0              53W / 310W |      3MiB / 81559MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the relevant libraries\n",
    "import json\n",
    "import os\n",
    "\n",
    "import tqdm\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "from langchain.text_splitter import  RecursiveCharacterTextSplitter\n",
    "from multiprocessing import Pool\n",
    "from pypdf import PdfReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pprint import PrettyPrinter\n",
    "pprint = PrettyPrinter(indent=4).pprint\n",
    "# os.environ['NVIDIA_API_KEY'] = \"<YOUR NVIDIA API KEY HERE>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and extract documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !gdown 1HmxAZerbIQfHo3evhys1nGX2lspda1RX -O /workspace/data/documents.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !unzip -o /workspace/data/documents.zip -d /workspace/data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract raw texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "PDF_PATH = \"/workspace/data/Annual 2023_proceedings for Hackathon/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a list of all .pdf filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 879/879 [00:00<00:00, 181920.12it/s]\n"
     ]
    }
   ],
   "source": [
    "pdf_files = []\n",
    "folder_content = os.listdir(PDF_PATH)\n",
    "for file in tqdm.tqdm(folder_content):\n",
    "    if file.endswith(\".pdf\"):\n",
    "        pdf_files.append(os.path.join(PDF_PATH, file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spawn a pool of CPU threads for text parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 6 0 (offset 0)\n",
      "Ignoring wrong pointing object 6 0 (offset 0)\n",
      "Ignoring wrong pointing object 6 0 (offset 0)\n",
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 11 0 (offset 0)\n",
      "Ignoring wrong pointing object 11 0 (offset 0)\n",
      "Ignoring wrong pointing object 11 0 (offset 0)\n",
      "Ignoring wrong pointing object 13 0 (offset 0)\n",
      "Ignoring wrong pointing object 13 0 (offset 0)\n",
      "Ignoring wrong pointing object 13 0 (offset 0)\n",
      "Ignoring wrong pointing object 15 0 (offset 0)\n",
      "Ignoring wrong pointing object 15 0 (offset 0)\n",
      "Ignoring wrong pointing object 15 0 (offset 0)\n",
      "Ignoring wrong pointing object 17 0 (offset 0)\n",
      "Ignoring wrong pointing object 17 0 (offset 0)\n",
      "Ignoring wrong pointing object 24 0 (offset 0)\n",
      "Ignoring wrong pointing object 11 0 (offset 0)\n",
      "Ignoring wrong pointing object 14 0 (offset 0)\n",
      "Ignoring wrong pointing object 35 0 (offset 0)\n",
      "Ignoring wrong pointing object 37 0 (offset 0)\n",
      "Ignoring wrong pointing object 19 0 (offset 0)\n",
      "Ignoring wrong pointing object 19 0 (offset 0)\n",
      "Ignoring wrong pointing object 26 0 (offset 0)\n",
      "Ignoring wrong pointing object 21 0 (offset 0)\n",
      "Ignoring wrong pointing object 21 0 (offset 0)\n",
      "Ignoring wrong pointing object 28 0 (offset 0)\n",
      "Ignoring wrong pointing object 23 0 (offset 0)\n",
      "Ignoring wrong pointing object 30 0 (offset 0)\n",
      "Ignoring wrong pointing object 33 0 (offset 0)\n",
      "Ignoring wrong pointing object 25 0 (offset 0)\n",
      "Ignoring wrong pointing object 32 0 (offset 0)\n",
      "Ignoring wrong pointing object 35 0 (offset 0)\n",
      "Ignoring wrong pointing object 37 0 (offset 0)\n",
      "Ignoring wrong pointing object 34 0 (offset 0)\n",
      "Ignoring wrong pointing object 37 0 (offset 0)\n",
      "Ignoring wrong pointing object 39 0 (offset 0)\n",
      "Ignoring wrong pointing object 38 0 (offset 0)\n",
      "Ignoring wrong pointing object 40 0 (offset 0)\n",
      "Ignoring wrong pointing object 41 0 (offset 0)\n",
      "Ignoring wrong pointing object 40 0 (offset 0)\n",
      "Ignoring wrong pointing object 46 0 (offset 0)\n",
      "Ignoring wrong pointing object 43 0 (offset 0)\n",
      "Ignoring wrong pointing object 42 0 (offset 0)\n",
      "Ignoring wrong pointing object 44 0 (offset 0)\n",
      "Ignoring wrong pointing object 45 0 (offset 0)\n",
      "Ignoring wrong pointing object 46 0 (offset 0)\n",
      "Ignoring wrong pointing object 47 0 (offset 0)\n",
      "Ignoring wrong pointing object 50 0 (offset 0)\n",
      "Ignoring wrong pointing object 49 0 (offset 0)\n",
      "Ignoring wrong pointing object 52 0 (offset 0)\n",
      "Ignoring wrong pointing object 51 0 (offset 0)\n",
      "Ignoring wrong pointing object 54 0 (offset 0)\n",
      "Ignoring wrong pointing object 53 0 (offset 0)\n",
      "Ignoring wrong pointing object 58 0 (offset 0)\n",
      "Ignoring wrong pointing object 55 0 (offset 0)\n",
      "Ignoring wrong pointing object 60 0 (offset 0)\n",
      "Ignoring wrong pointing object 56 0 (offset 0)\n",
      "Ignoring wrong pointing object 62 0 (offset 0)\n",
      "Ignoring wrong pointing object 64 0 (offset 0)\n",
      "Ignoring wrong pointing object 66 0 (offset 0)\n",
      "Ignoring wrong pointing object 68 0 (offset 0)\n",
      "Ignoring wrong pointing object 70 0 (offset 0)\n",
      "Ignoring wrong pointing object 72 0 (offset 0)\n",
      "Ignoring wrong pointing object 76 0 (offset 0)\n",
      "Ignoring wrong pointing object 78 0 (offset 0)\n",
      "Ignoring wrong pointing object 82 0 (offset 0)\n",
      "Ignoring wrong pointing object 84 0 (offset 0)\n",
      "Ignoring wrong pointing object 86 0 (offset 0)\n",
      "Ignoring wrong pointing object 88 0 (offset 0)\n",
      "Ignoring wrong pointing object 91 0 (offset 0)\n",
      "Ignoring wrong pointing object 93 0 (offset 0)\n",
      "Ignoring wrong pointing object 95 0 (offset 0)\n",
      "Ignoring wrong pointing object 97 0 (offset 0)\n",
      "Ignoring wrong pointing object 99 0 (offset 0)\n",
      "Ignoring wrong pointing object 102 0 (offset 0)\n",
      "Ignoring wrong pointing object 104 0 (offset 0)\n",
      "Ignoring wrong pointing object 107 0 (offset 0)\n",
      "Ignoring wrong pointing object 112 0 (offset 0)\n",
      "Ignoring wrong pointing object 114 0 (offset 0)\n",
      "Ignoring wrong pointing object 116 0 (offset 0)\n",
      "Ignoring wrong pointing object 118 0 (offset 0)\n",
      "Ignoring wrong pointing object 122 0 (offset 0)\n",
      "Ignoring wrong pointing object 124 0 (offset 0)\n",
      "Ignoring wrong pointing object 126 0 (offset 0)\n",
      "Ignoring wrong pointing object 128 0 (offset 0)\n",
      "Ignoring wrong pointing object 134 0 (offset 0)\n",
      "Ignoring wrong pointing object 136 0 (offset 0)\n",
      "Ignoring wrong pointing object 138 0 (offset 0)\n",
      "Ignoring wrong pointing object 140 0 (offset 0)\n",
      "Ignoring wrong pointing object 149 0 (offset 0)\n",
      "Ignoring wrong pointing object 6 0 (offset 0)\n",
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 11 0 (offset 0)\n",
      "Ignoring wrong pointing object 13 0 (offset 0)\n",
      "Ignoring wrong pointing object 15 0 (offset 0)\n",
      "Ignoring wrong pointing object 17 0 (offset 0)\n",
      "Ignoring wrong pointing object 19 0 (offset 0)\n",
      "Ignoring wrong pointing object 21 0 (offset 0)\n",
      "Ignoring wrong pointing object 23 0 (offset 0)\n",
      "Ignoring wrong pointing object 36 0 (offset 0)\n",
      "incorrect startxref pointer(3)\n",
      "Multiple definitions in dictionary at byte 0x7a for key /Subtype\n",
      "Multiple definitions in dictionary at byte 0x1af for key /Subtype\n",
      "Multiple definitions in dictionary at byte 0x77e for key /Subtype\n",
      "Multiple definitions in dictionary at byte 0x7a for key /Subtype\n",
      "Multiple definitions in dictionary at byte 0x1af for key /Subtype\n",
      "Multiple definitions in dictionary at byte 0x77e for key /Subtype\n",
      "Ignoring wrong pointing object 6 0 (offset 0)\n",
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 11 0 (offset 0)\n",
      "Ignoring wrong pointing object 13 0 (offset 0)\n",
      "Ignoring wrong pointing object 15 0 (offset 0)\n",
      "Ignoring wrong pointing object 17 0 (offset 0)\n",
      "Ignoring wrong pointing object 19 0 (offset 0)\n",
      "Ignoring wrong pointing object 21 0 (offset 0)\n",
      "Ignoring wrong pointing object 23 0 (offset 0)\n",
      "Ignoring wrong pointing object 25 0 (offset 0)\n",
      "Ignoring wrong pointing object 27 0 (offset 0)\n",
      "Ignoring wrong pointing object 36 0 (offset 0)\n",
      "Ignoring wrong pointing object 38 0 (offset 0)\n",
      "Ignoring wrong pointing object 40 0 (offset 0)\n",
      "Ignoring wrong pointing object 43 0 (offset 0)\n",
      "Ignoring wrong pointing object 49 0 (offset 0)\n",
      "Ignoring wrong pointing object 51 0 (offset 0)\n",
      "Ignoring wrong pointing object 54 0 (offset 0)\n",
      "Ignoring wrong pointing object 66 0 (offset 0)\n",
      "Ignoring wrong pointing object 68 0 (offset 0)\n",
      "Ignoring wrong pointing object 73 0 (offset 0)\n",
      "Ignoring wrong pointing object 81 0 (offset 0)\n",
      "Ignoring wrong pointing object 83 0 (offset 0)\n",
      "Multiple definitions in dictionary at byte 0x7a for key /Subtype\n",
      "Multiple definitions in dictionary at byte 0x1af for key /Subtype\n",
      "Multiple definitions in dictionary at byte 0x77e for key /Subtype\n",
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 11 0 (offset 0)\n",
      "Ignoring wrong pointing object 13 0 (offset 0)\n",
      "Ignoring wrong pointing object 15 0 (offset 0)\n",
      "Ignoring wrong pointing object 17 0 (offset 0)\n",
      "Ignoring wrong pointing object 33 0 (offset 0)\n",
      "Ignoring wrong pointing object 35 0 (offset 0)\n",
      "Ignoring wrong pointing object 37 0 (offset 0)\n",
      "Ignoring wrong pointing object 39 0 (offset 0)\n",
      "Ignoring wrong pointing object 43 0 (offset 0)\n",
      "Ignoring wrong pointing object 45 0 (offset 0)\n",
      "Ignoring wrong pointing object 47 0 (offset 0)\n",
      "Ignoring wrong pointing object 51 0 (offset 0)\n",
      "Ignoring wrong pointing object 53 0 (offset 0)\n",
      "Ignoring wrong pointing object 55 0 (offset 0)\n",
      "Ignoring wrong pointing object 59 0 (offset 0)\n",
      "Ignoring wrong pointing object 61 0 (offset 0)\n",
      "Ignoring wrong pointing object 63 0 (offset 0)\n",
      "Multiple definitions in dictionary at byte 0x7a for key /Subtype\n",
      "Multiple definitions in dictionary at byte 0x1af for key /Subtype\n",
      "Multiple definitions in dictionary at byte 0x77e for key /Subtype\n",
      "Ignoring wrong pointing object 6 0 (offset 0)\n",
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 11 0 (offset 0)\n",
      "Ignoring wrong pointing object 13 0 (offset 0)\n",
      "Ignoring wrong pointing object 15 0 (offset 0)\n",
      "Ignoring wrong pointing object 17 0 (offset 0)\n",
      "Ignoring wrong pointing object 19 0 (offset 0)\n",
      "Ignoring wrong pointing object 31 0 (offset 0)\n",
      "Ignoring wrong pointing object 45 0 (offset 0)\n",
      "Ignoring wrong pointing object 6 0 (offset 0)\n",
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 11 0 (offset 0)\n",
      "Ignoring wrong pointing object 13 0 (offset 0)\n",
      "Ignoring wrong pointing object 15 0 (offset 0)\n",
      "Ignoring wrong pointing object 17 0 (offset 0)\n",
      "Ignoring wrong pointing object 26 0 (offset 0)\n",
      "Ignoring wrong pointing object 28 0 (offset 0)\n",
      "Ignoring wrong pointing object 30 0 (offset 0)\n",
      "Ignoring wrong pointing object 32 0 (offset 0)\n",
      "Ignoring wrong pointing object 6 0 (offset 0)\n",
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 11 0 (offset 0)\n",
      "Ignoring wrong pointing object 13 0 (offset 0)\n",
      "Ignoring wrong pointing object 15 0 (offset 0)\n",
      "Ignoring wrong pointing object 17 0 (offset 0)\n",
      "Ignoring wrong pointing object 19 0 (offset 0)\n",
      "Ignoring wrong pointing object 21 0 (offset 0)\n",
      "Ignoring wrong pointing object 29 0 (offset 0)\n",
      "Ignoring wrong pointing object 6 0 (offset 0)\n",
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 11 0 (offset 0)\n",
      "Ignoring wrong pointing object 13 0 (offset 0)\n",
      "Ignoring wrong pointing object 15 0 (offset 0)\n",
      "Ignoring wrong pointing object 17 0 (offset 0)\n",
      "Ignoring wrong pointing object 19 0 (offset 0)\n",
      "Ignoring wrong pointing object 28 0 (offset 0)\n",
      "Ignoring wrong pointing object 38 0 (offset 0)\n",
      "Ignoring wrong pointing object 40 0 (offset 0)\n",
      "Ignoring wrong pointing object 57 0 (offset 0)\n",
      "Ignoring wrong pointing object 59 0 (offset 0)\n",
      "Ignoring wrong pointing object 61 0 (offset 0)\n",
      "Ignoring wrong pointing object 63 0 (offset 0)\n",
      "Ignoring wrong pointing object 9 0 (offset 0)\n",
      "Ignoring wrong pointing object 15 0 (offset 0)\n",
      "Ignoring wrong pointing object 19 0 (offset 0)\n",
      "Ignoring wrong pointing object 6 0 (offset 0)\n",
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 11 0 (offset 0)\n",
      "Ignoring wrong pointing object 13 0 (offset 0)\n",
      "Ignoring wrong pointing object 15 0 (offset 0)\n",
      "Ignoring wrong pointing object 17 0 (offset 0)\n",
      "Ignoring wrong pointing object 19 0 (offset 0)\n",
      "Ignoring wrong pointing object 21 0 (offset 0)\n",
      "Ignoring wrong pointing object 30 0 (offset 0)\n",
      "Ignoring wrong pointing object 32 0 (offset 0)\n",
      "Ignoring wrong pointing object 37 0 (offset 0)\n",
      "Ignoring wrong pointing object 44 0 (offset 0)\n",
      "Ignoring wrong pointing object 46 0 (offset 0)\n",
      "Ignoring wrong pointing object 48 0 (offset 0)\n",
      "incorrect startxref pointer(3)\n",
      "Ignoring wrong pointing object 6 0 (offset 0)\n",
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 11 0 (offset 0)\n",
      "Ignoring wrong pointing object 13 0 (offset 0)\n",
      "Ignoring wrong pointing object 15 0 (offset 0)\n",
      "Ignoring wrong pointing object 24 0 (offset 0)\n",
      "Ignoring wrong pointing object 32 0 (offset 0)\n",
      "Ignoring wrong pointing object 34 0 (offset 0)\n",
      "Ignoring wrong pointing object 36 0 (offset 0)\n",
      "incorrect startxref pointer(3)\n",
      "Ignoring wrong pointing object 6 0 (offset 0)\n",
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 11 0 (offset 0)\n",
      "Ignoring wrong pointing object 13 0 (offset 0)\n",
      "Ignoring wrong pointing object 21 0 (offset 0)\n",
      "Ignoring wrong pointing object 24 0 (offset 0)\n",
      "Ignoring wrong pointing object 38 0 (offset 0)\n",
      "Ignoring wrong pointing object 40 0 (offset 0)\n",
      "Ignoring wrong pointing object 6 0 (offset 0)\n",
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 11 0 (offset 0)\n",
      "Ignoring wrong pointing object 13 0 (offset 0)\n",
      "Ignoring wrong pointing object 15 0 (offset 0)\n",
      "Ignoring wrong pointing object 17 0 (offset 0)\n",
      "Ignoring wrong pointing object 19 0 (offset 0)\n",
      "Ignoring wrong pointing object 6 0 (offset 0)\n",
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 11 0 (offset 0)\n",
      "Ignoring wrong pointing object 13 0 (offset 0)\n",
      "Ignoring wrong pointing object 15 0 (offset 0)\n",
      "Ignoring wrong pointing object 17 0 (offset 0)\n",
      "Ignoring wrong pointing object 26 0 (offset 0)\n",
      "Ignoring wrong pointing object 34 0 (offset 0)\n",
      "Ignoring wrong pointing object 6 0 (offset 0)\n",
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 11 0 (offset 0)\n",
      "Ignoring wrong pointing object 13 0 (offset 0)\n",
      "Ignoring wrong pointing object 15 0 (offset 0)\n",
      "Ignoring wrong pointing object 17 0 (offset 0)\n",
      "Ignoring wrong pointing object 19 0 (offset 0)\n",
      "Ignoring wrong pointing object 21 0 (offset 0)\n",
      "Ignoring wrong pointing object 23 0 (offset 0)\n",
      "Ignoring wrong pointing object 25 0 (offset 0)\n",
      "Ignoring wrong pointing object 35 0 (offset 0)\n",
      "Ignoring wrong pointing object 37 0 (offset 0)\n",
      "Ignoring wrong pointing object 39 0 (offset 0)\n",
      "Ignoring wrong pointing object 41 0 (offset 0)\n",
      "Ignoring wrong pointing object 54 0 (offset 0)\n",
      "Ignoring wrong pointing object 6 0 (offset 0)\n",
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 11 0 (offset 0)\n",
      "Ignoring wrong pointing object 13 0 (offset 0)\n",
      "Ignoring wrong pointing object 15 0 (offset 0)\n",
      "Ignoring wrong pointing object 17 0 (offset 0)\n",
      "Ignoring wrong pointing object 26 0 (offset 0)\n",
      "Ignoring wrong pointing object 31 0 (offset 0)\n",
      "Ignoring wrong pointing object 33 0 (offset 0)\n",
      "Ignoring wrong pointing object 6 0 (offset 0)\n",
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 11 0 (offset 0)\n",
      "Ignoring wrong pointing object 13 0 (offset 0)\n",
      "Ignoring wrong pointing object 15 0 (offset 0)\n",
      "Ignoring wrong pointing object 17 0 (offset 0)\n",
      "Ignoring wrong pointing object 19 0 (offset 0)\n",
      "Ignoring wrong pointing object 28 0 (offset 0)\n",
      "Ignoring wrong pointing object 30 0 (offset 0)\n",
      "Ignoring wrong pointing object 33 0 (offset 0)\n",
      "Ignoring wrong pointing object 35 0 (offset 0)\n",
      "Ignoring wrong pointing object 37 0 (offset 0)\n",
      "Ignoring wrong pointing object 44 0 (offset 0)\n",
      "Ignoring wrong pointing object 18 0 (offset 0)\n",
      "Ignoring wrong pointing object 49 0 (offset 0)\n",
      "Ignoring wrong pointing object 20 0 (offset 0)\n",
      "Ignoring wrong pointing object 22 0 (offset 0)\n",
      "Ignoring wrong pointing object 6 0 (offset 0)\n",
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 11 0 (offset 0)\n",
      "Ignoring wrong pointing object 13 0 (offset 0)\n",
      "Ignoring wrong pointing object 15 0 (offset 0)\n",
      "Ignoring wrong pointing object 25 0 (offset 0)\n",
      "Ignoring wrong pointing object 27 0 (offset 0)\n",
      "Ignoring wrong pointing object 38 0 (offset 0)\n",
      "Ignoring wrong pointing object 6 0 (offset 0)\n",
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 11 0 (offset 0)\n",
      "Ignoring wrong pointing object 13 0 (offset 0)\n",
      "Ignoring wrong pointing object 15 0 (offset 0)\n",
      "Ignoring wrong pointing object 17 0 (offset 0)\n",
      "Ignoring wrong pointing object 19 0 (offset 0)\n",
      "Ignoring wrong pointing object 28 0 (offset 0)\n",
      "Ignoring wrong pointing object 30 0 (offset 0)\n",
      "Ignoring wrong pointing object 33 0 (offset 0)\n",
      "Ignoring wrong pointing object 36 0 (offset 0)\n",
      "Ignoring wrong pointing object 38 0 (offset 0)\n",
      "Ignoring wrong pointing object 43 0 (offset 0)\n",
      "Ignoring wrong pointing object 45 0 (offset 0)\n",
      "Ignoring wrong pointing object 47 0 (offset 0)\n",
      "Ignoring wrong pointing object 50 0 (offset 0)\n",
      "Ignoring wrong pointing object 56 0 (offset 0)\n",
      "Ignoring wrong pointing object 59 0 (offset 0)\n",
      "Ignoring wrong pointing object 61 0 (offset 0)\n",
      "Ignoring wrong pointing object 6 0 (offset 0)\n",
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 11 0 (offset 0)\n",
      "Ignoring wrong pointing object 13 0 (offset 0)\n",
      "Ignoring wrong pointing object 15 0 (offset 0)\n",
      "Ignoring wrong pointing object 24 0 (offset 0)\n",
      "Ignoring wrong pointing object 26 0 (offset 0)\n",
      "Ignoring wrong pointing object 37 0 (offset 0)\n",
      "Ignoring wrong pointing object 39 0 (offset 0)\n",
      "Ignoring wrong pointing object 41 0 (offset 0)\n",
      "Ignoring wrong pointing object 43 0 (offset 0)\n",
      "Ignoring wrong pointing object 45 0 (offset 0)\n",
      "Ignoring wrong pointing object 6 0 (offset 0)\n",
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 11 0 (offset 0)\n",
      "Ignoring wrong pointing object 13 0 (offset 0)\n",
      "Ignoring wrong pointing object 15 0 (offset 0)\n",
      "Ignoring wrong pointing object 17 0 (offset 0)\n",
      "Ignoring wrong pointing object 19 0 (offset 0)\n",
      "Ignoring wrong pointing object 21 0 (offset 0)\n",
      "Ignoring wrong pointing object 23 0 (offset 0)\n",
      "Ignoring wrong pointing object 25 0 (offset 0)\n",
      "Ignoring wrong pointing object 36 0 (offset 0)\n",
      "Ignoring wrong pointing object 38 0 (offset 0)\n",
      "Ignoring wrong pointing object 40 0 (offset 0)\n",
      "Ignoring wrong pointing object 42 0 (offset 0)\n",
      "Ignoring wrong pointing object 46 0 (offset 0)\n",
      "Ignoring wrong pointing object 48 0 (offset 0)\n",
      "Ignoring wrong pointing object 52 0 (offset 0)\n",
      "Ignoring wrong pointing object 54 0 (offset 0)\n",
      "Ignoring wrong pointing object 56 0 (offset 0)\n",
      "Ignoring wrong pointing object 65 0 (offset 0)\n",
      "Ignoring wrong pointing object 6 0 (offset 0)\n",
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 11 0 (offset 0)\n",
      "Ignoring wrong pointing object 13 0 (offset 0)\n",
      "Ignoring wrong pointing object 20 0 (offset 0)\n",
      "Ignoring wrong pointing object 25 0 (offset 0)\n",
      "Ignoring wrong pointing object 27 0 (offset 0)\n",
      "Ignoring wrong pointing object 6 0 (offset 0)\n",
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 11 0 (offset 0)\n",
      "Ignoring wrong pointing object 13 0 (offset 0)\n",
      "Ignoring wrong pointing object 15 0 (offset 0)\n",
      "Ignoring wrong pointing object 17 0 (offset 0)\n",
      "Ignoring wrong pointing object 19 0 (offset 0)\n",
      "Ignoring wrong pointing object 6 0 (offset 0)\n",
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 11 0 (offset 0)\n",
      "Ignoring wrong pointing object 13 0 (offset 0)\n",
      "Ignoring wrong pointing object 15 0 (offset 0)\n",
      "Ignoring wrong pointing object 24 0 (offset 0)\n",
      "Ignoring wrong pointing object 38 0 (offset 0)\n",
      "Ignoring wrong pointing object 6 0 (offset 0)\n",
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 11 0 (offset 0)\n",
      "Ignoring wrong pointing object 13 0 (offset 0)\n",
      "Ignoring wrong pointing object 15 0 (offset 0)\n",
      "Ignoring wrong pointing object 17 0 (offset 0)\n",
      "Ignoring wrong pointing object 26 0 (offset 0)\n",
      "Ignoring wrong pointing object 28 0 (offset 0)\n",
      "Ignoring wrong pointing object 30 0 (offset 0)\n",
      "Ignoring wrong pointing object 34 0 (offset 0)\n",
      "Ignoring wrong pointing object 36 0 (offset 0)\n",
      "Ignoring wrong pointing object 39 0 (offset 0)\n",
      "Ignoring wrong pointing object 6 0 (offset 0)\n",
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 11 0 (offset 0)\n",
      "Ignoring wrong pointing object 13 0 (offset 0)\n",
      "Ignoring wrong pointing object 15 0 (offset 0)\n",
      "Ignoring wrong pointing object 17 0 (offset 0)\n",
      "Ignoring wrong pointing object 19 0 (offset 0)\n",
      "Ignoring wrong pointing object 29 0 (offset 0)\n",
      "Ignoring wrong pointing object 31 0 (offset 0)\n",
      "Ignoring wrong pointing object 33 0 (offset 0)\n",
      "Ignoring wrong pointing object 51 0 (offset 0)\n",
      "Ignoring wrong pointing object 54 0 (offset 0)\n",
      "Ignoring wrong pointing object 58 0 (offset 0)\n",
      "Ignoring wrong pointing object 61 0 (offset 0)\n",
      "Ignoring wrong pointing object 63 0 (offset 0)\n",
      "Ignoring wrong pointing object 67 0 (offset 0)\n",
      "Ignoring wrong pointing object 75 0 (offset 0)\n",
      "Ignoring wrong pointing object 77 0 (offset 0)\n",
      "Ignoring wrong pointing object 79 0 (offset 0)\n",
      "Ignoring wrong pointing object 89 0 (offset 0)\n",
      "Ignoring wrong pointing object 145 0 (offset 0)\n",
      "Ignoring wrong pointing object 12 0 (offset 0)\n",
      "Ignoring wrong pointing object 14 0 (offset 0)\n",
      "Ignoring wrong pointing object 34 0 (offset 0)\n",
      "Ignoring wrong pointing object 6 0 (offset 0)\n",
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 11 0 (offset 0)\n",
      "Ignoring wrong pointing object 13 0 (offset 0)\n",
      "Ignoring wrong pointing object 22 0 (offset 0)\n",
      "Ignoring wrong pointing object 24 0 (offset 0)\n",
      "Ignoring wrong pointing object 26 0 (offset 0)\n",
      "Ignoring wrong pointing object 28 0 (offset 0)\n",
      "Ignoring wrong pointing object 30 0 (offset 0)\n",
      "Ignoring wrong pointing object 32 0 (offset 0)\n",
      "Ignoring wrong pointing object 34 0 (offset 0)\n",
      "Ignoring wrong pointing object 36 0 (offset 0)\n",
      "Ignoring wrong pointing object 38 0 (offset 0)\n",
      "Ignoring wrong pointing object 40 0 (offset 0)\n",
      "Ignoring wrong pointing object 42 0 (offset 0)\n",
      "Ignoring wrong pointing object 44 0 (offset 0)\n",
      "Ignoring wrong pointing object 46 0 (offset 0)\n",
      "Ignoring wrong pointing object 48 0 (offset 0)\n",
      "Ignoring wrong pointing object 50 0 (offset 0)\n",
      "Ignoring wrong pointing object 56 0 (offset 0)\n",
      "Ignoring wrong pointing object 58 0 (offset 0)\n",
      "Ignoring wrong pointing object 60 0 (offset 0)\n",
      "Ignoring wrong pointing object 62 0 (offset 0)\n",
      "Ignoring wrong pointing object 64 0 (offset 0)\n",
      "Ignoring wrong pointing object 66 0 (offset 0)\n",
      "Ignoring wrong pointing object 68 0 (offset 0)\n",
      "Ignoring wrong pointing object 6 0 (offset 0)\n",
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 11 0 (offset 0)\n",
      "Ignoring wrong pointing object 13 0 (offset 0)\n",
      "Ignoring wrong pointing object 15 0 (offset 0)\n",
      "Ignoring wrong pointing object 24 0 (offset 0)\n",
      "Ignoring wrong pointing object 26 0 (offset 0)\n",
      "Ignoring wrong pointing object 31 0 (offset 0)\n",
      "Ignoring wrong pointing object 37 0 (offset 0)\n",
      "Ignoring wrong pointing object 49 0 (offset 0)\n",
      "Ignoring wrong pointing object 79 0 (offset 0)\n",
      "Ignoring wrong pointing object 102 0 (offset 0)\n",
      "Ignoring wrong pointing object 173 0 (offset 0)\n",
      "Ignoring wrong pointing object 181 0 (offset 0)\n",
      "Ignoring wrong pointing object 184 0 (offset 0)\n",
      "Ignoring wrong pointing object 197 0 (offset 0)\n",
      "Ignoring wrong pointing object 224 0 (offset 0)\n",
      "Ignoring wrong pointing object 6 0 (offset 0)\n",
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 11 0 (offset 0)\n",
      "Ignoring wrong pointing object 13 0 (offset 0)\n",
      "Ignoring wrong pointing object 15 0 (offset 0)\n",
      "Ignoring wrong pointing object 17 0 (offset 0)\n",
      "Ignoring wrong pointing object 19 0 (offset 0)\n",
      "Ignoring wrong pointing object 21 0 (offset 0)\n",
      "Ignoring wrong pointing object 23 0 (offset 0)\n",
      "Ignoring wrong pointing object 21 0 (offset 0)\n",
      "Ignoring wrong pointing object 23 0 (offset 0)\n",
      "Ignoring wrong pointing object 26 0 (offset 0)\n",
      "Ignoring wrong pointing object 6 0 (offset 0)\n",
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 11 0 (offset 0)\n",
      "Ignoring wrong pointing object 13 0 (offset 0)\n",
      "Ignoring wrong pointing object 22 0 (offset 0)\n",
      "Ignoring wrong pointing object 24 0 (offset 0)\n",
      "Ignoring wrong pointing object 26 0 (offset 0)\n",
      "Ignoring wrong pointing object 31 0 (offset 0)\n",
      "Ignoring wrong pointing object 33 0 (offset 0)\n",
      "Ignoring wrong pointing object 38 0 (offset 0)\n",
      "Ignoring wrong pointing object 51 0 (offset 0)\n",
      "Ignoring wrong pointing object 53 0 (offset 0)\n",
      "Ignoring wrong pointing object 55 0 (offset 0)\n",
      "Ignoring wrong pointing object 59 0 (offset 0)\n",
      "Ignoring wrong pointing object 61 0 (offset 0)\n",
      "Ignoring wrong pointing object 67 0 (offset 0)\n",
      "Ignoring wrong pointing object 69 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populate .jsonl from extracted .pdf files\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/workspace/data/raw/documents.jsonl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m     documents \u001b[38;5;241m=\u001b[39m pool\u001b[38;5;241m.\u001b[39mmap(extract_text_from_pdf, pdf_files)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPopulate .jsonl from extracted .pdf files\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/workspace/data/raw/documents.jsonl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mw\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m document \u001b[38;5;129;01min\u001b[39;00m tqdm\u001b[38;5;241m.\u001b[39mtqdm(documents):\n\u001b[1;32m     13\u001b[0m         f\u001b[38;5;241m.\u001b[39mwrite(json\u001b[38;5;241m.\u001b[39mdumps({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m: document}) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py:310\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    308\u001b[0m     )\n\u001b[0;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/workspace/data/raw/documents.jsonl'"
     ]
    }
   ],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    texts = []\n",
    "    reader = PdfReader(pdf_path)\n",
    "    texts = [page.extract_text() for page in reader.pages]\n",
    "    return \"\\n\".join(texts) \n",
    "\n",
    "with Pool(64) as pool:\n",
    "    documents = pool.map(extract_text_from_pdf, pdf_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Populate .jsonl from extracted .pdf files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 879/879 [00:00<00:00, 6101.15it/s]\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(\"/workspace/data/raw\", exist_ok=True)\n",
    "with open(\"/workspace/data/raw/documents.jsonl\", \"w\") as f:\n",
    "    for document in tqdm.tqdm(documents):\n",
    "        document = document.strip().replace(\"84th EAGE Annual Conference & Exhibition  \", \"\")\n",
    "        f.write(json.dumps({\"text\": document}) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"text\": \"Multisensor UAS testing to support avalanche forecasting and monitoring  \\n \\nIntroduction  \\n Snow avalanches pose a major natural hazard in Arctic and Alpine countries. Transport networks in \\nNorway are especially vulnerable and for this reason, the GEOSFAIR (GEO hazard Survey F rom the \\nAIR) research project is investigating how uncrewed aerial systems (UAS) can help to forecast and \\nmonitor roadside avalanches and other natural hazards.  The main objective of the project is  to develop \\nmethodologies and workflows  for UAS data collect ion in the Norwegian Public Road s Administration  \\n(NPRA)  decision support system , for faster and better assessment of roadside  avalanche hazard s.  \\n \\nRemote sensing provides good spatial coverage to improve avalanche monitoring, from ground, air or \\nspace and using optical, laser or radar sensors (Eckerstorfer  et al., 2016). UAS  remote sensing and \\nsurface mapping has been  used for improving avalanche forecasting using either LiDAR (Light \\nDetection and Ranging)  mapping techniques (Chrustek and Wezyk, 2009) or SfM ( Structure -from -\\nMotion) from optical cameras  (Adams et al., 2018 ), or both (Harder et al., 2020) . Other UAS -based \\nremote sensors that are being tested includ e multispectral cameras  (Maier et al., 2022)  and thermal \\ncameras (Steinkogler et al., 2015) . Subsurface mapping using ground penetrating radar  (GPR)  mounted \\non drones has been given less attention so far, and  outcomes can be ambiguous  (Jenssen  et al., 2019, \\nValence et al., 2022 ).   \\n \\nWithin the aforementioned GEOSFAIR project, several UAS sensor s are being evaluated , including \\nLiDAR, optical, and GPR sensors . This abstract reports on an extensive field test  that was carried out \\nin Grasdalen, Norway, in April 2022. W e present the results of the test , which utilized data from several \\nUAS sensor s, and which were validated against ground truth data.  \\n Methods  \\n \\nThe field test was conducted at NGI s avalanche research station Fonnbu at Stryn efjellet (Vestland \\ncounty, Norway). The area is well instrumented and has been used for avalanche research since 1973 . \\nThe terrain around the station  is steep and varied , reaching from 875 to 1600 m  asl. The area is \\ncharacterized by a  coastal climate allowing for large amount s of precipitation and large influence of \\nsnow wind transport  on avalanche risk . The surveys last ed for five days, and the weather was mostly \\ncalm , with  good visibility allowing for safe drone flights. Air temperatures  at the research station (938 \\nm asl) ranged from -16.5 to +3.4\\u00b0C  during the test . A short precipitation  event occurred o n the first day \\nwith a little snow  (3-4 cm).   \\n Fifteen  snow observations  were recorded  in snow  pits at different locations over the week  to correlate \\ndrone measurements with ground truth data. Snow  height above terrain varie d from 2.5 m at Fonnbu \\nlocation to 12- 15 m height  in cornice  areas.  In addition, nine  ground  control points (GCPs) were placed \\nand georeferenced in order to calibrate and validate drone surface measurements. Different m ultirotor  \\nand VTOL  (Vertical Take- Off and Landing)  drones were  tested carrying various paylo ad sensors that \\ncollect ed optical (RGB and multispectral) images , thermal (radiometric infrared ) images, \\nphotogrammetry models , LiDAR  point clouds  and GPR  profiles . \\n Results  \\n LiDAR surveys were carried out with a DJI Zenmuse  L1 sensor mounted on a DJI Matrice 300 RTK \\ndrone. The data sets were recorded by flying at 60 m above ground level  altitude, with a terrain -\\nfollowing function, 7 m/s flight speed, 50 % sideways overlap, and dual return mode. A repetitive scan \\npattern was used, and the sampling rate varie d between 240 000 and 120 000 pts/s. The LiDAR data \\nwas pre-processed in the DJI Terra software and additional editing, and va lidation of the point clouds \\nwas completed in CloudCompare and ArcGIS Pro software.  \\nNumerous photogrammetry surveys were also carried out using DJI Mavic 2 Pro, DJI Phantom 4 RTK, \\nVTOL Quantum Systems Trinity F90+ with Sony RX1RII camera and DJI Matrice 300 RTK with P1  \\n \\n \\ncamera. Testing under different light conditions  show ed that the SfM reconstructions varied in quality \\nand were  more difficult when  light conditions were extremely bright and with fresh snow (uniform, \\nlittle texture) . Two major types of survey missions were conducted, including altitude -locked, double -\\ngridline missions and terrain- following (altitude varying) missions. Examples of d igital elevation \\nmodels derived from LiDAR and photogrammetry reconstruction techniques are displayed in Figure 1. \\nSnow height  values  were derived by subtracting  the national airborne LiDAR  terrain model (with 1  m \\nresolution ) from the snow surface  models . \\n \\n \\nFigure 1  Examples of LiDAR ( top) and photogrammetry ( bottom ) results from the test . The LiDAR \\nresult  shows the snow height  (Nor. \\\"sn\\u00f8dy bde\\\")  draped on hillshaded snow surface. The \\nphotogrammetry result  shows an excerpt of snow height in a  roadside  avalanche release area.  \\n\\n \\n \\nSeveral drone GPR measurements were also carried out, to test the ability to fly in steep mountainous \\nterrain and to determine the best flight altitude. The GPR system was a light pulse shielded antenna \\nZond Aero from Radsys with a central frequency of 1 GHz. The GPR was mounted on a DJI Matrice \\n300 RTK, combined with a radar altimeter and terrain- following algorithm to allow flights close to the \\nterrain. Figure 2 (top) shows an example of a GPR profile recorded along a steep slope. Tests at flight \\naltitude s between 1.5 and 5 m showed that the best compromise between data quality and flight safety \\nwas between 2 and 3 m. The GPR data profile enabled identification of the snow surface and the \\ninterface between snow and ground. This interface was not linear as the ground surface was composed \\nof rock boulders and strong hyperbola diffractions were visible. Internal snow layers were also visible and can be correlated to our own snow pit measurements.  \\n \\nFigure 2 Examples of r esults of  drone GPR survey (top), handheld thermal camera imaging of snow \\npits (bottom left) and drone multispectral survey (bottom right).  \\n \\nAdditional sensors were tested, including thermal and multispectral imaging cameras. Figure 2 (bottom \\nleft) display s results o f testing a handhel d thermal camera (Fluke Ti -400 series) to measure snow surface \\ntemperatures. Results correlated well with temperatures measured using a conventional thermometer . \\nSimilar  airborne  measurements were carried out using a DJI Matric 300 RTK and H20T thermal camera  \\npayload. Another set of data was recorded using  a multispectral camera ( Micasense  RedEdge MX) both \\nhandheld and from a DJI Matrice 300 RTK . Captured imagery was processed with Pix4DFields . Figure \\n2 (bottom right ) depicts  a visualization of the MCARI index, which is a standard metric  of chlorophyll \\nabsorption used for agricultural purposes  and showed here for illustration. The 10 different channels of \\nthe multispectral cameras (444 to 842 nm cent re wavelength) should, in theory, enable  discrimination \\nof different  snow grain types at the surface  of the snow pack . \\n\\n \\n \\n \\nConclusion  \\n We present a summary of an extensive field test  that had the overall goal to test different UAS  and \\nsensor payload s and to evaluate  the usefulness of derived data for  support ing the NPRA in operational \\ndecisions related to snow avalanche hazard . The  tested  technologies  provide d information on snow \\nsurface  characteristics , snow heights and snow pack properties. Integration of the different sensors for \\nspecific measurements and avalanche problems is part of on- going work.  \\n \\nAcknowledgement  \\n \\nThis work is part of the GEOSFAIR project funded by the NPRA and the Research Council of Norway \\n(Innovation Project for the Public Sector, grant no. 321035) , with in -kind contributions from NGI and \\nSINTEF.  \\n References  \\n Adams, M.S., B\\u00fchler, Y. and Fromm, R., 2018. Multitemporal accuracy and precision assessment of  \\nunmanned aerial system photogrammetry for slope -scale snow depth maps in Alpine  \\nterrain.  Pure and Applied Geophysics , 175(9), pp.3303- 3324.  \\nChrustek, P., Wezyk, P., 2009. Using high resolution LiDAR data to estimate potential avalanche  \\nrelease areas on the example of Polish mountain regions. Proc. International Snow Science  \\nWorkshop, 2009, Davos, Switzerland.  \\nEckerstorfer, M., B\\u00fchler, Y., Frauenfelder, R. and Malnes, E., 2016. Remote sensing of snow  \\navalanches: Recent advances, potential, and limitations.  Cold Regions Science and \\nTechnology , 121, pp.126- 140. \\nHarder, P., et al. 2020. Improving sub- canopy snow depth mapping with unmanned aerial vehicles:  \\nLiDAR  versus structure- from -motion techniques. The Cryosphere, 14(6): 1919\\u20131935  \\nMaier, K., Nascetti, A., Van Pelt, W. and Rosqvist, G., 2022. Direct photogrammetry with  \\nmultispectral imagery for UAV -based snow depth estimation.  ISPRS Journal of  \\nPhotogrammetry and Remote Sensing, 186, pp.1- 18. \\nJenssen, R.O.R., Eckerstorfer, M. and Jacobsen, S., 2019. Drone -mounted ultrawideband radar for  \\nretrieval of snowpack properties. IEEE Transactions on Instrumentation and  \\nMeasurement , 69(1), pp.221- 230. \\nSteinkogler, W.; Sovilla, B. & Lehning, M., 2015. Thermal energy in dry snow avalanches. The  \\nCryosphere, 9, pp.1819- 1830 \\nValence, E., Baraer, M., Rosa, E., Barbecot, F. and Monty, C., 2022. Drone -based ground- penetrating  \\nradar (GPR) application to snow hydrology. The Cryosphere, 16(9), pp.3843- 3860.\"}\n",
      "{\"text\": \"\\nUsing full waveform inversion with limited -offset seismic data to improve pre -salt imaging in \\nthe Kwanza basin  \\n \\nIntroduction  \\n \\nTypically, seismic imaging in a complex salt environment requires rich azimuthal coverage and long \\noffsets to illuminate the subsurface, an accurate velocity model and imaging algorithm (Kang et al., \\n2021 ). Moreover, g eometry of the salt , in offshore Angola , has variable complexity, and the majority \\nof the data acquired to date is narrow -azimuth (NAZ) towed stream er.  Based on previous s tudies  \\nperformed in this area , reliable subsalt images can be achieved with NAZ data, but an accurate velocity \\nmodel is critical to success  (Shmelev et al., 2016 ; Branston et al., 2022). Herein we demonstrate the \\nrejuvenation  of multi -survey NAZ  seismic data by applying broadband signal processing and focusing \\non velocity model building and depth imaging, using new developments in  full-waveform inversion \\n(FWI) such as enhanced template -matching (ETM -FWI) automation. It resulte d in the reduction of  \\nstructural uncertainties , and improved  understanding  of the depositional context at pre -salt targets , \\nmigration pathways  and trapping styles.  We show how improvements in model building contribute to \\na significantly improved image of t he legacy seismic data . \\nMore than 4.2 billion barrels of oil equivalent  has been discovered , such as Cameia, Mavinga, Orca , in \\npre-salt play  over the outer Kwanza basin covering blocks 20 and 21 (Figure 1). Carbonate mounds \\n(Aptian Microbialites)  which are developed on the basement highs  overlain by salt  are the key targets \\nin the deep -water . Most of the exploration wells drilled so far had seismic imaging as one of the key \\ndeterrents in successfully mapping pre -salt carbonate reserv oirs and predicting accurate reservoir \\nproperties.  Distortion and poor focussing of the seismic image associated with unresolved overburden \\nheterogeneity complicates the understanding of deeper structure and fracture delineation.  \\nFigure 1 Regional seismic cross -section showing hydrocarbon discoveries in Kwanza basin  \\nThe combination of uncertainty in the velocity model and strong contrasts associated with salt and \\ncarbonates make it challenging for FWI to converge when classic objec tive function s are used.  Together \\nwith the acquisition geometry limitations, we identified the challenges in the target area with input data \\ncomprising three neighbouring surveys with limited overlap. Two surveys were acquired with 4800 m \\nstreamers towed at 7  m depth with sources towed at 5 m depth . The third orthogonal survey was acquired \\nwith a maximum offset of 8000 m, an 8 m cable depth and source at 6  m depth . A spatially variant high -\\nresolution anisotropic model was built by combining common -image -point ( CIP) tomography \\n(Woodward et al., 2008) to refine  the model prior to FWI, enhanced template -matching FWI (ETM -\\nFWI) (Vigh et al., 2020a; Kang et al., 2020) to update sediment, salt, and car bonate velocities \\nsimultaneously. CIP  tomography  was used again below salt. Integrating these technologies improved \\nimaging of the key target , despite the limitation s of the legacy dataset . Additionally, ETM -FWI enabled \\nuse of the full record to provide greater depth of penetration, updating the model globally without \\ndifferentiating speci fic lithologies and eliminating  the need to carry out interpretation -based salt \\ngeometry scenario  testing . The signal processing included 3D multi -cable adaptive  deghosting to \\n\\n \\n \\n\\nattenuate source and receiver ghost effects and broaden  the useable frequency content of the input data . \\nThis was preceded by noise attenuation and followed by multiple attenuation workflows .   \\nMethod  \\n \\nThe multistage  velocity model building (VMB)  workflow can be broken into three  main stages : Initial \\nmodel building, post-salt and salt VMB , and pre-salt VMB.   \\n \\nThe l egacy velocity model consisted of a series of individual tilted -transverse isot ropic (TTI) models \\nfor each of the  adjacent  surveys . The legacy  model building was  driven mostly by multiscale CIP \\ntomography  with geologic constraints (Zdraveva et al., 20 13) to update the post- and pre -salt sediment  \\nregions of the model . The salt structure  was introduced by iterative sediment  and salt flood  scenarios  to \\ninterpret surfaces representing top and base of salt . The starting Vp model for the reimaging was \\nobtained by merging these legacy model s. To help remov e the short scale length  variations  and the \\ninconsiste ncy of salt/sediment models between surveys , the salt velocities  were removed  before \\nstructurally smooth ing the single sediment model . In parallel, anisotropy functions  were d erived  using \\nprevious experience in the area  (Zdraveva et al. , 2011 ) and available  well data. Kirchhoff depth  \\nmigration  (KDM) , forward modelling  and 1D ray -tracing analysis  were used throughout to validate  the \\nmodel building stages.  \\nInput model to FWI : FWI convergence issue s occur  when there is a large difference between the \\nrecorded  and modelled seismic waveforms, therefore the quality of the starting model is crucial. To \\naddress this,  the low -wavenumber velocity component was updated through two iterations of CIP \\ntomography, focused on the post -salt section, with  a priori geological information to constrain the shape \\nof the model update. The resultant model was used to build the salt model; top salt was interpreted using \\nthe second tomography image, followed by a salt flood with a constant velocity of 4500 m/s drive n \\nfrom well data and finally the base salt was interpreted using the salt flood image. The smoothed  legacy \\nvelocity was inserted below base salt honouring the depth change due to the changing overburden \\nsediment and salt geometry. Once all zones were put t ogether, a smoothing was applied to remove the \\nsalt boundary to allow ETM -FWI upda tes to refine the salt geometry .  \\nPost salt and salt model building : ETM -FWI with automatic decision making (Halliday et al., 2022) \\nwas used to update the entire velocity  field simultaneously , avoiding \\u2018locked in \\u2019 errors associated with \\na layer -by-layer top -down approach.  The approach depends on matching the local patterns between the \\nrecorded and modelled full shot record, decomposing the wavefield into its directionality (dip), phase, \\nand frequency. It then  classifies events according to criteria for adjustive FWI (Jiao et al., 2015) and \\nleast-squares FWI, providing additional measures to dete rmine  which objective function to use without \\nmanual intervention. This occurs for each iteration, and output QC maps indicate the model update \\ndirection. Utilising ETM -FWI with automatic decision making ensure s robust  matching  (Figure 2) , even \\nin the pres ence of strong velocity contrasts .  \\n \\nFigure 2 Interleaved shot QC . (a) Raw shots interleaved with modelled shots (blue shaded)  using \\ninput model to FWI . (b) Interleaved shot QC after ETM -FWI.  \\n\\n \\n \\n\\nResults  \\n \\nThe results demonstrated that this work flow was able to resolve velocity heterogeneity  (Figure 3) by \\nsimultaneously updat e sediment, salt, and carbonate layers .  \\n \\n \\nFigure 3 Legacy velocity model overlaid on the legacy KDM image  (left) versus  new velocity model \\noverlaid on the new  reprocessed KDM image  (right) .  \\n \\nFigure 4 shows the uplift observed in the new seismic image compared to the legacy image. The image \\nusing the new model shows significant uplift around salt flanks and b ase-salt allowing for better \\nunderstanding the syn -rift system located pre -salt and resulting in increased confidence .  \\n \\n \\nFigure 4 (a) legacy KDM image using legacy velocity model , and  (b) reprocessed KDM  image with the \\nnew velocity model . The green  arrows  on image ( b) show improved  defin ition of salt geometry  (arrows \\n1 and 2) , clear  base salt reflector  (arrow 3) and improved imaging of the pre salt reservoir section \\n(arrow 4) ; compared to areas indicated by the red arrows on image (a).   \\n  \\n  \\n\\n \\n \\n\\nConclusion  \\nWe demonstrated a successful application of a velocity model building workflow employing ETM -FWI \\nto use the full shot record to  update post-salt and salt velocities simultaneously, using an offset -limited , \\nnarrow -azimut h legacy dataset . This approach has accurately determined the  contrasting carbonate and \\nanhydrite layers surrounding the salt, simplifying and improv ing continuity and focusing of the base of \\nsalt and pre -salt reflectors.  This has reduced geological uncertaint ies and helped to better image  pre-\\nsalt targets and  predict reservoir properties . \\n \\nAcknowledgements  \\n \\nWe thank SLB multiclient and Ag\\u00eancia Nacional de Petr\\u00f3leo, G\\u00e1s e Biocombust\\u00edveis (ANPG) for \\npermission to present the results. We also  thank Olga Zdraveva, Suganda Tewari, Claire Field, and \\nShipra Mahat.  \\n \\nReferences  \\n \\nBranston, M., Chapelle, M., Rathee, D., Ramsumair, R., Thompson, J., & Campbell, R. [2022] What is \\nthe Right Level of Acquisition Effort to Enable Successful, Cost -Effectiv e Exploration? 83rd EAGE \\nAnnual Conference & Exhibition,  Vol. 2022, 1 -5. \\n \\nHalliday, D., Bloor, R., Cheng, X. and Elbadry, M. [2022] Automated Decision Making for Full -\\nWaveform Inversion. 83rd EAGE Annual Conference & Exhibition,  Vol. 2022, 1 -5). \\n \\nKang, W.,  Brand, N., Cheng, X., Vigh, D., Seymour, N. and Ishak, M., 2021, October. Reliability, \\nEfficiency, and Robustness: Enhanced Template -Matching Full -Waveform Inversion Using Sparse \\nOcean -Bottom Node Data. In  82nd EAGE Annual Conference & Exhibition  (Vol. 2021, No. 1, pp. 1 -\\n5). European Association of Geoscientists & Engineers.  \\n  \\nKang, W., Reisdorf, A., Madden, S., Robertson, V., Chen, D., Chen, Z., Xu, J., Cheng, X. and Vigh, D. \\n[2020] Salt Reshaping with Template -Matching Full -Waveform Inversion. 82nd EAG E Annual \\nConference & Exhibition Workshop Programme, Volume 2020, 1 \\u2013 5. \\n \\nShmelev, A.A., Cooke, A., Zdraveva, O. and Penwarden, J. [2016] High Resolution Model Building  \\nand Broadband Imaging in Deep Water Offshore Angola.  78th EAGE Annual Conference and \\nExhibition, Volume 2016, 1 -5. \\n \\nVigh, D., Cheng, X. and Kang, W. [2020a] Full Waveform Inversion with OBN data collection via \\nMad Dog examples. 90th Annual International Meeting, SEG, Post -convention workshop.  \\n \\nWoodward, M.J., Nichols, D., Zd raveva, O., Whitfield, P. and Johns, T. [2008] A decade of \\ntomography. Geophysics , 73, no. 5, VE5 -VE11.  \\n \\nZdraveva, O., Hydal, S. and Woodward , M. [2013]  Tomography with geological constraints: An \\nalternative solution for resolving carbonates. 83rd Annual International Meeting, SEG , Expanded \\nAbstracts, 4770 \\u20134774.  \\nZdraveva, O., Woodward, M., Nichols, D. and Osypov, K. [ 2011]  Building Anisotropic Models for \\nDepth Imaging: Comparing different approaches. 12th International Congress of the Brazilian \\nGeophysical Society, Extended Abstracts.\"}\n",
      "{\"text\": \"\\nSurface distributed acoustic sensing  (S-DAS)  for high resolution near surface characterization . \\n \\nIntro duction  \\n \\nIn the last few years , experiments on t he use of distributed acoustic sensing (DAS) using fibre  optics as \\nseismic sensors deployed on the surface has been performed by academia , oil and gas  operators, \\nconsortia,  and service providers  (Bakulin et al ., 2020) . While routinely  geophysical application s have \\nbeen mainly commerciali sed in the borehole environment , when deployed on the surface, surface \\ndistributed acoustic sensing (S -DAS) configuration s pose new challenges due to the peculiarity  of the \\nreceiver response and specifics of DAS data.  \\n \\nIn this paper, we describe the design and acquisition of a large -scale  3D land field test and processing \\nof the S -DAS data . We focus  on the latest advancement in the use of surface -wave analysis and \\ninversion and its application to S -DAS recording  through comparison and validation of co -located \\nmulticomponent (3C) geophones  and conventional high-density surface seismic nodal acquisitio n. \\n \\nWe show the contribution of multi -modal dispersion curves that leads to a multi -modal high -resolution \\nsurface wave inversion  that has been validated with the sub -surface profiling from non -seismic \\nmeasurement s such as  resistivity acquisition s (i.e., electrical resistivity tomography ). \\n \\nSurface DAS experiment  \\n \\nIn January 2022 , SLB WesternGeco Multiclient  completed the acquisition of a geophysical program in \\nthe Yoakum  County in West Texas, USA  (Bachrach et al ., 2022) . The Greenfield project comprises : \\n \\n\\u25aa High-density surface seismic acquisition , acquired with a fleet of  high-productivity heavy \\nvibrator s in slip -sweep mode and nodal geophones in orthogonal shooting layout . \\n\\u25aa 2D resistivity profiles in dipole -dipole  configuration . \\n\\u25aa 2D line of passive vertical geophone s. \\n\\u25aa A 3D layout  of 11,300 ft (3444 m) of optical fibre  cable  to record  S-DAS seismic data . \\n\\u25aa 2D line of 3-component sensor s (3C)  spaced every  10 ft (3 m) and deployed close and parallel to \\na long segment of the fibre  (see green line in Figure  1). \\n\\u25aa A dedicated sweep design and source layout , designed and acquired specifically for the S-DAS \\nexperiment.  \\n \\n \\nFigure 1  Survey design and trenching profile for the Greenfield 3D S -DAS experiment.  On the sketch \\non the left in blue the planned layout for the 11 ,300 ft  (3444 m)  of fiber optic cable serpentine, in green \\nthe co -located deployment of 3C multicomponent sensors; in red the location of the  planned  1700 shots . \\n \\nA cable  with six optical fibres  has been trenched 1 ft (30 cm) deep in the loose , loamy soil of the West \\nTexas cornfield  and following a serpentine shape as visualized in Figure 1 in blue . \\n\\n \\n \\n\\nA heterodyne interrogator has been connected to a high -volume  data storage equipped with 260  TB of \\ndisk space ready to accommodate the expected large data volume (up to 4  TB/h). \\nAt the end of cable , the six optical fibres  have been connected in pairs on a back -loop configuration \\nallowing the light signal to travel twice through th e cable doubling the recording length to 22 ,600 ft \\n(6888 m) . The interrogator has been switched on twice:  \\n \\n\\u25aa Phase 1: to liste n for two days to the fleet of vibroseis in slip -sweep mode from the high -\\nproductivity configuration simultaneously to the surface -seismic survey . \\n\\u25aa  Phase 2: in a much quieter  environment after the completion of the surface -seismic operation \\nwith a dedicated  source pattern (see Fig ure 1 in red) . The locations  of the  source s have been \\ndesigned to account for  the sensitivity of the fibre  cable, that favour s incident angles of \\nwavefronts parallel to the cable axis . \\n \\nS-DAS data  example  \\n \\nAn example of  a raw correlated seismic shot recorded for the entire 22,600 ft (6888 m) length of the \\nfibre  during the Phase 2 acquisition  is shown in Figure 2. The location of the shot is shown in green in \\nthe geometry layout sketch. The raw optical data have been realized w ith a spatial sampling of 10  ft (3 \\nm) and gauge  length  (GL)  of 20  ft (6 m) . In Figure 2, the locat ion of the shot is  very close to the first \\nturn of the cable, correspond ing to the point at which the cable from the segment on the surface is \\nburied. The strong dominating energy component clearly visible in the seismic record is ground -roll: \\nsurface -waves that travel cylindrically starting from the source point  with an elliptic particle motion \\ncontained in the  vertical plane (characterized by a vertical and a horizontal component ).  \\n \\n \\n \\nFigure  2 Single correlated shot  for the entire 22,600 ft (6888 m) of the recording fibre . The raw optical \\ndata have been realized with a spatial sampling of 10  ft (3 m)  and gauge  length of 20  ft (6 m) . Note the \\nmirroring effect caused by the back -loop configuration after the two fibres  inside the cable have been \\nspliced together , thus  doubling  the record ing active fibre . \\n \\nClosely observing the central part of the seismogram, it is noticeable that the dimming of the surface \\nwave amplitude corresponds to the portion of the fibre that lays orthogonally to the main direction of \\npropagation of the surface waves. This phenomenon is expected and peculiar to the DAS cable due to \\nthe combination of the directional sensitivity of the fibre to incident angle (theta) of the wavefront \\n(which  is a function of cos^2(theta ); see Sayed et al ., 2020 ) and t he sensitivity to the horizontal \\ncomponent of the elliptic particle motion , which is recorded by the S -DAS cabl e. This makes  the fibre \\nalmost transparent to ground -roll that comes from the crossline direction.  \\n\\n \\n \\n\\n \\n \\nSurface -wave analysis and inver sion \\n \\nIn this paper we will focus on the analysis of the results from the high -resolution spectral analysis of \\nthe ground -roll energy recorded by the fibre  cable . Dispersion curves for multiple modal events  are \\nautomatically picked  in the frequency -wavenum ber (FK)  space for each realized station along the cable \\n(spatial sampling at 10  ft (3 m) ), and the shear wave velocity ( Vs) depth  profile  is computed as a result \\nof their inversion  (Strobbia, 2010).  The same surface -wave analysis and inversion workflow has been \\napplied for comparison and validation to the corresponding segment of the horizontal (X) component \\nof the 3C sensors oriented in the same direction of the fibre  (see Figure 3 on the right).  \\n \\n \\n \\nFigure 3 Surface -wave analysis and inversion from co -located segment of fibre  optic cable (left) and \\n3C-X component geophone (right). Both the segments are spatially sampled every 10  ft (3 m) . A single \\nshot gather  is shown on the top; the high -resolution FK semblance for a single station is shown in the \\ncentre  and Vs multimodal inversion profile is shown at the bottom.  \\n \\nA single segment of the raw correlated shot gather is shown in Figure 3. The S -DAS seismic signa l \\n(Figure 3 on the left) realized from the heterodyne system is , in this case , proportional to the strain that \\nhas been demonstrated to be proportional to the displacement. The same segment from the co -located \\n3C-X component (Figure 3 on the right) of the geophone is instead proportional to the velocity. This \\nexpresses as a higher sensitivity of the DAS system towards the low -end of the frequency spectrum \\ncompared to conventional geophones. Both the segments are spatially sampled every 10  ft (3 m) . \\n \\nThe spectral analysis and the picking of the dispersion curves has been automated through a machine -\\nlearning (ML) enabled picker on both the S -DAS and 3C -X data. The high -resolution FK semblance \\n(see central pics in Figure 3) for a co -located location show s a higher sensitivity of the DAS cable \\n\\n \\n \\n\\ncompared to the 3C -X for a broad range of the frequency spectra . For this comparison the dispersion \\ncurves have  been picked for the fundamental and the first mode: the result  from the inversion is shown \\nat the bottom  of Figure 3.  \\n \\nDiscussion and conclusions  \\n \\nThe S -DAS data acquired from the fibre  optic shows a higher sensitivity compared to conventional \\nvertical geophone  (Figure  4). The direct comparison with the multicomponent co -located nodes shows \\nthat S -DAS data c an be compared to a single horizontal component oriented along the cable axis, but \\nwith the advantage of a higher sensitivity to the ground -roll and a denser spatial sampling at a much \\nlower cost. The dispersion curves spectra from the S -DAS data have been  inverted leading to a velocity \\nprofile that exhibit s a high resolution spatially and in depth.  The correlation of the velocity models \\nobtained from the different acquired components, both seismic and non -seismic, provided the best \\navailable framework to u nderstand the complexity of the near surface . This provides opportunities for  \\nnovel applications in the investigation of shallow targets .  \\n \\n \\nFigure 4 Vs multimodal inversion profile from conventional nodes acquisition , ERT resistivity profile  \\nand S -DAS are compared on a collocated section.  The lithological interpretation from the ERT profile \\nis overlaid.  \\n \\nAcknowledgments  \\n \\nWe would like to thank SLB WesternGec o Multiclient  for permission to use the S -DAS data , OptiQ \\nSLB fibre  optic division , the SLB Cambridge SCR R&E department for the knowledge support and \\nDawson for the acquisition and logistic s in the field.  \\n \\nReferences  \\n \\nBachrach, R., Busanello, G., Sayed, A., 2022, Applications of surface distributed acoustic sensing f or \\nCO2 storage and windfarms: Results from large scale experiment, GET 2022  \\n \\nBakulin, A., Silvestrov, I and Pevzner, R., 2020, Surface seismic  with DAS: An emerging alternative \\nto modern point -sensor acquisition, The Leading Edge  (2020),  39(11): 808  \\n \\nSayed, A., Shujat, A and Stewart, R., 2020, Distributed acoustic sensing (DAS) to velocity transform \\nand its benefits, SEG International Exposition and 90th Annual Meeting.  \\n \\nStrobbia C., Vermeer, P.L., Laake A., Glushchenko, A. and Re S. [2010] Surface wav es: processing, \\ninversion and removal. First Break, Vol. 28, 85 -91.\"}\n",
      "{\"text\": \"84th EAGE Annual Conference & Exhibition \\nDeep-salt: 3D salt segmentation from inaccurate migrated subsurface offset gathers using \\nconvolutional LSTM layers \\n \\nIntroduction \\n \\n An accurate seismic image depends directly on the velocity model used in migration. The \\nconventional geophysics tools, such as Tomography and Full-waveform inversion(FWI), recover the \\nvelocity model with a satisfactory resolution; however, they can not recover salt inclusions with high \\nresolution. As a result, salt inclusion over the velocity model is often a manual and interpretative task, \\nrequiring many tests and iterations to find a reasonable salt geometry (Dellinger et al. 2017).   \\n Deep learning (DL) has emerged as an alternative to conventional methods for many geophysical \\napplications (Yu and Ma, 2021). Recent studies investigated DL to solve tasks related to velocity model \\nbuilding (VMB) (AlAli and Anifowose 2022), where it was observed that salt inclusions are particularly \\nwell delimited over the predicted velocity models (Klatt et al. 2022). In order to reduce the data size \\nand the complexity of the task, some approaches addressed only the task of salt inclusions. Two \\nexamples of applications focused on salt are the segmentation of the bottom of the salt from the FWI \\nresponse by AlAli et al. (2022) and the segmentation of the complete salt geometry from the subsurface \\noffset gathers migrated with the sediment velocity by Muller et al. (2022).  \\n Despite the successful results, previous salt inclusion guided by DL relied on 2D simulations and \\n2D DL architectures. The extension to 3D simulations requires careful DL design to avoid memory \\nissues on the GPUs. In this work, we propose to extend the salt segmentation methodology proposed \\nby Muller et al. (2022) to 3D data,  using as DL architecture a U-Net with convolutional LSTM layers \\nsimilar to the one proposed by Arbelle and Raviv (2019). We tested this approach over a synthetic data set \\nand showed that the DL model could segment 3D salt geometries.  \\n \\nData generation \\n \\n Our training/validation/test set consists of 30 synthetic velocity models with 2 km of depth, 4,5 \\nkm of extension in the inline direction, and 3 km in the crossline direction. The spatial sampling rate is \\nequal to 12,5 m for the three directions. We constructed the 3D velocity models using real salt \\ngeometries from velocity models built with tomography and FWI in regions with complex salt structures \\nand high exploration interest in the past decade. To increase the complexity and diversity of the training \\nset, we applied over those geometries a 3D augmentation algorithm (Solovyev et al. 2022), which \\nrotates, flips, and resizes the original salt geometries. After the augmentation process, we resized the \\nsalt geometry to fit into the desired size and included it on a simple sediment velocity background. To \\ncheck for the generalisation ability of our approach, we also used the SEG/EAGE  3D  salt model \\n(Aminzadeh et al. 1996) in our tests because it presents sediment velocities and structures different from \\nthe ones in our training set. Figure 1 (a) shows one of the 30 generated models from the training set and \\n(b) the 3D SEG/EAGE salt model for comparison. \\n \\n \\nFigure 1: (a) shows one example of the velocity models used for the training/validation process. (b) \\nshows the SEG/EAGE 3d salt model used to test the generalisation ability of the DL predictions. \\n\\n \\n \\n84th EAGE Annual Conference & Exhibition \\n We simulated synthetic seismic shots for each model using a finite-difference wave propagator, \\nisotropic, acoustic, with second-order in time, eight-order in space, and exponential attenuation on the \\nabsorbing boundaries. The acquisition geometry defines 4800 shots, with 50 meters of increment in \\ninline and crossline directions. The simulated receivers are ten streamers 2000m long, spread with a \\nmaximum lateral distance of 500m from the source. We linearly extrapolated the original cubes in order \\nto accommodate the acquisition geometry. \\n We migrated the simulated shots using the smoothed sediment velocity, without any salt \\ninclusions, using an RTM cross-correlation extended imaging condition (Sava and Fomel 2003). The \\nRTM generates cubes for each desired subsurface offset value, which will then be used as channels of \\nthe input image to the neural network, defining the 3D salt geometry prediction in a pipeline similar to \\nthe one proposed by Muller et al. (2022). This kind of prediction relies on the fact that when using \\nextended imaging conditions, reflections migrated with an accurate velocity focalise on the zero \\nsubsurface offset. Otherwise, reflections migrated with a wrong velocity spread for the higher \\nsubsurface offsets. Thus, it is expected that the top of salt and sediment reflections to focalise at zero \\nsubsurface offset, and flanks and the base of the salt energy to be spread to higher subsurface offsets. \\n \\nDL architecture \\n \\n The proposed DL pipeline relies on supervised learning. The inputs are the cubes of migrated \\nsubsurface offsets, with each value of subsurface offset corresponding to one channel of the input image. \\nThe outputs are the cubes with the segmented salt geometry. One possibility to deal with 3D data is to \\nuse a DL architecture with 3D convolutional blocks. However, despite being successfully used for 3D \\ngeophysical tasks, e.g. (Shi et al. 2019), 3D convolutional blocks require much memory and present a \\nlonger training time.  \\n One possible alternative to handle the spatial information, and reduce the computational cost, is \\nthe use of Convolutional LSTM (C-LSTM). C-LSTM is a variant of LSTM (Long Short-Term \\nMemory), a special kind of recurrent neural network capable of learning long-term dependencies. C-\\nLSTM contains a convolution operation at each gate in the LSTM cell, capturing spatial features by \\nconvolution operations in multiple-dimensional data. Good examples of applications with C-LSTM are \\nfound in medical problems, especially the ones where it is possible to accommodate local spatial \\ninformation in 2D image sequences. For example, C-LSTM were used in volumetric data sets to \\nsegment 3D data represented as a stack of 2D slices (Chen et al. 2016). \\n In this work, we tested a network similar to the one proposed by Arbelle and Raviv (2019), where \\nthe C-LSTM blocks were incorporated in every scale of the encoder section of the U-Net architecture. \\nThe loss used for the supervised training is the Jaccard loss because of its capability to address the class \\nimbalance. We split the migrated cubes in sequences of five 2D slices along the crossline direction, and \\nthese five slices are the input for the training and prediction process. The label for the training process, \\ni.e., the output, is the central slice of the selected region with the segmented salt geometry. Thus, our \\ninput images present the dimensions [5,160,320,11] represented by the sequential 2D slices, vertical \\nsamples, horizontal samples, and subsurface offsets. The output is an image with [160,320], \\ncorresponding to the salt segmented of the central slice of the C-LSTM sequence. We run our DL \\nexperiments using TensorFlow in an NVIDIA RTX A5000. \\n \\nResults \\n \\n The proposed DL model was trained for eighty epochs, selecting the state with the smaller \\nvalidation loss. Then, the DL model was applied over unseen data, initially, two volumes not previously \\nused for training/validation but coming from the same data generation procedure. We used a sliding \\nwindow to accommodate the five samples for the C-LSTM layers to predict each slice of the volume. \\nThe results for two test volumes are in Figure 2, where we plot the predicted salt geometry in the first \\ncolumn and the difference between the predicted and the true salt geometry in the second column. It is \\npossible to observe that the salt geometry was recovered with high accuracy, with small errors in the \\ncontours and more significant errors in the salt flanks and bottoms. However,  the estimated salt masks \\nare not drastically off and can be used as an initial approach to salt inclusion over the velocity model to \\nbe posteriorly reinterpreted. The predictions present a Jaccard index for the test volumes equal to 0.88. \\n \\n \\n84th EAGE Annual Conference & Exhibition \\n  \\n \\nFigure 2: (a) and (c) show the predicted salt geometries of the test volumes. (b) and (d) show the \\ncorresponding difference between the predicted and the real geometry, where blue indicates a false \\nnegative and red indicates a false positive. \\n \\n We tested the generalisation ability of C-LSTM-U-Net by segmenting the salt geometry of the \\nSEG/EAGE 3D salt model. We preserved the original model structures and velocity values and just \\nrescaled the model to fit the model size used in this study. The predicted salt geometry is in Figure 3 \\n(a), and we show the prediction error in figure (b). When comparing with the test results, more mistakes \\nare visible for the salt geometry of the SEG/EAGE model, with some spurious inclusions in the deeper \\nregions and more significant errors in the contours. The contour errors can be due to the velocity model \\nused in the migration. Since there is no sediment model without salt available for use, we have chosen \\none slice without salt and applied a gaussian smooth over this slice which was then replicated to create \\nthe migration velocity volume. The Jaccard index obtained for the SEG/EAGE model was equal to 0.65. \\n \\n \\nFigure 3: (a) shows the salt geometry obtained for the SEG/EAGE model, and (b) shows the difference \\nbetween the predicted and the real geometry, where blue indicates a false negative and red false \\npositive. \\n \\n \\n \\n \\nConclusions \\n \\n\\n \\n \\n84th EAGE Annual Conference & Exhibition \\n Salt inclusion is particularly time-demanding over regions of allochthonous salt forming complex \\ngeometries, such as overhangs, teardrops and tongues. The method proposed here aims to solve the salt \\ninclusion in one step with DL, reducing the time of the iterative process of salt flooding. Our results \\nshow that RTM migration using the sediment velocity generates subsurface offset gathers, which serve \\nas the input for a DL model to identify the 3D true salt geometry, even for complex geometries.  \\n We showed that a C-LSTM-U-Net architecture is enough to solve the 3D problem in a simplified \\nway, using a sequence of 2D slices as input for the training and prediction process. The predictions \\nobtained present continuous salt geometries in both crossline and inline directions. Furthermore, the C-\\nLSTM-U-Net has small demands of memory compared with 3D convolutional neural networks and is \\nalso faster to be trained.  \\n This work is proof of the concept of salt segmentation over data migrated with 3D geometries and \\nstill requires further studies to be extended to real seismic data. In future studies, expanding the size of \\nthe volumes used for shot simulation and migration will be necessary, increasing the depth of the interest \\ngeometries. A higher number of training volumes will probably be necessary to reach an accurate result \\nwhen using larger volume sizes. Another improvement to make this technology viable for real data is \\nto apply a matching filter between the wavelet used for synthetic data generation and the wavelet from \\nreal data.  \\n \\n \\nReferences \\nAlAli, A. and Anifowose, F. [2022]. Seismic velocity modeling in the digital transformation era: a \\nreview of the role of machine learning. J Petrol Explor Prod Technol  12, 21\\u201334. \\nAlAli, A., Kazei, V., Kalita, M. and Alkhalifah, T. [2022]. Deep learning unflooding for robust subsalt \\nwaveform inversion. Geophysical Prospecting. \\nAminzadeh, F., Burkhard, N., Long, J. Kunz, T., Duclos, P.[1996]. Three dimensional seg/eaeg models \\n- an update: The Leading Edge , 15, 131\\u2013134.  \\nArbelle, A. and Raviv, T. R.[2019]. Microscopy Cell Segmentation Via Convolutional LSTM \\nNetworks , 2019 IEEE 16th International Symposium on Biomedical Imaging, 1008-1012. \\nChen, J., Yang, L., Zhang, Y., Alber, M., Chen, D. Z. [2016]. Combining fully convolutional and \\nrecurrent neural networks for 3d biomedical image segmentation, Advances in Neural Information \\nProcessing Systems , 3036\\u20133044. \\nDellinger, J., Brenders, A.J., Sandschaper, J.R., Regone, C., Etgen, J., Ahmed, I., Lee, K. J. [2017]. The \\ngarden banks model experience, The Leading Edge , 36, 151\\u2013158.  \\nKlatt, M., Faria, E.L., Muller, A.P.O., Coelho, J.M., Gonz\\u00e1lez, J.L., de Albuquerque, M.P., Bom, C.R., \\nCorreia, M.D. [2022]. Deep learning strategy for salt model building, Geophysics  87, IM221-IM233.  \\nMuller, A.P.O., Costa, J.C., Bom, C.R., Faria, E.L., Klatt, M., Teixeira, G., de Albuquerque, M.P. \\n[2022]. Complete identification of complex salt geometries from inaccurate migrated subsurface offset \\ngathers using deep learning, Geophysics  87, R453-R463. \\nSava, P. C., and Fomel, S. [2003]. Angle-domain common-image gathers by wavefield continuation \\nmethods, Geophysics  68, 1065\\u20131074 \\nShi, Y., Wu, X., Fomel, S. [2019]. SaltSeg: Automatic 3D salt segmentation using a deep convolutional \\nneural network, Interpretation  7, SE113-SE122 . \\nSolovyev, R., Kalinin, A.A., Gabruseva, T.[2022]. 3D convolutional neural networks for stalled brain \\ncapillary detection, Computers in Biology and Medicine  141, 105089. \\nYu, S. and Ma, J. [2021]. Deep learning for geophysics: Current and future trends,  Reviews of \\nGeophysics  59, e2021RG000742.\"}\n",
      "{\"text\": \"Construction of porous model using equivalent pore aspect ratio distribution function\\nIntroduction\\nResearch on wave-induced \\ufb02uid \\ufb02ow (WIFF) phenomenon proved that the dispersion of seismic waves\\nat different scales are positively correlated with the physical properties, which is related to both the\\n\\ufb02uid and pore structure in the pores. Given that the \\ufb02uid in pores has a signi\\ufb01cant effect on velocity,\\nthe inherent in\\ufb02uence of pore structure is often neglected. Therefore, understanding the propagation of\\nelastic waves in dry rocks is meaningful to evaluate the effect of pore structure in heterogeneous rocks.\\nAlthough it is very attractive to determine the distribution characteristics of pore structure by using con-\\nventional logging data (acoustic log or electrical log), its implementation is a prevailing challenge. As a\\nestablished technology, imaging logging is appropriate to estimate the characteristics of pore structure,\\nbut the corresponding interpretation is low-ef\\ufb01ciency and empirical, which has great limitations. An\\nimportant research \\ufb01eld of rock physics is to characterize the pore structure quantitatively in rocks by\\nusing the ideal geometric shape to simplify the actual complex structure.\\nMost rock physics models assume that rocks contain only one or two types of pores with \\ufb01xed equiv-\\nalent pore aspect ratio (EPAR). This assumption can well explain the relation of velocity with porosity\\n(Kumar and Han, 2005), but there are some limitations in the experimental data about the variation of\\nvelocity with pressure. Accepting the fact that, the microcracks in the rock will gradually close with the\\nincrease of pressure, which has a great impact on the velocity. Importantly, the \\ufb01xed EPAR assumption\\nmakes pore closure a discontinuous process, which results in that the microstructure porosity and the\\ncorresponding EPAR under different pressures can not be explained in a uni\\ufb01ed framework. In the rock\\nphysics model proposed by David and Zimmerman (2012), several sets of microcracks with different\\nEPAR exist within rock. The distribution of microstructures and the related elastic properties could be\\ndescribed with the help of microcrack density. On this basis, we carried out the analysis of measurement\\ndata and designed a 2D porous model using the obtained pore structure parameters. The effectiveness\\nof equivalent pore structure is veri\\ufb01ed by elastic wave propagation simulation based on \\ufb01nite element\\nmethod.\\nMethod\\nStiff pores usually do not \\u201cclose\\u201d under experimental pressure conditions. Even for pores with EPAR\\nof 0.01, the differential pressure required to complete closure is 800 MPa, so the non-closable pores\\nhave little effect on the relationship between velocity and pressure in the experiment. We assume that\\nthe rock contains a set of stiff pores with EPAR \\u03b1san and several sets of microcracks with different\\nEPAR in contrast with the assumption of \\ufb01xed EPAR. In the process of con\\ufb01ning pressure decreasing,\\nthe change of elastic modulus can be considered to be produced by the addition of microstructure.\\nMoreover, the number of initial microcrack sets is related to the pressure interval range from 0 MPa to\\nthe closure pressure. The elastic modulus can be expressed as the function of microcrack density based\\non Mori-Tanaka\\u2019s method \\uf8f1\\n\\uf8f4\\uf8f4\\uf8f2\\n\\uf8f4\\uf8f4\\uf8f3Kb\\nK=1+16(1\\u2212\\u03bdb)(1+\\u03bdb)\\u0393\\n9(1\\u22122\\u03bdb)\\n\\u00b5b\\n\\u00b5=1+32(1\\u2212\\u03bdb)(5\\u2212\\u03bdb)\\u0393\\n45(2\\u2212\\u03bdb). (1)\\nwhere \\u0393is the microcrack density, Kband\\u00b5bare bulk modulus and shear modulus of dry rock at higher\\npressure, Kand\\u00b5are elastic modulus which contain microcracks, and \\u03bdbis Poisson\\u2019s ratio.\\nBased on equation (1), we can estimate the related microcrack density \\u0393(P)in each pressure interval\\nduring the unloading process. Once the pressure drops to 0 MPa, the \\u0393(P)can be used to calculate the\\ncumulative microcrack density \\u00af\\u0393(\\u03b1). Further to this, the initial crack aspect ratio distribution \\u03b1ican be\\nobtained as follows\\n\\u03b1i=3\\n4\\u03c0\\u222b\\u00af\\u0393(\\u03b1)\\n\\u00af\\u0393i[\\nC(\\u0393)\\u2212Chp]\\n\\u0393dP\\nd\\u0393d\\u0393 (2)\\nwhere \\u00af\\u0393iis the initial microcrack density at 0 MPa, Cis the bulk compressibility, and Chpis the com-\\n84thEAGE Annual Conference & Exhibition\\npressibility at high pressures, which can be determined according the measurement range or nonlinear\\nleast square \\ufb01tting.\\nFigure 1(a) shows the measured results of compressional and shear wave velocities of two sandstone\\nsamples under different con\\ufb01ning pressures, in which sample 1 is shaly sandstone and sample 2 is clean\\nsandstone. It can be clearly observed that the response characteristics of the two samples to pressure\\nremain similar, notwithstanding the effects of mineral content on velocity. During the experiment, an\\nobvious increasing trend in velocity was observed until 20MPa. The reason for this phenomenon is\\nthat many microcracks are closed under the pressure higher than their closure pressure. As the pressure\\nincreased continually, the velocity variation trend is relatively smooth because of the amount of unclosed\\nmicrocracks in rock. The dotted line in Figure 1(a) describes the exponential law of rock elastic modulus\\nwith pressure (Zimmerman, 1991), and the \\ufb01tting results are in good consistency with the measured data.\\nAccording to the above analysis, we can use equation (1) and (2) to calculate the initial pore aspect ratio\\ndistribution and the corresponding porosity of the microcracks in the two samples, as shown in Figure\\n1(b). It is proved that there are a large number of microcracks with small EPAR in samples, and the\\ndistribution range of EPAR in shaly sandstone (sample 1) is larger. By discretizing the corresponding\\ndistribution function, we can obtain the initial EPAR under 0MPa, as shown in Figure 1(c). As an aside,\\nthe EPAR of stiff pores is estimated from the high pressure measurement data by using rock physics\\nmodel.\\n0 20 40 60 80 100\\nDifferential pressure (MPa)1.52.53.54.55.56.5P-wave velocity (km/s)\\n1.52.53.54.55.56.5\\nS-wave velocity (km/s)Sandstone Sample 1\\nSandstone Sample 2\\nFitting Line\\n0 0.5 1 1.5 2 2.5\\nEquivalent microcrack aspect ratio 10-300.150.300.45Cumulative microcrack density\\n00.511.522.533.5\\nMicrocrack porosity\\u00d710-5\\nCumulative Density (Sample 1)\\nCumulative Density (Sample 2)\\nMicrocrack porosity (Sample 1)\\nMicrocrack porosity (Sample 2)\\n\\u00d7-4 -3.5 -3 -2.5 -2 -1.5 -1 -0.5\\nLog equivalent pore aspect ratio10-810-610-410-2100PorosityEPAR distribution (Sample 1)\\nEPAR distribution (Sample 2)a) b) c)\\nFigure 1 Inversion of aspect ratio distribution of two sandstone samples: a) The velocity-pressure curve\\nin dry sandstone samples, b) The cumulative microcrack density and the corresponding microcrack\\nporosity, c) The initial EPAR distribution.\\nUsing the calculated porosity and EPAR of stiff pores and microcracks, we can construct an equivalent\\ntwo-dimensional porous geometrical model, and its elastic response can be solved by \\ufb01nite difference\\nmethod or \\ufb01nite element method. The similarity principle is the theoretical basis for studying the in-\\n\\ufb02uence of pore structure on seismic wave propagation by using the forward modeling of porous model.\\nSimilar to the in-lab seismic physical modeling, the geomerty objects should keep equal proportions\\nwith the core at the real pore scale, and ensure that the product of the wave propagation distance and\\nits measured frequency is always consistent in a period, which is equivalent to the same propagation\\nvelocity in natural and simulated media. According to the elasticy theory, the differential equation of\\nmotion can be expressed\\n\\u03c1m\\u2202u\\n\\u2202t2\\u2212\\u2207\\u00b7\\u03c3=F (3)\\nwhere \\u03c1mis the density of the solid matrix, u(x,y)is the displacement vector in 2D plane, \\u03c3is the\\nelements of the stress tensor and Fis the stress vector caused by external source. The motion equation\\ncontrolling the pore \\ufb02uid domain is\\n1\\nKf\\u22022p\\n\\u2202t2+\\u2207\\u00b7[\\n\\u22121\\n\\u03c1f\\u2207p]\\n=0 (4)\\nwhere \\u03c1fandKfis the pore \\ufb02uid and the bulk modulus and pis the pressure. Importantly, the stress and\\nacceleration are continuous on the boundary between solid matrix and pore \\ufb02uid. In similar 2D porous\\n84thEAGE Annual Conference & Exhibition\\ngeometrical models, some essential parameters, such as porosity, pore aspect ratio and pore distribution,\\nhave signi\\ufb01cant effects on elastic wave propagation. To exemplify this problem, we designed three\\ndiffernet models for numerical simulation. These models have the same porosity (6.8 %) and solid\\nmatrix properties, but there are obvious differences in the distribution of pores.\\na) b) c)\\nFigure 2 Differnet distribution of pores in 2D plane: a) Regular pore distribution, b) Regular pore\\ndistribution with different major axis orientation, c) Random pore distribution.\\nThe source and geophone in the numerical simulation are placed at the top and bottom of the model\\nrespectively. The forward modeling results show that different pore space distribution will make great\\neffect on the propagation velocity of elastic waves. Compared with Figure 2(b), the propagation velocity\\nof Figure 2(a) is signi\\ufb01cantly faster, which is related to the angle between the elastic wave propagation\\ndirection and the long axis of the pore. In addition, in the case of random distribution, the wave velocity\\nin the numerical simulation is close to that calculated by the DEM model (Wang et al., 2015).\\nExample\\nAccording to the above analysis, we can determine the porosity and EPAR of two different types of\\npores (stiff pore and microcrack) in the 2D model. Other properties including solid matrix properties\\ncan be referenced by David and Zimmerman\\u2019s paper. We assume the spatial distribution of pores obey\\nrandom distribution, and the equivalent geometric model obtained is shown in Figure 3(a) and Figure\\n3(b). The geometric model can basically reveal the pore structure characteristics of rock. Moreover, the\\npores are independent of each other, and the \\ufb02uid in the pores is gas. Simiarly, the source and geophone\\nin the numerical simulation are placed at the top and bottom of the model respectively.\\nAfter con\\ufb01rming the low re\\ufb02ection boundary, we can use FEM to simulate the elastic waves propagation\\nin the geometric model. Due to the existence of microcracks in the model, the relationship between\\nthe length of the minor axis and the minimum grid element should be determined in advance which\\ncould improve the computational ef\\ufb01ciency. Even so, the number of mesh elements after division still\\nreaches 107. The details of part of the grid elements are shown in Figure 3(c) and Figure 3(d). The\\nvelocity of the sample can be calculated according to the length of the model and the \\ufb01rst arrival time.\\nThe simulation results have a good correlation with the measured velocity, and the error of sample 2 is\\nsigni\\ufb01cantly lower than that of sample 1, which may be related to its low porosity. But how the factors\\nsuch as distribution and pore size affect the wave velocity of the sample, further research are needed.\\n84thEAGE Annual Conference & Exhibition\\nFigure 3 The established porous geometric model and the corresponding meshing results: a) Geometric\\nmodel of sample 1, b) Geometric model of sample 2, c) Partial \\ufb01nite element mesh of sample 1, d) Partial\\n\\ufb01nite element mesh of sample 2.\\nConclusions\\nIn this abstract, we analyzed the characteristics of non-closable pore and microcrack in dry rock samples\\nby David-Zimmerman model. The results show that there is an obvious difference in the distribution\\nof microfractures between pure sandstone and shaly sandstone, and the samples with high shale content\\nhave more developed microcracks and stronger heterogeneity. The results show that the random distri-\\nbution is more consistent with the pore distribution in the equivalent geometrical model. Therefore, the\\ncorresponding 2D porous model is designed in a random distribution. With the help of the \\ufb01nite element\\nmethod, we calculated the elastic wave response. The validity of the method is preliminarily proved, but\\nthe in\\ufb02uence of many factors including pore distribution and pore size on elastic wave need to be further\\nstudied.\\nAcknowledgements\\nThis work is sponsored by National Natural Science Foundation of China (grant no.42174130), the\\nNational Key R & D Program of China (grant no.2018YFA0702501) and the Science and Technology\\nProject of CNPC (grant no.2019A-3310). It is published with the permission of the State Key Laboratory\\nof Petroleum Resources and Prospecting.\\nReferences\\nDavid, E.C. and Zimmerman, R.W. [2012] Pore Structure Model for Elastic Wave Velocities in Fluid-\\nSaturated Sandstones. Journal of Geophysical Research: Solid Earth ,117(B7).\\nKumar, M. and Han, D.H. [2005] Pore Shape Effect on Elastic Properties of Carbonate Rocks. In: SEG\\nTechnical Program Expanded Abstracts 2005 . Society of Exploration Geophysicists, 1477\\u20131480.\\nWang, Z., Wang, R., Weger, R.J., Li, T. and Wang, F. [2015] Pore-Scale Modeling of Elastic Wave\\nPropagation in Carbonate Rocks. Geophysics ,80(1), D51\\u2013D63.\\nZimmerman, R.W. [1991] Compressibility of Sandstones . No. 29 in Developments in Petroleum Science.\\nElsevier, Amsterdam.\\n84thEAGE Annual Conference & Exhibition\"}\n",
      "{\"text\": \"Simultaneous deblending, deconvolution and Doppler-shift correction of marine vibrator data\\nIntroduction\\nNon-impulsive marine sources (NIMS), such as marine vibrators, appear as a viable option to ful\\ufb01ll pos-\\nsible new environmental constraints on marine acquisitions. They can also provide low frequencies and\\noffer opportunities for ef\\ufb01cient acquisition strategies by blending many phase-encoded sources together.\\nHowever, they present a unique set of processing challenges, such as the handling of the Doppler effect,\\nwhich stems from having a moving boat (and possibly receivers), together with a long emitting source.\\nLong-emitting sources in a marine acquisition context can be blended to make them economically viable\\nalternatives to more conventional gun arrays. In this case, new sweeps can be started without waiting\\nfor others to \\ufb01nish. With blending, many sweeps can be started and overlapped in a way that make them\\namenable to deblending techniques. We propose and illustrate an inversion strategy to simultaneously\\ndeblend, Doppler-shift and deconvolve blended marine vibrator (MVib) data. Our approach assumes\\nthat the seismic signal is sparse in a given domain. While the blending operator acts on common-shot\\ngathers, the Doppler-shift correction and deconvolution happen on receiver gathers (CRG). A hyperbolic\\nfunctional (Li et al., 2012) in the model space yields the desired sparseness. A hyperbolic functional in\\nthe data space improves the inversion\\u2019s robustness to non-Gaussian noise.\\nDoppler-shift correction and deconvolution of moving MVib data\\nFigure 1: Blended acquisition with three boats.We quickly review the approach presented in\\nGuitton et al. (2021) for the deconvolution and\\nDoppler-shift correction (DSC) of moving, ma-\\nrine vibrator data. The goal of this process is to\\nretrieve the data we would have acquired with-\\nout a moving source (DSC applied) and without a\\nsweep (deconvolved). This approach follows the\\nframework developed by Hampson and Jakubow-\\nicz (1995) to explain the relationships between\\nsweep, source motion and data. Assuming that\\na source boat moves at a constant speed while\\nemitting a repeatable sweep signal, with a non-\\nvarying sea surface, the relationship between an observed CRG dfrom a moving sweep signal and its\\ndeconvolved counterparts dibecomes\\nCdi\\u2261d (1)\\nwhere Cis a slanted, multi-dimensional stationary convolution operator with the sweep. This convolu-\\ntion operator depends on the shot sampling which, in turn, depends on the boat speed and on the sweep\\nsignal. For a non-moving source, the convolution operator becomes 1D and no Doppler effect is present.\\nIn theory, the correlation of the 2D sweep with the recorded data should yield phase-corrected traces.\\nHowever, due to the shot sampling in CRGs, the 2D correlation will also generate aliasing artifacts. Gui-\\ntton et al. (2021) propose to interpolate the missing traces (shots) to a \\ufb01ner grid to mitigate the aliasing\\nartifacts, while deconvolving the sweep, by minimizing\\nf1(m) =|SCLm\\u2212d|h+\\u03b5|m|h, (2)\\nwhere|.|his the hyperbolic functional which varies between the \\u21131and\\u21132norm. In equation 2, Lmare\\nthe deconvolved data on a \\ufb01ne grid where aliasing is mitigated and mis a data representation (dictionary)\\nwhere sparseness yields to data interpolation. In this work, the linear Radon domain is used. Other\\nrepresentations in the curvelet, wavelet, Fourier domains are also available. Finally, Sis a mapping\\noperator between the positions of the interpolated modeled data on a \\ufb01ne grid and the positions of the\\nobserved data.\\n84thEAGE Annual Conference & Exhibition\\nDeblending, deconvolution and DSC of MVib data\\nBlending could make the acquisition of MVib data time ef\\ufb01cient by circumventing the issue of having\\nlong emitting sources. We propose to extend equation 2 by adding a blending operator \\u03a6. Deblend-\\ning, together with interpolation and deconvolution, happens in common receiver gathers by forcing the\\nseismic data to be sparse in a given domain minimizing\\nf2(m) =|\\u03a6SCLm\\u2212d|h+\\u03b5|m|h. (3)\\nEquation 3 shows that estimating msimultaneously interpolates, deconvolves and deblends the data.\\nThe core of this method is the regularization term (i.e. the sparseness constraint) that attenuates the\\nincoherent part of the signal in a domain of choice ( L\\u2032can be curvelet, Fourier wavelet, Radon, etc...\\ntransforms). In this work, the incoherency in the data representation comes from the blending of shot\\ngathers and the interpolation/reconstruction step needed to deconvolve the sweep. We use the linear\\nRadon transform in patches on CRGs as our dictionary ( Lmin equation 3 becomes PLm where Pis a\\npatching/stitching operator that makes the linear Radon transform local).\\nExamples\\nFigure 2: Blended acquisition with two sources\\n(blue and red).We \\ufb01rst blend synthetic MVib data using a three\\nsource boats geometry (Figure 1). Here, 80 shots\\nrecorded on only 15 static nodes are used. Shot\\nand receiver spacing is 24 meters for a boat speed\\nof 4.8 m/s. Note that there is a small random\\nperturbation added to the \\ufb01ring times of the two\\nrightmost boats. The maximum frequency is\\n25 Hz to speed up computations, and there is lit-\\ntle to no Doppler shift in that case. The synthetic\\nMVib data without and with blending (i.e. \\u03a6\\u2020d)\\nare shown in Figure 3. We apply equation 2 to\\nthe data in Figure 3 to obtain Figure 4: while the\\ndeconvolution works well on the unblended case,\\nit fails on the deblended data due to truncation and blending effects, as expected (we would never run\\nthis in practice). Figure 5 shows the whole deblending, deconvolution and DSC process applied to the\\nblended data. The difference panels in Figure 5 proves that our approach is able to perform three tasks\\nat once and yields data (left of Figure 5) very similar to those obtained without blending (left of Figure\\n4.)\\nFigure 3: Synthetic Mvib data acquired without (left) and with (right) blending (geometry of Figure 1\\nafter running \\u03a6\\u2020d). 80 shots, recorded on 15 receivers are blended using the geometry of Figure 1.\\nNow, we look at the blending and deblending of MVib \\ufb01eld data acquired in 2022 (Alfaro et al., 2023).\\n84thEAGE Annual Conference & Exhibition\\nFigure 4: Deconvolution and DSC of the synthetic data in Figure 3 using equation 2. Left: no blending.\\nRight: blending. This process is applied on CRGs. The artifacts on the right panel are due to the\\ntruncations of the sweep in the pseudo-deblended CRGs and the presence of blending noise. The sweep\\nneeds to be handled before deblending or during, as proposed in this work.\\nFigure 5: Left: Simultaneous deblending, deconvolution and DSC of the blended synthetic data with\\nequation 3. Right: difference with left panel of Figure 4. The proposed approach is quite succesful at\\nhandling three processing steps at once thanks to the sparseness assumption.\\nWe arti\\ufb01cially blend 80 shots gathers (ds=12.5 m) recorded on 150 nodes (dr=50 m). The sweep is 5\\ns long and the blending sequence with two sources is shown in Figure 2: the boats \\ufb01re on position but\\nwith a small random time delay within [\\u22120.5,0.5]s. The boat speed is 1.8 m/s. Data are low-passed to\\n35 Hz to improve computational ef\\ufb01ciency. This blending sequence optimizes the acquisition time by\\noverlapping sweeps from two sources (red and blue dots in Figure 2) on one booat. A similar blended\\nacquisition line was acquired in 2022. The \\ufb01eld MVib data without and with blending (i.e. \\u03a6\\u2020d) are\\nshown in Figure 6. Applying equation 2 to the individual CRGs of Figure 6 (i.e. without blending)\\nyields the left panel of Figure 7, while applying equation 3 to the blended shots yields the right panel of\\nFigure 7. The two panels of Figure 7 are similar, proving that the simultaneous processing (deblending,\\ndeconvolution, DSC) is feasible.\\nConclusion\\nWe show the \\ufb01rst results of deblending MVib data and demonstrate that all pre-processing steps (de-\\nblending, DSC, deconvolution) can be achieved by enforcing a sparse representation of the signal in\\na given domain. Our goal is to use this approach on an actual blended MVib survey that we recently\\n84thEAGE Annual Conference & Exhibition\\nFigure 6: Field Mvib data acquired without (left) and with (right) blending (geometry in Figure 2 after\\nrunning \\u03a6\\u2020d). 80 shots, recorded on 150 nodes are blended using the geometry of Figure 3.\\nFigure 7: Left: DSC and deconvolution of the data in the left panel of Figure 6 using equation 2. Right:\\nDeblending, DSC and deconvolution of the arti\\ufb01cially blended data using equation 3. The two panels\\nare quite close, proving the ef\\ufb01ciency of the simultaneous processing.\\nacquired. Other processing steps, such as deghosting, could be incorporated as well in our approach.\\nAcknowledgments\\nWe thank TotalEnergies and the MVJIP for permission to publish this work.\\nReferences\\nAlfaro, R., Secker, S., Zamboni, E., Cozzens, A., Henderson, N., Jenkerson, M., Nechayuk, V ., Johnson,\\nG. and Karran, J. [2023] Validation of an Alternative Seismic Source: The Integrated Projector Node\\nMarine Vibrator Pilot Seismic Survey. 84th EAGE Annual Conference and Exhibition , submitted.\\nGuitton, A., Duquet, B., Secker, S., Mascomere, J.P. and Feltham, A. [2021] A deconvolution-\\ninterpolation approach with sparse inversion to mitigate the Doppler effect in marine vibrators data.\\nFirst International Meeting for Applied Geoscience and Energy, Expanded Abstracts , 2555\\u20132559.\\nHampson, G. and Jakubowicz, H. [1995] The effects of source and receiver motion on seismic data.\\nGeophysical Prospecting ,43(2), 221\\u2013244.\\nLi, Y ., Zhang, Y . and Claerbout, J. [2012] Hyperbolic estimation of sparse models from erratic data.\\nGeophysics ,77(1), V1\\u2013V9.\\n84thEAGE Annual Conference & Exhibition\"}\n",
      "{\"text\": \"\\nIntroduction  \\nIn this paper, we present a high-resolution FWI case study from the foothills of the Southern \\nCarpathians, in Romania. In this hilly area, the Spineni dataset was acquired in early 2022, composed \\nof approximately 85% dynamite shot points and 15% vibroseis shot po ints. It represents a total area of \\n500 km2 that fills and follows the reprocessing of the Getic Merge project for a total area of 3000 km2 \\n(Meffre et al., 2022). The large maximum offset x/y of 6 and 11 km, respectively, offers an optimal \\ndesign for wide-azimuth subsurface illumination, which is crucial in areas with complex structural \\ngeology as described by Krezsek et al. (2011). The reservoir, located below a thick overthrust at around \\n5 km depth, has a number of closures with uncertainties in size and crest positioning. This thrust \\ninclusion in the northern part of the survey, the so called Burdigalian wedge, is characterized by a very \\ncomplex folded structure with steep dips and strong lateral and vertical velocity variations with high \\nvelocities. In this context, a high-resolution FWI velocity model was used to help e nhance the imaging \\nand be\\ntter define the spill point of the prospect. \\nNear surface characterization \\nThe challenges identified in this area include the large variation of the topography and the presence of \\nvery slow velocities (700 m/s) in the weathering zone (WZ), as shown by the up-hole information. This \\ncreates strong imaging distortions from the very shallow down to the target depth. Ground roll \\ncontamination and irregular near offset distribution make access to near surface reflectivity very \\nchallenging. That is why multi-wave inversion (MWI) was used obtain an initial high vertical resolution \\nanisotropic velocity model for subsequent acoustic FWI updates. MWI, by minimizing the residuals of \\nthe first break (FB), ground roll dispersion curves (DC), and WZ base two-way times, allows for a \\nr\\neliable multi-parameter reconstruction of Vp, Vs, and \\uf065 (Bardainne, 2018; Donno et al., 2021; Prieux et \\nal., 2020). Thanks to a dense acquisition design, we were able to clearly identify and pick the base of \\nthe WZ from the receiver-side operators of a surface consistent deconvolution (Retailleau, 2015 ) \\n(\\nFigure s 1c-d). In addition, the good low-frequency content coming from the broadband signal produced \\nby the dynamite shots coupled with long offsets of up t o 10 km, enabled us to further resolve the l ong-\\nto-mid wavelength velocity of the near surface model. The output QC (Figure 1) showed reliable and \\ngeological ly consistent results with a flatter base of the WZ and fewer structural undulations compared \\nto the FB tomography model.  \\nFigure 1:  (a) FB tomography model; (b) MWI + FWI  7 Hz near surface model; (c) and (d) The base \\nof the weathering zone interpreted on the deconvolution operators using models (a) and (b), \\nrespectively; (e) and (f) Stacks after Kirchhoff pre-stack depth migration (PSDM) with models (a) and \\n(b), respectively. \\n50Hz high resolution land FWI: a case study in the Carpathian foothills\\n \\n \\n\\nComplex imaging underneath overthrust folded structures  \\nThe velocity shown in Figure 2a has already been  through a complete near surface update as described \\nabove, plus a first pass of multilayer TTI tomography. In addition, a pass of FWI from 7 Hz up to 12Hz \\nusing diving and reflect ed waves for the deeper section update (Figure  2b) shows a global improvement \\nof focusing (Figure  2e). However, some residual structural distortions and poor focusing were still \\nvisible below the thick thrust composed of heavily folded structures with strong velocity contrasts. To \\ncapture more detail in the velocity mode l to resolve these imaging problems, high-resolution FWI was \\nused (Zhang et al., 2020 ). As the FWI frequency was increased from 12 Hz to 40 Hz , gradual imaging \\nimprovements were observed due to the additional details in the model . Beyond 40 Hz, no significant \\nimaging improvement s were observed. The 50  Hz FWI velocity model fully captured the strong \\nvariations and velocity contrasts (Figure 2c), and the structures exhibit ed nice details and improved \\nfocusing of the target area (Fig ure 2f).  \\n \\nFigure 2 : (a) Section of initial model after a complete near surface update , (b) FWI at 12  Hz and (c) \\n50 Hz illustrating the complexity of the thrust  and high -velocity contrast in the target area.  \\nCorresponding  zoomed section  of stacks after Kirchhoff pre -stack depth migration (PSDM) with (d) \\ninitial , (e) FWI 12  Hz and (f) 50 Hz models to illustrate  the impact on the imaging in the target area . \\n \\nIn addition, FWI  Imaging ( Zhang et al., 2020 ), was derived from the 50 Hz FWI velocity. FWI  Imaging \\nis seen to benefit from additional information provided by the full wavefield data and uses least -squares \\ndata fitting , which is essential to image below complex geological features ( Salaun et al., 2021 ). Figure \\n\\n \\n \\n\\n3c shows clearer fault definition, better amplitude continuity , and fewer  migration art ifacts, as \\nhighlighted by the white arrows.  \\n \\nFigure 3 : (a) 50 Hz FWI velocity  model , (b) Kirchhoff  PSDM  stack  and (c) FWI Imaging reflectivity  \\nderived from (a). The yellow and red  extrema color palette  on the reflectivity  underlines stronger \\nvelocity contrast s to help delineate potential reservoir spill points . \\n \\nDe-risking fault closure uncertainties in the prospect area  \\nThe main prospect sitting below the overthrust, shown in the  white polygon on the depth slice of Figure s \\n4b and 4e, has multidirectional fault -dependent closures. Figure 4b shows the PSDM depth slice cutting \\nthrough the prospect area, which has high uncertainties in both  size and crest position in depth. The 50 \\nHz FWI velocity  model  in Figure s 4d and 4f  exhibits very high  resolution , which  captur es the complex \\nvelocity contrast of the  upper folded structures and the underlying stratigraphy . As a result, the fault  \\n \\nFigure 4: (a-b) 12  Hz FWI model and PSDM stack; (d -e) 50  Hz FWI model and PSDM stack ; (c-f) \\nSonic log  comparison with 12  HZ and 50  Hz FWI model , respectively.  \\n \\n\\nclosures around the prospect area are better imaged (Figure 4e), allowing considerable de-risking of the \\nuncertainties in their interpretation. Moreover, the velocity contrasts are also well correlated with the \\nwell\\u2019s sonic log velocities, providing further validation of the 50  Hz FWI model. \\nConclusions \\nIn this paper , we emphasiz ed the crucial importance of addi ng high-resolution details t o the velocity \\nfield using FWI i n a comple x foothill s geological settin g. The resulting model has greatly improved the \\nimagin g below th e overthrust an d help ed de-ri sk th e interpretation uncertainties of the reservoir \\nstructur e and fault closur e. In additi on, the associat ed high-resoluti on velocit y model c an al so be used \\nwith Kirchhoff PSDM conventional imagi ng to facilitate detail ed reservoir interpretation work.  \\nThese results, in reaching 50 Hz, which is high frequency for land imaging, were obtained also thanks \\nto the well-designed long offset WAZ acquisition, strong low-frequency content of the dynamite source, \\nand the  continuous  effort  made  on computation  technology  over last few years.  For the near  future, \\nelastic  FWI could potentially  offer  further  uplifts by  deriving improved  Vp and Vs  models  in such a \\nhigh velocity contrast medium. \\nAcknowledgements \\nWe are grateful to O MV PETRO M and CGG for their permission t o publish this study. We also thank \\nall those who were involved in the project. \\nReferences \\nBardainne, T. [2018] Joint inversion of refracted P-waves, surface waves and reflectivity. 80th EAGE \\nConference & Exhibition , Extended Abstracts, We K 02. \\nDonno, D., Farooqui, S., Mostafa, K., McCarthy, D., Solyga, D., Courbin, J., Prescott, A., Delmas, \\nL. and Le Meur, D. [2021] Multiwave inversion: A key step for depth model building - examples \\nfrom the Sultanate of Oman. The Leading Edge, 40(8), 610-618. \\nKrezsek, C., Oterdoom, H., Dzido, P., Barbu, V., Bland, S., Arnberger, K. and Lapadat, A. [2011] \\nNew insights into the hydrocarbon system of the getic Depression Romania: Implications for \\nExploration. 73rd EAGE Conference & Exhibition Workshops, Extended Abstracts. \\nMeffre, A., Prieux, V., Retailleau, M., Le Meur, D., Monteiro, A. A., Bouzouita, Z., Wang, F., Mestiri, \\nS., Markos, T., Vermeulen, J., Orosz, J. and Tyler, E. [2022] Revival of legacy land seismic surveys \\nusing advanced processing technologies - Example from the Carpathian foothills. First Break, 40(1), \\n45-52. \\nPrieux V., Bardainne T., Meffre A., Prigent H., Van Kleed F.J., Waqas M. and Hou L. [2020] \\nStructurally constrained anisotropic Multi-Wave Inversion utilizing Machine Learning and Big Data \\non a Middle East OBC project.  EAGE Annual Conference & Exhibition Online, Extended Abstracts. \\nRetailleau, M.G. [2 015], Imaging the Near Surface Using Surface-consistent Prediction Operators - \\nExamples from the Middle East. 77th EAGE Conference & Exhibition , Extended Abstracts, Th N116 \\n09. \\nSalaun N., Reinier M., Espin I.  and Gigou G. [2 021] FWI velocity and imaging: A case study in \\nthe Johan Castberg area. 82nd EAGE Annual Conference & Exhibition , Extended Abstracts. \\nZhang, Z., Wu, Z., Wei, Z., Mei, J., Huang, R. and Wang, P. [2 020] FWI Imaging: Full-wavefield \\nimaging through full-waveform inversion. 90th SEG International Meeting , Expanded Abstracts, \\n656-660\"}\n",
      "{\"text\": \"Dolomitization Model of Pelletoid of Arab C Formation in Southern Arabian  Gulf  \\n \\nIntroduction  \\n \\nThe Arabian Gulf in the late Jurassic is a semi -enclosed sea with shallow water  (Morad, 2012) . The \\nsouthern Arabian gulf presents a gentle landform, which is well suited for the deposition  of extensive \\ncarbonate sediments  (Figure 1) . With the closure of the ancient Tethys Ocean and the arid subtropical \\nclimate , it gradually changed from restricted platform to evaporative platform , and a large amount of \\nevaporite was produced, which also cal led Sabkha.  Among these evaporites, dolomite is the most \\ndeveloped  (DiLoreto , 2019) . From the remaining structural  and components, it can be seen that the \\noriginal dolomite sediments were mainly pelletoid.  Dolomitization  is the main geological process that \\nturn pelletoid into dolomites. However, there are still many unanswered questions . How does \\ndolomitization convert pelletoid  into dolomites?  Why do dolomites have different structures and \\ncrystals?  This paper mainly ans wers these two questions.  \\n \\n \\nFigure 1  Sedimentation and facies belts along the southern Arabian Gulf (cited in Kendall , 2011)  \\n \\nMethod and Data  \\n \\nTaking the Jurassic  Arab C  Formation of BH oilfield in United Arab Emirates  as an example, the study \\nwas carried out. The BH field is located in the south Persian Gulf, which is tectonically located in the \\nRubal -Khali Basin .  There are a total of 3 coring wells in the Arab C  Formation. The cumulative length \\nof the core is 120 m, there are 410 cast thin sections, there are 3263 physical property samples, there are \\n44 mercury injection  samples .  \\n \\nFirstly, according to the cast thin sections , the Pelletoid  types  are determined . Secondly,  the diagenesis \\nof the Pelletoid  is comprehensively recognized  based on the theory of petrology and geophysics, \\nincluding the diagenetic types, diageneti c environment  and diagenetic product.  Finally, the \\ndolomitization  model of Pelletoid  of Arab C Formation in Southern Persian Gulf  is established and \\ndiscussed based on various geological theories and data.   \\n \\nResults  \\n \\nThere are four  types dolomite in Arab C formation in BH oilfield, which were called type  \\u2160, type \\u2161, \\ntype \\u2162, type \\u2163. The type \\u2160 showed that the inner pelletoid  was replaced by dolomite  and the pelletoid  \\ncontour were preserved . The intergranular pore were filled with small calcite, locally dolomite filled \\n\\n \\n \\n(Figure 2a) . The type \\u2161 showed that the pelletoid  were almost dissolved, leaving moldic pores. However, \\nthe pelletoid  margin was still preserved, which is in a dark brown colour. The intergranular pore was \\nmainly filled with a large amounts of dolomite, as a result, the pores and particles are reversed  (Figure \\n2b). The type \\u2162 showed  that the pelletoid are completely repl aced by domomite, but the primitive \\nstructure was preserved and some dissolved pore are developed.  The dolomite size were pretty small \\nand large amounts of micropores were developed (Figure 2c) . The type \\u2163  showed that the pelletoid  \\nwere completely replaced  by dolomite and intense dissolution also occurred. The pelletoid structure is \\nnot obvious and large amounts intercrystalline pore and vug pore developed (Figure 2d).  \\n \\n \\nFigure 2 Dolomite types of Arab C formation in BH oilfield . (a) W ell BH-3, 9696.0 ft , the remaining \\npelletoid is obvious, the dark brown part is calcite;(b) W ell BH-3, 9693.0 ft, the pores and particles \\nare reversed, the dolomite filled the pores, the dark brown part is calcite and around the moldic \\npore;(c)  Well BH-3, 9678.8 ft,  the pell etoid are completely replaced by domomite, but the primitive \\nstructure was preserved and some dissolved pore are developed; (d) Well BH-3, 9712.05 ft,  the \\npelletoid structure is not obvious, some vug pore is developed.  \\n \\nThe type  \\u2160. The original rock of d olomite  of type  \\u2160 is pelletoid  limestone  (Figure 3a ). Diagenesis includes \\ngypsification , dolomitization, dissolution and cementation. In the penecontemporaneous  stage, some  \\npelletoid s were replaced by gypsum but the contour of the pe lletoid  were still recognizable. The \\nprecipitation of gypsum promoted dolomitization  due to consumption of Ca2+. As a result, the pelletoid \\nand gypsum were replaced by dolomite . Dolomite scatter  in the inner gypsum and pe lletoid.  (Figur e \\n3b). In marine environment, micrization  occurs at the margin  of pelletoid , forming micrite envelop . The \\nmicrite envelop has stable chemical property . The meteoric  water selectively dissolves the inner \\npelletoid , leaving the micrite envelop undissolved . The dissolution form pores in the grain s, but a small \\namount of calcite still  remains ( Figure 3c ). The cementation occurred because of the saturated fluid \\nformed by the dissolution , form ing of iso-rim blade  calcite at the margin  of pelletoid . With the \\ndevelopment int o the pores, the calcite develops into equiaxed granules, and the calcite occupy a large \\nnumber of intergranular pores  (Figure 3 d). Dolomitization occurred again during the burial period, \\nforming euhedral  fine-grained dolomite, which further filled the res idual pores ( Figure 3 d), and \\neventually formed the type  \\u2160 of dolomite as show in cast thin sections  (Figur e 2a). \\n \\n \\nFigure 3 The forming process of ty pe \\u2160 dolomite . (a) pelletoid limestone; (b) gypsification; (c) \\ndolom itization  in pelletoid; (d) mic ritization in marine environment and dissolution in  meteoric \\nenvironment; (e) iso -rim cementation in meteoric environment; (f) Dolomitization in burial \\nenvironment  \\n \\nThe type \\u2161.  The original rock of dolomite of type  \\u2161 is pelletoid limestone  (Figur e 4a). In the \\npenecontemporaneous  stage, some  pelletoid were replaced by gypsum and the some intergranular pores  \\nwere filled with gypsum. The precipitation of gypsum consumes Ca2+, improves Mg2+/Ca2+, and \\npromotes dolomitization. The dolomite not only metasoma sis pelletoid , but also metasomasis  the \\n\\ngypsum formed in the early stage. The dolomite is discretely embedded in the gypsum and pelletoid  \\n(Figur e 4b). When the sea level decline, dissolution occurred in meteoric  envi ronment, and a large \\nnumber of  pelletoid  were intense dissolved , forming intragranular pores. Some particle dissolution was \\ninsufficient, and a small amount of micrite remained. Cementation occurred after the dissolution in \\nmeteoric environment because of the enrich of  Ca2+, form ing of iso- rim blade-like or equiaxed granular \\ncalcite at the margin  of pelletoid  (Figur e 4c). Mg2+/Ca2+ was enhanced  again by cementation. In the \\nburial environment, the temperature and pressure conditions promoted the occurrence of dolomitization. The newly precipitated  dolomite crystals filled the intergranular pores, resulting in the pores inversion \\n(Figur e 4d), and eventually formed the type  \\u2161 of dolomite as show in cast thin sections  (Figur e 2b). \\nFigure 4 The forming process of ty pe \\u2161 dolomite . (a) pelletoid  limestone; (b) gypsification  and \\ndolomiti zation  in pelletoid; ( c) micritization in marine environment , dissolution  and iso-rim \\ncementation in meteoric environment; ( d) Dolomitization in burial environment  \\nT\\nhe type \\u2162. The original rock of dolomite of type  \\u2162 is pelletoid limestone  (Figur e 5a). Diagenesis \\nincludes gypsification , dolomitization and dissolution. In penecontemporaneous  stage , gypsification  \\nwas weak, while dolomitization , and pe lletoid  were fully metasomatized by dolomite  (Figur e 5b). \\nAlthough the structure of spheroids was well preserved, they almost changed from limestone  to \\ndolomite stone . In meteoric environment , the residual limestone was dissolved and  pelletoid  with weak \\ndolomitization dissolved to form mold ic pores . The rock was completely transformed into dolomite \\n(Figur e 5c). In the later burial process , compaction has little influence on porosity. Dolomitization in \\nburied environment forms a few coarse euhedral  dolomite filling pores  (Figur e 5d), forming the t ype \\u2162 \\nof dolomite as show in cast thin sections  (Figur e 2c). \\nFigure 5 The forming process of typ e \\u2162 dolomite . (a) pelletoid limestone; (b)  weak gypsification  \\nand intense dolomitization; (c ) dissolution in meteoric environment; (d) Dolomitization in burial \\nenvironment  \\nT\\nhe type \\u2163. The original rock of dolomite of type  \\u2163 is pelletoid limestone  (Figur e 6a ). In \\npenecontemporaneous stage , the pe lletoid  underwent intense gypsification , and some intergranular \\npores were filled by  gypsum. And then, intense dolomitization occurred, and the pe lletoid  and gypsum \\nwere fully metasomatized by fine dolomite, in which the dolomites in the gypsum were discrete, and \\nthe dolomites in the pe lletoid  were in clusters (Figure 6b). In meteoric environment , the resid ual \\nlimestone was completely dissolved, and the pe lletoid  with low metasomatic degree of dolomite were \\nselectively dissolved to form mold ic pores, while the particles with high degree of dolomization were \\ndissolved to form intergranular pores (Figure 6c). As dissolution continues and burial depth increases, \\nthe pore was adjust ed, the cluster dolomite becomes loose, and the contour of the moldic pore  becomes \\nblurred. The distribution of the dolomite crystal is uniformized as a whole, the por osity d oes not ch ange, \\n\\nbut the connectivity of the inter crystalline  pore is significantly improved (Figure 6d), forming the t ype \\n\\u2163 of dolomite as show in cast thin sections  (Figur e 2d). \\nFigure 6 The forming process of type \\u2163 dolomite.  (a) pelletoid limestone; (b)  intense gypsification \\nand dolomiti zation ; (c) dissolution in meteoric environment; (d)  pore system ad justment  \\nDis\\ncussion  \\nThe \\nArab C Formation was in a tidal flat environment, in which facies include supratidal (Sab kha),  \\nintertidal and subtidal . Different facies have different water energy, lithology, exposure probability and \\ntime, as a result, the diagenesis such as gypsification, dolomitization, dissolution and cementation  in \\ndifferent area vary greatly. That\\u2019s why there are four type of dolomite even the primitive particles are \\nthe same pelletoids.  The pellet oids were mainly in the intertidal facies, subtidal facies with relatively \\nhigh energy could also developed. I ntertidal facies was easy to exposure, and gypsification and \\ndolomitization was intense in penecontemporaneous stage . The type \\u2162  and type \\u2163 probably deposited \\nin the intertidal facies. S ubtidal facies mainly under the water  where it is not easy to exposure . When \\nthe wave base decline, it would exposure for a short time . It\\u2019s not only that the gypsification and \\ndolomitization have an important impact on the pelletoids , but also that dissolution and cementation  \\ntransform the pelletoids and pores.  The type \\u2160, type \\u2161, probably deposited in the intertidal facies.  \\n Co\\nnclusions  \\nTh\\ne pelletoids  limestone of Arab C  formation in BH oilfield is mainly developed in the intertidal and \\nsubtidal facies and has experienced intense diagenetic transformation intensity.  The most important \\ndiagenesis includes gyp sification, dolomitization, dissolution and cementation.  Due to the environment \\ndifference, response to sea level fluctuation and exposure time, the diagenesis, especially the extent of dolomitazition is different. As a result, the d olomite has  different c haracter in structure and crystal.  \\nAc\\nknowledgements  \\nThi\\ns paper was funded by China Petroleum Science and Technology Major Project \\u201cResearch on Key \\ntechnologies for effective development of ultra -low permeability carb onate reservoi r\\u201d, No.2021DJ3202 \\nand \\u201c The Theory and Key Technology Research on Efficient Development of Abnormal High -Pressure, \\nSour, Light Oil Carbonate Reservoirs \\u201d, No.2022DJ3211.  \\nRe\\nferences  \\nMor\\nad S, Al-Aasm I , Nader F H. 2012. Impact of diagenesis on the spatial and temporal distribution of \\nreservoir quality in the Jurassic Arab D and C members, offshore Abu Dhabi oilfield, United Arab Emirates. GeoArabia, 17: 17 -56. \\nDiLoreto Z A, Bontognal i T R R, Al Disi Z A, et al. 2019. Microbial community composition and \\ndolomite formation in the hypersaline microbial mats of the  Khor Al -Adaid sabkhas, Qatar . \\nExtremophiles, 23: 201-218. \\nKendall C G, & Alsharhan A S. 2011. Coastal Holocene carbonates of Abu Dhabi, UAE: depositional \\nsetting, geomorphology, and role of cyanobacteria in micritization. Quaternary carbonate and evaporite \\nsedimentary facies and their ancient analogues. International Association of Sedimentologists Special Publication, 43, 205-220.\"}\n",
      "{\"text\": \"\\nCoupling Genetic algorithm and Random Forest for robust prediction of CO 2 storage efficiency \\nin underground formation s \\n \\nIntroduction  \\nSubsurface aquifers have long been acknowledged as the most significant storage areas worldwide. \\nAround 1000 -10000 Gt of CO 2 might be stored underground in saline deposits, which equals 99 percent \\nof the world's total storage capacity (IPCC, 2005; Zhang et al., 2020) . As a general rule, there are four \\nprimary ways in which CO 2 cannot escape from storage sites: s tructural, residual solubility, and mineral \\ntrappings (Al-khdheeawi et al., 2018) . These mechanisms have been different performances d ependent \\non saline aquifers and cap -rock properties (Sun et al., 2021) . It is observed that each of these processes \\nis engaged on various time frames. Nonethe less, the most significant strategy of assuring safe storage \\nefficiency is the trapping of residuals and solubility (Al-khdheeawi et al.,  2018) .  \\nSo far, several studies have applied the physics -based reservoir modeling approach for estimating the \\ntrapping efficiency at various storage sites around the globe. Deep saline aquifer CO2 storage \\nperformance may be estimated using a modeling t echnique if the available data is made accessible. \\nSubsurface data such as geological and geophysical must be acquired in order to employ the simulation \\nmethod (Vo Thanh et al., 2020; Vo Thanh and Lee, 2021) . Physical reservoir modeling, which \\nconsumes a lot of computer resources, is thus not always a viable choice for forecasting storage \\nperformance. As a res ult, running a single scenario through the full -field model takes an inordinate \\namount of time. Simulator work might last anything from a few days to several weeks (Ghassemzadeh \\net al., 2021; Jeong et al., 2018; Ma et al., 2020) . Due to this latter, the genetic algorithm is adopted for \\ncoupling with RF to  develop the robust the ML model for our research purpose . To the best of our \\nknowledge, no work has been completed using datasets from diversity reservoir characterization models \\nand GA -RF to forecast CO 2 trapping performance in saline aquifers. For start ers, the databank for this \\nresearch was compiled using various field observations and credible literature. In order to train the \\nrobust machine learning models from the databank, the GA is paired with the RF. A comparison with \\npast research will be used to  determine if the proposed ML model is better. After that, a quantitative \\nexamination of the features that affect CO 2 trapping performance will take place. In the end, the GA -\\nRF model will be used to estimate CO 2 storage in the Sleipner  field and compared to findings from \\ncommercial software reservoir simulations.  \\nMethod  \\nThe overview workflow constructing robust ML models to predict CO 2 trapping efficiency for saline \\nformations is depicted in  Figure 1.  Firstly, the data collection was co nducted to generate the inputs and \\noutputs for the databank. Then, RF and GA were used to build the robust ML models to evaluate residual \\ntrapping index (RTI) and solubility trapping index (STI) using selected variables in Table 1. Moreover, \\ntuning hyperpa rameter was conducted using the k -fold cross -validation (k -FCV) and genetic algorithm \\n(GA) process to optimize the robust ML models. Next, the superiority of robust ML models was \\ndemonstrated by comparing them with previous studies. Ultimately, the robust ML models have verified \\nthe accuracy and stability by using the 2019 Sleipner benchmark. Powerful ML models predicted the \\nCO 2 RTI and STI of Sleipner field. They were then compared with CO 2 trapping with commercial \\nsoftware  \\nResult and discussion  \\nFigure 2 shows the prediction performance of the GA -RF model for training and testing datasets. It is \\nexhibited that the data samples in the training RTI and STI set are close with the fitting line ( Figure 2a \\n& 2b). For the testing phase, the data samples are distri buted closely around the fitting line ( Figure 2c \\n& 2d), although the distribution presents several scatter points that are located not closely the fitting \\nline. These training and testing results prove that the GA -RF model can accurately reproduce the \\nsimulation CO 2 trapping data and has relative power and generalization capability to predict the blind \\ndata. As the excellent performance model that drives accurate evaluations of CO2 trapping efficiency \\n(RTI and STI) in deep underground saline aquifers, t he GA -RF model was further compared with \\nexisting ML study, namely Artificial Neural Network Geological CO 2 storage proposed by Kim et al. \\n(2017) . To ensure the fairness of the comparison with previous works, it should be recapped that we \\nonly  \\n \\n \\n\\nrecorded the RMSE and R2 in these works. The statistical indicator comparison between GA -RF and \\nANN models are denoted in Table 1 .  \\n \\nTable 1.  The c omparative of this work and previous model  \\nIn addition, Table 2  compares CO 2 trapping performance between our hybrid GA -RF model and \\nsimulation result of Sleipner storage site. This comparison reflected that that the prediction of the GA -\\nRF model is nearly perfect matching to the field simulation results for both RTI and STI. The field \\napplication demonstrated that the constructed GA -RF model can be adapted to provide the feasibility \\ninvestigation on CO 2 trapping efficiency in a large scale geological storage project as well as Sleipner \\nsite. Recap that it is essential to perform i nitial evaluation of potential trapping efficiency for CO 2 geo-\\nsequestration in underground saline formations. The GA -RF model can estimate the residual and \\nsolubility trapping efficiency with reasonable results by considering only nine variables, such as \\ninjection time, post injection period, injection rate, salinity, depth, residual gas saturation, permeability, \\nthickness, porosity. Thus, the proposed GA -RF based ML model can be utilized as a fast and robust \\noption to estimate the feasibility of geologica l CO 2 storage sites.  \\n \\nTable 2. The results are compared between commercial software and machine learning  \\n \\n \\n \\n \\n \\n \\n \\nConclusion  \\nThis study constructed a powerful machine learning technique, namely GA -RF model, to predict the \\nCO 2 trapping efficiency based on nine parameters: injection time, post injection period, injection rate, \\nsalinity, depth, residual gas saturation, permeability thickness, porosity. The reasonable and robustness \\nof the hybrid GA -RF model was checked by investi gating the statistical indicators. Also, the superiority \\nof the constructed model was pointed out by comparisons with previous studies. In addition, the \\nimportance of input features to the CO 2 trapping efficiency was explored using the PI. Moreover, the \\nGA-RF model was adapted for predicting CO 2 storage index in Sleipner sites with excellent agreement \\nbetween simulated and our ML model  \\n  Training set  Testing set  \\nSource  Model  R2 RMSE  R2 RMSE  \\nGA-RF 0.9977  0.0103  0.9929  0.0185  This work  \\nANN  0.9884  0.88 0.9888  1.26 Kim et al.2017  \\n RTI STI \\nTime  Simulated  ML predicted  Simulated  ML predicted  \\n20 0.3684  0.3856  0.2258  0.2565  \\n30 0.4038  0.3996  0.2764  0.2678  \\n40 0.4421  0.4277  0.2988  0.2898  \\n\\u2026 .. \\u2026. \\u2026. \\u2026.. \\n1000  0.2027  0.2140  0.7302  0.7137  \\n \\n \\n\\nFigure 1. The entire workflow is proposed for this study  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nFigure 2. The graphical analysis is showing the correlation relationship between the predicted and \\nsimulated CO 2 trapping index using GA -RF model for training set ((a) & (b)) and testing set ((c)&(d)).  \\n \\n \\n\\n \\nReference  \\nAl-khdheeawi, E.A., Vialle, S., Barifcani, A., Sarmadivaleh, M., Iglauer, S., 2018. Impact of Injection \\nScenario on CO2 Leakage and CO2 Trapping Capacity in Homogeneous Reservoirs Model \\ndescription and initialization, in: Offshore Technology Conference A sia. Kuala Lumpur, \\nMalaysia.  \\nAltmann, A., Tolo, L., Sander, O., Lengauer, T., 2010. Permutation importance\\u202f: a corrected feature \\nimportance measure. BIOINFORMATICS 26, 1340 \\u20131347. \\nhttps://doi.org/10.1093/bioinformatics/btq134  \\nBreiman, L.E.O., 2001. Random Forests. Mach. Learn. 45, 5 \\u201332. \\nGhassemzadeh, S., Perdomo, M.G., Haghigh, M., Abbasnejad, E., 2021. A data -driven reservoir \\nsimulation for natural gas reservoirs. Neural Comput. Appl. https://doi.org/10.1007/s00521 -021-\\n05886 -y \\nIPCC, 2005. IPCC special repo rt on carbon dioxide capture and storage. Cambridge University Press, \\nUK/New York, NY, USA.  \\nJeong, H., Sun, A.Y., Lee, J., Min, B., 2018. A learning -based data -driven forecast approach for \\npredicting future reservoir performance. Adv. Water Resour. 118, 95 \\u2013109. \\nhttps://doi.org/10.1016/j.advwatres.2018.05.015  \\nKim, Y., Jang, H., Kim, J., Lee, J., 2017. Prediction of storage efficiency on CO2 sequestration in \\ndeep saline aquifers using artificial neural network. Appl. Energy 185, 916 \\u2013928. \\nhttps://doi.org/10.10 16/j.apenergy.2016.10.012  \\nMa, S., Zhang, Y., Lv, J., Ge, Y., Yang, H., Li, L., 2020. Big data driven predictive production \\nplanning for energy -intensive manufacturing industries. Energy 211, 118320. \\nhttps://doi.org/10.1016/j.energy.2020.118320  \\nSong, Y., Su ng, W., Jang, Y., Jung, W., 2020. Application of an artificial neural network in predicting \\nthe effectiveness of trapping mechanisms on CO2 sequestration in saline aquifers. Int. J. Greenh. \\nGas Control 98, 103042. https://doi.org/10.1016/j.ijggc.2020.10304 2 \\nSun, X., Bi, Y., Guo, Y., Ghadiri, M., Mohammadinia, S., 2021. CO2 geo -sequestration modeling \\nstudy for contact angle estimation in ternary systems of brine , CO 2 , and mineral. J. Clean. \\nProd. 283. https://doi.org/10.1016/j.jclepro.2020.124662  \\nVo Thanh , H., Lee, K. -K., 2021. Application of machine learning to predict CO2 trapping \\nperformance in deep saline aquifers. Energy 122457. \\nhttps://doi.org/10.1016/j.energy.2021.122457  \\nVo Thanh, H., Sugai, Y., Sasaki, K., 2020. Application of artificial neural net work for predicting the \\nperformance of CO2 enhanced oil recovery and storage in residual oil zones. Sci. Rep. 10, \\n18204. https://doi.org/10.1038/s41598 -020-73931 -2 \\nZhang, J., Feng, Q., Zhang, X., Shu, C., Wang, S., Wu, K., 2020. A Supervised Learning Appro ach \\nfor Accurate Modeling of CO 2 \\u2212 Brine Interfacial Tension with Application in Identifying the \\nOptimum Sequestration Depth in Saline Aquifers. \\nhttps://doi.org/10.1021/acs.energyfuels.0c00846\"}\n",
      "{\"text\": \"\\nMissing Seismic Trace Estimation using G enerative Adversarial Network: Image -to-Image \\nTranslation Method  \\n \\nIntroduction  \\n \\nGood quality seismic data is one of the key ele ments that contributes to better subsurface \\ncharacterization over the course of a field\\u2019s life cycle. However, seismic data are typically distributed \\nsparsely or are often incomplete due to several factors such as obstruction due to constructions and oil \\nand gas platforms, constrained seismic survey design due to economics (Fu et al., 2018) or wipeout \\nzones on seismic that is caused by shallow gas. Missing and incomplete seismic traces often poses \\nchallenges to multi -trace processing algorithms in particula r and degrades processing that will impact \\nseismic interpretation and reservoir characterizations.  \\n \\nWith the advancement of processing capacity in recent years, Artificial Intelligence (AI) has sparked \\nsignificant interest and has been extensively applied to  address numerous problems in the field of \\ngeosciences. We have seen accelerating efforts in deep learning application to the prediction of missing \\nseismic traces that has exceeded the traditional interpolation techniques. These deep learning methods \\ndirec tly employ seismic data for training and learns how to interpolate as compared to the conventional \\ninterpolation techniques (Li et al., 2022). Wang et al. applied one of the deep learning methods known \\nas Convolutional Autoencoder (CAE) (Wang et al., 2020)  for the missing traces prediction.  \\n \\nThe performance of CAE, on the other hand, deteriorates as the number of traces that are missing \\nincreases. We introduce Image -to-image translation (I2I translation), a novel appr oach that has been \\napplied in this study to address this problem. The I2I approach uses a Generative Adversarial Network \\n(GAN) as the backbone of the method. In contrast to CAE, I2I helps the model to make a prediction \\nand enable the model to assess how cl osely the prediction matches the actual data by utilizing a \\ndiscriminator inside the model.  \\n \\nImage -to-Image (I2I) Translation Method  \\n \\nI2I establishes a zero -sum game between the generator G model and the discriminator D model, using \\nGAN as the algorithm's foundation. The g enerator G model attempts to generate a prediction image, \\nwhile the discriminator D model attempts to differentiate between the prediction and actual data. This \\nmethod aims to locate a Nash equilibrium between the generator G and discriminator D models (Pa ng \\net al., 2022). This objective can be written in the mathematical expression below:  \\n \\n\\u2112\\ud835\\udc50\\ud835\\udc3a\\ud835\\udc34\\ud835\\udc41 (\\ud835\\udc3a,\\ud835\\udc37)=\\ud835\\udd3c\\ud835\\udc65,\\ud835\\udc66[log\\ud835\\udc37 (\\ud835\\udc65,\\ud835\\udc66)]+\\ud835\\udd3c\\ud835\\udc65,\\ud835\\udc67[log (1\\u2212\\ud835\\udc37 (\\ud835\\udc65,\\ud835\\udc3a(\\ud835\\udc65,\\ud835\\udc67))] \\n \\nwhere the observed image x and random noise vector z learn to mapping output image y. Thus, G tries \\nto minimize this objective against an adversarial D that tries to maximize it (Isola et al., 2017).  \\n \\nThe I2I technique makes use of the U-Net architecture as the generator model in order to supply \\nadequate predic tion data for the purpose of deceiving the discriminator model  (Ronneberger et \\nal., 2015 ). U-Net is an architecture of a Convolutional Neural Network (CNN) that was built \\nexpressly for the purpose of making p redictions about each data point. The L1 or Mean \\nAbsolute Error loss function is what the U-Net makes use of when it comes time to evaluate \\nthe generator model. However, if you only apply the L1 loss,  the Generator will produce \\noutputs that are unclear and  will have problems with its ability to anticipate. As a result, it has \\nbeen suggested to use PatchGAN as a discriminator in order to provide an additional loss \\nfunction to the generator in order to improve the results of predictions  (Larsen et al., 2016 ).  \\n \\nPatchGAN  is utilized to ensure outcome of the Generator model may be evaluated at the scale \\nof patches as opposed to the complete seismic section as is done for the Discriminator. To \\n \\n \\n\\nevaluate each seismic section patch, the discriminator performs inspections. Conv olutionally \\nexecuting the PatchGAN throughout the seismic segment is done first, and then the resulting \\nbinary cross entropy  loss is averaged before being used to evaluate both the generator and \\ndiscriminator models. (see Figure 1).  \\n \\n \\nFigure 1  Image -to-Image Translation general workflow scheme.  \\n \\nDataset Preparation  \\n \\nThe dataset used for this study is 2D post -stack Penobscot seismic data, located in the Scotian Basin, to \\nthe north of Sable Island and offshore of Nova Scotia (Campbell et al., 2015). The seismic dataset is \\nconditioned by slicing up into a patch with dime nsions of 256 pixels by 256 pixels so that the network \\ncan easily identify the correlation between the input and output. This ensures a more effective training \\nprocess and will require fewer computational resources. Additionally, the seismic data needs to be \\nnormalized (data normalization) prior input into the model to ensure each input parameter or pixel has \\na data distribution that is consistent with itself. The process of normalizing the data involves taking the \\nmean and subtracting it from each data poi nt, then dividing the value that is obtained by the standard \\ndeviation. These speeds up the convergence process while the network is being trained.  \\n \\nThe generation of input and output for the training method of the model is the focus of the second phase. \\nAs shown in Figure 2, a portion of the original seismic traces is removed, and as an output, we \\nreconstruct the original seismic with the remaining seismic traces in their entirety. As a result, the I2I \\nmodel will learn the correlation and make an effort to e stimate the missing traces in order to bring them \\ninto line with the actual one. The model was made more reliable through the use of this research by \\nselecting every 25 increments of inline and crossline seismic data.  \\n \\n \\nInput (Trace Removal)Sample number\\nSample number\\nTrace number Trace numberOutput (Original Data)\\n \\n \\n\\nFigure 2 The step of generating input and output data for I2I model training process. Input data is \\nobtained by removing several seismic traces form original seismic data, while the output is the original \\nfull seismic data.  \\n \\nResult and Discussion  \\n \\nThe predicted results for the part with its traces removed can be observed in the right figure of Figure \\n3, which  is juxtaposed with the original seismic data on the left side of Figure 3. The outcomes of the \\npredicte d seismic section appear to be geological, and they have a high degree of similarity to the \\nprimary data in  general.  \\n \\n \\nFigure 3 Comparison between the original and predicted seismic data I2I mode within the red box \\nsection. Note that the fault is preserved,  and migration artifact is excluded in prediction result.  \\n \\nIt is interesting to note that geological  elements have been preserved quite well, such as the overall \\ndipping  angles of the reflectors and the significant fault that was present in the original data, and that \\nthese aspects  have been recreated by the forecast. In addition to this, it has been not ed that artifacts such \\nas migration  smiles are not included in the prediction. Figure 4 shows the amplitude spectrum of original \\nand predicted  that is superimposed to observe the consistency, and the cross -correlation coefficient \\nbetween the amplitude  of the signal.  \\n \\n \\nFigure 4  The amplitude spectrum analysis (left) and cross -correlation coefficient b etween original \\nseismic data and seismic trace (right). This analysis shows that the amplitude spectrum is highly \\npreserved, and the cross -correlation achieved overall higher than 0.8 cross -correlation coefficient.  \\nMigrationartifact\\nisexcludedInput PredictedSample number\\nSample numberFaultispreserved\\nOriginalSample number\\nTrace number\\n\\n \\n \\n\\n \\nIt can be observed from the amplitude spe ctrum plot that the projected seismic results show very good \\nconsistency to the actual data across the missing traces area. This indicates that the I2I translation \\napproach preserves the amplitude of the signal. The cross -correlation between the anticipate d and \\noriginal seismic data shows that average values over the deleted portion are more than 0.8. Based on \\nthe amplitude spectrum and the cross correlation coefficient between original seismic and the seismic \\ntrace, we can validate that the prediction is a cceptable and reliable for this dataset.  \\n \\nConc lusions \\n \\nIn this study, we introduced Image -to-Image (I2I) translation method that can be successfully applied \\nfor the prediction of missing seismic traces in post -stack 2D seismic data. With a cross -correlation \\ncoefficient t hat is larger than 0.8 and an amplitude spectrum that is substantially conserved, the I2I \\nmodel can accurately estimate the missing trace on the Penobscot seismic data with approximately 25 \\nminutes of training time or 1500 epochs of data to use as a basis.  In addition, the findings of the forecast \\nkept the fault delineation while removing the artifact impact that was present in the initial data. This \\nwas achieved by maintaining the fault delineation. Furthermore, the pre -trained model can be used to \\ninterpo late seismic data with similar geomorphological structures through transfer learning , a machine \\nlearning approach that allows to reapply the knowledge from the model to a different dataset . We \\nconclude that I2I translation approach is applicable to the recovery of seismic data with missing traces \\nand further studies are being carried out to test the applicat ion on 3D seismic volumes.   \\n \\nReferences  \\n \\nCampbell, T. J., Richards, F. W. B., Silva, R. L., Wach, G., and Eliuk, L. [2015]. Interpretation of the \\nPenobscot 3D seismic volume using constrained sparse spike inversion, Sable sub -Basin, offshore \\nNova Scotia. Marine and Petroleum Geology , 68. https:// doi.org/10.1016/j.marpetgeo.2015.08.009  \\n \\nFu, L., Zhang, M., Liu, Z., and Li, H. [2018]. Reconstruction of seismic data with missing traces using \\nnormalized Gaussian weighted filter. Journal of Geophysics and Engineering , 15(5). \\nhttps://doi.org/10.1088/1742 -2140/aac31c  \\n \\nIsola, P., Zhu, J. Y., Zhou, T., and Efros, A. A. [2017]. Image -to-image translation with conditional \\nadversarial networks. Proceedings - 30th IEEE Conference on Computer Vision and Pattern \\nRecognition, CVPR 2017 , 2017 -January . https://doi.or g/10.1109/CVPR.2017.632  \\n \\nLarsen, A. B. L., S\\u00f8nderby, S. K., Larochelle, H., and Winther, O. [2016]. Autoencoding beyond \\npixels using a learned similarity metric. 33rd International Conference on Machine Learning, ICML \\n2016 , 4. \\n \\nLi, X., Wu, B., Zhu, X., and Yang, H. [2022]. Consecutively Missing Seismic Data Interpolation \\nBased on Coordinate Attention Unet. IEEE Geoscience and Remote Sensing Letters , 19. \\nhttps://doi.org/10.1109/LGRS.2021.3128511  \\n \\nPang, Y., Lin, J., Qin, T., and Ch en, Z. [2022]. Image -to-Image Translation: Methods and \\nApplications. IEEE Transactions on Multimedia , Vol. 24. \\nhttps://doi.org/10.1109/TMM.2021.3109419  \\n \\nRonneberger, O., Fischer, P., and Brox, T. [2015]. Unet. MICCAI2015 . https://doi.org/10.1007/978 -3-\\n319-24574 -4_28  \\n \\nWang, Y., Wang, B., Tu, N., and Geng, J. [2020]. Seismic trace interpolation for irregularly spatial \\nsampled data using convolutional autoencoder. Geophysics , 85(2). https://doi.org/10.1190/geo2018 -\\n0699.1\"}\n"
     ]
    }
   ],
   "source": [
    "!head /workspace/data/raw/documents.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning raw documents with NeMo Curator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: \n",
      "Text cleaning and language filtering\n",
      "\n",
      "Takes as input a directory consisting of .jsonl files with one\n",
      "document per line and outputs to a separate directory the text\n",
      "with fixed unicode. Also, performs language filtering using\n",
      "the 'language' field within each JSON object.\n",
      "\n",
      "       [-h] [--input-data-dir INPUT_DATA_DIR]\n",
      "       [--input-local-data-dir INPUT_LOCAL_DATA_DIR] --output-clean-dir\n",
      "       OUTPUT_CLEAN_DIR\n",
      "       [--output-removed-document-dir OUTPUT_REMOVED_DOCUMENT_DIR]\n",
      "       [--log-dir LOG_DIR] [--min-document-length MIN_DOCUMENT_LENGTH]\n",
      "       [--output-language OUTPUT_LANGUAGE] [--cpus-per-node CPUS_PER_NODE]\n",
      "\n",
      "options:\n",
      "  -h, --help            show this help message and exit\n",
      "  --input-data-dir INPUT_DATA_DIR\n",
      "                        Input directory consisting of .jsonl files that are\n",
      "                        accessible to all nodes. Use this for a distributed\n",
      "                        file system (default: None)\n",
      "  --input-local-data-dir INPUT_LOCAL_DATA_DIR\n",
      "                        Input directory consisting of .jsonl files. Use this\n",
      "                        argument when a distributed file system is not\n",
      "                        available. (default: None)\n",
      "  --output-clean-dir OUTPUT_CLEAN_DIR\n",
      "                        The output directory to where the cleaned jsonl files\n",
      "                        will be written (default: None)\n",
      "  --output-removed-document-dir OUTPUT_REMOVED_DOCUMENT_DIR\n",
      "                        An optional argument for writing the files not kept\n",
      "                        during the text cleaning and language filtering\n",
      "                        process. Used mostly for QC purposes (default: None)\n",
      "  --log-dir LOG_DIR     The output log directory where node and local ranks\n",
      "                        will write their respective log files (default:\n",
      "                        ./log/text_cleaning)\n",
      "  --min-document-length MIN_DOCUMENT_LENGTH\n",
      "                        Documents with fewer than min-document-length number\n",
      "                        of characters will be discarded (or written to the\n",
      "                        output-removed-document-dir) (default: -1)\n",
      "  --output-language OUTPUT_LANGUAGE\n",
      "                        An optional argument to clean only one language. If\n",
      "                        not specified, then all languages are preserved\n",
      "                        (default: None)\n",
      "  --cpus-per-node CPUS_PER_NODE\n",
      "                        The number of CPUs to use on each node. This can be\n",
      "                        used in conjunction with mpirun/srun scheduler to\n",
      "                        customize the MPI ranks vs local processes launched\n",
      "                        during a job (default: 256)\n"
     ]
    }
   ],
   "source": [
    "!text_cleaning --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "!text_cleaning --input-data-dir /workspace/data/raw --output-clean-dir /workspace/data/clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"text\": \"Multisensor UAS testing to support avalanche forecasting and monitoring  \\n \\nIntroduction  \\n Snow avalanches pose a major natural hazard in Arctic and Alpine countries. Transport networks in \\nNorway are especially vulnerable and for this reason, the GEOSFAIR (GEO hazard Survey F rom the \\nAIR) research project is investigating how uncrewed aerial systems (UAS) can help to forecast and \\nmonitor roadside avalanches and other natural hazards.  The main objective of the project is  to develop \\nmethodologies and workflows  for UAS data collect ion in the Norwegian Public Road s Administration  \\n(NPRA)  decision support system , for faster and better assessment of roadside  avalanche hazard s.  \\n \\nRemote sensing provides good spatial coverage to improve avalanche monitoring, from ground, air or \\nspace and using optical, laser or radar sensors (Eckerstorfer  et al., 2016). UAS  remote sensing and \\nsurface mapping has been  used for improving avalanche forecasting using either LiDAR (Light \\nDetection and Ranging)  mapping techniques (Chrustek and Wezyk, 2009) or SfM ( Structure -from -\\nMotion) from optical cameras  (Adams et al., 2018 ), or both (Harder et al., 2020) . Other UAS -based \\nremote sensors that are being tested includ e multispectral cameras  (Maier et al., 2022)  and thermal \\ncameras (Steinkogler et al., 2015) . Subsurface mapping using ground penetrating radar  (GPR)  mounted \\non drones has been given less attention so far, and  outcomes can be ambiguous  (Jenssen  et al., 2019, \\nValence et al., 2022 ).   \\n \\nWithin the aforementioned GEOSFAIR project, several UAS sensor s are being evaluated , including \\nLiDAR, optical, and GPR sensors . This abstract reports on an extensive field test  that was carried out \\nin Grasdalen, Norway, in April 2022. W e present the results of the test , which utilized data from several \\nUAS sensor s, and which were validated against ground truth data.  \\n Methods  \\n \\nThe field test was conducted at NGI s avalanche research station Fonnbu at Stryn efjellet (Vestland \\ncounty, Norway). The area is well instrumented and has been used for avalanche research since 1973 . \\nThe terrain around the station  is steep and varied , reaching from 875 to 1600 m  asl. The area is \\ncharacterized by a  coastal climate allowing for large amount s of precipitation and large influence of \\nsnow wind transport  on avalanche risk . The surveys last ed for five days, and the weather was mostly \\ncalm , with  good visibility allowing for safe drone flights. Air temperatures  at the research station (938 \\nm asl) ranged from -16.5 to +3.4°C  during the test . A short precipitation  event occurred o n the first day \\nwith a little snow  (3-4 cm).   \\n Fifteen  snow observations  were recorded  in snow  pits at different locations over the week  to correlate \\ndrone measurements with ground truth data. Snow  height above terrain varie d from 2.5 m at Fonnbu \\nlocation to 12- 15 m height  in cornice  areas.  In addition, nine  ground  control points (GCPs) were placed \\nand georeferenced in order to calibrate and validate drone surface measurements. Different m ultirotor  \\nand VTOL  (Vertical Take- Off and Landing)  drones were  tested carrying various paylo ad sensors that \\ncollect ed optical (RGB and multispectral) images , thermal (radiometric infrared ) images, \\nphotogrammetry models , LiDAR  point clouds  and GPR  profiles . \\n Results  \\n LiDAR surveys were carried out with a DJI Zenmuse  L1 sensor mounted on a DJI Matrice 300 RTK \\ndrone. The data sets were recorded by flying at 60 m above ground level  altitude, with a terrain -\\nfollowing function, 7 m/s flight speed, 50 % sideways overlap, and dual return mode. A repetitive scan \\npattern was used, and the sampling rate varie d between 240 000 and 120 000 pts/s. The LiDAR data \\nwas pre-processed in the DJI Terra software and additional editing, and va lidation of the point clouds \\nwas completed in CloudCompare and ArcGIS Pro software.  \\nNumerous photogrammetry surveys were also carried out using DJI Mavic 2 Pro, DJI Phantom 4 RTK, \\nVTOL Quantum Systems Trinity F90+ with Sony RX1RII camera and DJI Matrice 300 RTK with P1  \\n \\n \\ncamera. Testing under different light conditions  show ed that the SfM reconstructions varied in quality \\nand were  more difficult when  light conditions were extremely bright and with fresh snow (uniform, \\nlittle texture) . Two major types of survey missions were conducted, including altitude -locked, double -\\ngridline missions and terrain- following (altitude varying) missions. Examples of d igital elevation \\nmodels derived from LiDAR and photogrammetry reconstruction techniques are displayed in Figure 1. \\nSnow height  values  were derived by subtracting  the national airborne LiDAR  terrain model (with 1  m \\nresolution ) from the snow surface  models . \\n \\n \\nFigure 1  Examples of LiDAR ( top) and photogrammetry ( bottom ) results from the test . The LiDAR \\nresult  shows the snow height  (Nor. \\\"snødy bde\\\")  draped on hillshaded snow surface. The \\nphotogrammetry result  shows an excerpt of snow height in a  roadside  avalanche release area.  \\n\\n \\n \\nSeveral drone GPR measurements were also carried out, to test the ability to fly in steep mountainous \\nterrain and to determine the best flight altitude. The GPR system was a light pulse shielded antenna \\nZond Aero from Radsys with a central frequency of 1 GHz. The GPR was mounted on a DJI Matrice \\n300 RTK, combined with a radar altimeter and terrain- following algorithm to allow flights close to the \\nterrain. Figure 2 (top) shows an example of a GPR profile recorded along a steep slope. Tests at flight \\naltitude s between 1.5 and 5 m showed that the best compromise between data quality and flight safety \\nwas between 2 and 3 m. The GPR data profile enabled identification of the snow surface and the \\ninterface between snow and ground. This interface was not linear as the ground surface was composed \\nof rock boulders and strong hyperbola diffractions were visible. Internal snow layers were also visible and can be correlated to our own snow pit measurements.  \\n \\nFigure 2 Examples of r esults of  drone GPR survey (top), handheld thermal camera imaging of snow \\npits (bottom left) and drone multispectral survey (bottom right).  \\n \\nAdditional sensors were tested, including thermal and multispectral imaging cameras. Figure 2 (bottom \\nleft) display s results o f testing a handhel d thermal camera (Fluke Ti -400 series) to measure snow surface \\ntemperatures. Results correlated well with temperatures measured using a conventional thermometer . \\nSimilar  airborne  measurements were carried out using a DJI Matric 300 RTK and H20T thermal camera  \\npayload. Another set of data was recorded using  a multispectral camera ( Micasense  RedEdge MX) both \\nhandheld and from a DJI Matrice 300 RTK . Captured imagery was processed with Pix4DFields . Figure \\n2 (bottom right ) depicts  a visualization of the MCARI index, which is a standard metric  of chlorophyll \\nabsorption used for agricultural purposes  and showed here for illustration. The 10 different channels of \\nthe multispectral cameras (444 to 842 nm cent re wavelength) should, in theory, enable  discrimination \\nof different  snow grain types at the surface  of the snow pack . \\n\\n \\n \\n \\nConclusion  \\n We present a summary of an extensive field test  that had the overall goal to test different UAS  and \\nsensor payload s and to evaluate  the usefulness of derived data for  support ing the NPRA in operational \\ndecisions related to snow avalanche hazard . The  tested  technologies  provide d information on snow \\nsurface  characteristics , snow heights and snow pack properties. Integration of the different sensors for \\nspecific measurements and avalanche problems is part of on- going work.  \\n \\nAcknowledgement  \\n \\nThis work is part of the GEOSFAIR project funded by the NPRA and the Research Council of Norway \\n(Innovation Project for the Public Sector, grant no. 321035) , with in -kind contributions from NGI and \\nSINTEF.  \\n References  \\n Adams, M.S., Bühler, Y. and Fromm, R., 2018. Multitemporal accuracy and precision assessment of  \\nunmanned aerial system photogrammetry for slope -scale snow depth maps in Alpine  \\nterrain.  Pure and Applied Geophysics , 175(9), pp.3303- 3324.  \\nChrustek, P., Wezyk, P., 2009. Using high resolution LiDAR data to estimate potential avalanche  \\nrelease areas on the example of Polish mountain regions. Proc. International Snow Science  \\nWorkshop, 2009, Davos, Switzerland.  \\nEckerstorfer, M., Bühler, Y., Frauenfelder, R. and Malnes, E., 2016. Remote sensing of snow  \\navalanches: Recent advances, potential, and limitations.  Cold Regions Science and \\nTechnology , 121, pp.126- 140. \\nHarder, P., et al. 2020. Improving sub- canopy snow depth mapping with unmanned aerial vehicles:  \\nLiDAR  versus structure- from -motion techniques. The Cryosphere, 14(6): 1919–1935  \\nMaier, K., Nascetti, A., Van Pelt, W. and Rosqvist, G., 2022. Direct photogrammetry with  \\nmultispectral imagery for UAV -based snow depth estimation.  ISPRS Journal of  \\nPhotogrammetry and Remote Sensing, 186, pp.1- 18. \\nJenssen, R.O.R., Eckerstorfer, M. and Jacobsen, S., 2019. Drone -mounted ultrawideband radar for  \\nretrieval of snowpack properties. IEEE Transactions on Instrumentation and  \\nMeasurement , 69(1), pp.221- 230. \\nSteinkogler, W.; Sovilla, B. & Lehning, M., 2015. Thermal energy in dry snow avalanches. The  \\nCryosphere, 9, pp.1819- 1830 \\nValence, E., Baraer, M., Rosa, E., Barbecot, F. and Monty, C., 2022. Drone -based ground- penetrating  \\nradar (GPR) application to snow hydrology. The Cryosphere, 16(9), pp.3843- 3860.\"}\n",
      "{\"text\": \"\\nUsing full waveform inversion with limited -offset seismic data to improve pre -salt imaging in \\nthe Kwanza basin  \\n \\nIntroduction  \\n \\nTypically, seismic imaging in a complex salt environment requires rich azimuthal coverage and long \\noffsets to illuminate the subsurface, an accurate velocity model and imaging algorithm (Kang et al., \\n2021 ). Moreover, g eometry of the salt , in offshore Angola , has variable complexity, and the majority \\nof the data acquired to date is narrow -azimuth (NAZ) towed stream er.  Based on previous s tudies  \\nperformed in this area , reliable subsalt images can be achieved with NAZ data, but an accurate velocity \\nmodel is critical to success  (Shmelev et al., 2016 ; Branston et al., 2022). Herein we demonstrate the \\nrejuvenation  of multi -survey NAZ  seismic data by applying broadband signal processing and focusing \\non velocity model building and depth imaging, using new developments in  full-waveform inversion \\n(FWI) such as enhanced template -matching (ETM -FWI) automation. It resulte d in the reduction of  \\nstructural uncertainties , and improved  understanding  of the depositional context at pre -salt targets , \\nmigration pathways  and trapping styles.  We show how improvements in model building contribute to \\na significantly improved image of t he legacy seismic data . \\nMore than 4.2 billion barrels of oil equivalent  has been discovered , such as Cameia, Mavinga, Orca , in \\npre-salt play  over the outer Kwanza basin covering blocks 20 and 21 (Figure 1). Carbonate mounds \\n(Aptian Microbialites)  which are developed on the basement highs  overlain by salt  are the key targets \\nin the deep -water . Most of the exploration wells drilled so far had seismic imaging as one of the key \\ndeterrents in successfully mapping pre -salt carbonate reserv oirs and predicting accurate reservoir \\nproperties.  Distortion and poor focussing of the seismic image associated with unresolved overburden \\nheterogeneity complicates the understanding of deeper structure and fracture delineation.  \\nFigure 1 Regional seismic cross -section showing hydrocarbon discoveries in Kwanza basin  \\nThe combination of uncertainty in the velocity model and strong contrasts associated with salt and \\ncarbonates make it challenging for FWI to converge when classic objec tive function s are used.  Together \\nwith the acquisition geometry limitations, we identified the challenges in the target area with input data \\ncomprising three neighbouring surveys with limited overlap. Two surveys were acquired with 4800 m \\nstreamers towed at 7  m depth with sources towed at 5 m depth . The third orthogonal survey was acquired \\nwith a maximum offset of 8000 m, an 8 m cable depth and source at 6  m depth . A spatially variant high -\\nresolution anisotropic model was built by combining common -image -point ( CIP) tomography \\n(Woodward et al., 2008) to refine  the model prior to FWI, enhanced template -matching FWI (ETM -\\nFWI) (Vigh et al., 2020a; Kang et al., 2020) to update sediment, salt, and car bonate velocities \\nsimultaneously. CIP  tomography  was used again below salt. Integrating these technologies improved \\nimaging of the key target , despite the limitation s of the legacy dataset . Additionally, ETM -FWI enabled \\nuse of the full record to provide greater depth of penetration, updating the model globally without \\ndifferentiating speci fic lithologies and eliminating  the need to carry out interpretation -based salt \\ngeometry scenario  testing . The signal processing included 3D multi -cable adaptive  deghosting to \\n\\n \\n \\n\\nattenuate source and receiver ghost effects and broaden  the useable frequency content of the input data . \\nThis was preceded by noise attenuation and followed by multiple attenuation workflows .   \\nMethod  \\n \\nThe multistage  velocity model building (VMB)  workflow can be broken into three  main stages : Initial \\nmodel building, post-salt and salt VMB , and pre-salt VMB.   \\n \\nThe l egacy velocity model consisted of a series of individual tilted -transverse isot ropic (TTI) models \\nfor each of the  adjacent  surveys . The legacy  model building was  driven mostly by multiscale CIP \\ntomography  with geologic constraints (Zdraveva et al., 20 13) to update the post- and pre -salt sediment  \\nregions of the model . The salt structure  was introduced by iterative sediment  and salt flood  scenarios  to \\ninterpret surfaces representing top and base of salt . The starting Vp model for the reimaging was \\nobtained by merging these legacy model s. To help remov e the short scale length  variations  and the \\ninconsiste ncy of salt/sediment models between surveys , the salt velocities  were removed  before \\nstructurally smooth ing the single sediment model . In parallel, anisotropy functions  were d erived  using \\nprevious experience in the area  (Zdraveva et al. , 2011 ) and available  well data. Kirchhoff depth  \\nmigration  (KDM) , forward modelling  and 1D ray -tracing analysis  were used throughout to validate  the \\nmodel building stages.  \\nInput model to FWI : FWI convergence issue s occur  when there is a large difference between the \\nrecorded  and modelled seismic waveforms, therefore the quality of the starting model is crucial. To \\naddress this,  the low -wavenumber velocity component was updated through two iterations of CIP \\ntomography, focused on the post -salt section, with  a priori geological information to constrain the shape \\nof the model update. The resultant model was used to build the salt model; top salt was interpreted using \\nthe second tomography image, followed by a salt flood with a constant velocity of 4500 m/s drive n \\nfrom well data and finally the base salt was interpreted using the salt flood image. The smoothed  legacy \\nvelocity was inserted below base salt honouring the depth change due to the changing overburden \\nsediment and salt geometry. Once all zones were put t ogether, a smoothing was applied to remove the \\nsalt boundary to allow ETM -FWI upda tes to refine the salt geometry .  \\nPost salt and salt model building : ETM -FWI with automatic decision making (Halliday et al., 2022) \\nwas used to update the entire velocity  field simultaneously , avoiding 'locked in ' errors associated with \\na layer -by-layer top -down approach.  The approach depends on matching the local patterns between the \\nrecorded and modelled full shot record, decomposing the wavefield into its directionality (dip), phase, \\nand frequency. It then  classifies events according to criteria for adjustive FWI (Jiao et al., 2015) and \\nleast-squares FWI, providing additional measures to dete rmine  which objective function to use without \\nmanual intervention. This occurs for each iteration, and output QC maps indicate the model update \\ndirection. Utilising ETM -FWI with automatic decision making ensure s robust  matching  (Figure 2) , even \\nin the pres ence of strong velocity contrasts .  \\n \\nFigure 2 Interleaved shot QC . (a) Raw shots interleaved with modelled shots (blue shaded)  using \\ninput model to FWI . (b) Interleaved shot QC after ETM -FWI.  \\n\\n \\n \\n\\nResults  \\n \\nThe results demonstrated that this work flow was able to resolve velocity heterogeneity  (Figure 3) by \\nsimultaneously updat e sediment, salt, and carbonate layers .  \\n \\n \\nFigure 3 Legacy velocity model overlaid on the legacy KDM image  (left) versus  new velocity model \\noverlaid on the new  reprocessed KDM image  (right) .  \\n \\nFigure 4 shows the uplift observed in the new seismic image compared to the legacy image. The image \\nusing the new model shows significant uplift around salt flanks and b ase-salt allowing for better \\nunderstanding the syn -rift system located pre -salt and resulting in increased confidence .  \\n \\n \\nFigure 4 (a) legacy KDM image using legacy velocity model , and  (b) reprocessed KDM  image with the \\nnew velocity model . The green  arrows  on image ( b) show improved  defin ition of salt geometry  (arrows \\n1 and 2) , clear  base salt reflector  (arrow 3) and improved imaging of the pre salt reservoir section \\n(arrow 4) ; compared to areas indicated by the red arrows on image (a).   \\n  \\n  \\n\\n \\n \\n\\nConclusion  \\nWe demonstrated a successful application of a velocity model building workflow employing ETM -FWI \\nto use the full shot record to  update post-salt and salt velocities simultaneously, using an offset -limited , \\nnarrow -azimut h legacy dataset . This approach has accurately determined the  contrasting carbonate and \\nanhydrite layers surrounding the salt, simplifying and improv ing continuity and focusing of the base of \\nsalt and pre -salt reflectors.  This has reduced geological uncertaint ies and helped to better image  pre-\\nsalt targets and  predict reservoir properties . \\n \\nAcknowledgements  \\n \\nWe thank SLB multiclient and Agência Nacional de Petróleo, Gás e Biocombustíveis (ANPG) for \\npermission to present the results. We also  thank Olga Zdraveva, Suganda Tewari, Claire Field, and \\nShipra Mahat.  \\n \\nReferences  \\n \\nBranston, M., Chapelle, M., Rathee, D., Ramsumair, R., Thompson, J., & Campbell, R. [2022] What is \\nthe Right Level of Acquisition Effort to Enable Successful, Cost -Effectiv e Exploration? 83rd EAGE \\nAnnual Conference & Exhibition,  Vol. 2022, 1 -5. \\n \\nHalliday, D., Bloor, R., Cheng, X. and Elbadry, M. [2022] Automated Decision Making for Full -\\nWaveform Inversion. 83rd EAGE Annual Conference & Exhibition,  Vol. 2022, 1 -5). \\n \\nKang, W.,  Brand, N., Cheng, X., Vigh, D., Seymour, N. and Ishak, M., 2021, October. Reliability, \\nEfficiency, and Robustness: Enhanced Template -Matching Full -Waveform Inversion Using Sparse \\nOcean -Bottom Node Data. In  82nd EAGE Annual Conference & Exhibition  (Vol. 2021, No. 1, pp. 1 -\\n5). European Association of Geoscientists & Engineers.  \\n  \\nKang, W., Reisdorf, A., Madden, S., Robertson, V., Chen, D., Chen, Z., Xu, J., Cheng, X. and Vigh, D. \\n[2020] Salt Reshaping with Template -Matching Full -Waveform Inversion. 82nd EAG E Annual \\nConference & Exhibition Workshop Programme, Volume 2020, 1 – 5. \\n \\nShmelev, A.A., Cooke, A., Zdraveva, O. and Penwarden, J. [2016] High Resolution Model Building  \\nand Broadband Imaging in Deep Water Offshore Angola.  78th EAGE Annual Conference and \\nExhibition, Volume 2016, 1 -5. \\n \\nVigh, D., Cheng, X. and Kang, W. [2020a] Full Waveform Inversion with OBN data collection via \\nMad Dog examples. 90th Annual International Meeting, SEG, Post -convention workshop.  \\n \\nWoodward, M.J., Nichols, D., Zd raveva, O., Whitfield, P. and Johns, T. [2008] A decade of \\ntomography. Geophysics , 73, no. 5, VE5 -VE11.  \\n \\nZdraveva, O., Hydal, S. and Woodward , M. [2013]  Tomography with geological constraints: An \\nalternative solution for resolving carbonates. 83rd Annual International Meeting, SEG , Expanded \\nAbstracts, 4770 –4774.  \\nZdraveva, O., Woodward, M., Nichols, D. and Osypov, K. [ 2011]  Building Anisotropic Models for \\nDepth Imaging: Comparing different approaches. 12th International Congress of the Brazilian \\nGeophysical Society, Extended Abstracts.\"}\n",
      "{\"text\": \"\\nSurface distributed acoustic sensing  (S-DAS)  for high resolution near surface characterization . \\n \\nIntro duction  \\n \\nIn the last few years , experiments on t he use of distributed acoustic sensing (DAS) using fibre  optics as \\nseismic sensors deployed on the surface has been performed by academia , oil and gas  operators, \\nconsortia,  and service providers  (Bakulin et al ., 2020) . While routinely  geophysical application s have \\nbeen mainly commerciali sed in the borehole environment , when deployed on the surface, surface \\ndistributed acoustic sensing (S -DAS) configuration s pose new challenges due to the peculiarity  of the \\nreceiver response and specifics of DAS data.  \\n \\nIn this paper, we describe the design and acquisition of a large -scale  3D land field test and processing \\nof the S -DAS data . We focus  on the latest advancement in the use of surface -wave analysis and \\ninversion and its application to S -DAS recording  through comparison and validation of co -located \\nmulticomponent (3C) geophones  and conventional high-density surface seismic nodal acquisitio n. \\n \\nWe show the contribution of multi -modal dispersion curves that leads to a multi -modal high -resolution \\nsurface wave inversion  that has been validated with the sub -surface profiling from non -seismic \\nmeasurement s such as  resistivity acquisition s (i.e., electrical resistivity tomography ). \\n \\nSurface DAS experiment  \\n \\nIn January 2022 , SLB WesternGeco Multiclient  completed the acquisition of a geophysical program in \\nthe Yoakum  County in West Texas, USA  (Bachrach et al ., 2022) . The Greenfield project comprises : \\n \\n▪ High-density surface seismic acquisition , acquired with a fleet of  high-productivity heavy \\nvibrator s in slip -sweep mode and nodal geophones in orthogonal shooting layout . \\n▪ 2D resistivity profiles in dipole -dipole  configuration . \\n▪ 2D line of passive vertical geophone s. \\n▪ A 3D layout  of 11,300 ft (3444 m) of optical fibre  cable  to record  S-DAS seismic data . \\n▪ 2D line of 3-component sensor s (3C)  spaced every  10 ft (3 m) and deployed close and parallel to \\na long segment of the fibre  (see green line in Figure  1). \\n▪ A dedicated sweep design and source layout , designed and acquired specifically for the S-DAS \\nexperiment.  \\n \\n \\nFigure 1  Survey design and trenching profile for the Greenfield 3D S -DAS experiment.  On the sketch \\non the left in blue the planned layout for the 11 ,300 ft  (3444 m)  of fiber optic cable serpentine, in green \\nthe co -located deployment of 3C multicomponent sensors; in red the location of the  planned  1700 shots . \\n \\nA cable  with six optical fibres  has been trenched 1 ft (30 cm) deep in the loose , loamy soil of the West \\nTexas cornfield  and following a serpentine shape as visualized in Figure 1 in blue . \\n\\n \\n \\n\\nA heterodyne interrogator has been connected to a high -volume  data storage equipped with 260  TB of \\ndisk space ready to accommodate the expected large data volume (up to 4  TB/h). \\nAt the end of cable , the six optical fibres  have been connected in pairs on a back -loop configuration \\nallowing the light signal to travel twice through th e cable doubling the recording length to 22 ,600 ft \\n(6888 m) . The interrogator has been switched on twice:  \\n \\n▪ Phase 1: to liste n for two days to the fleet of vibroseis in slip -sweep mode from the high -\\nproductivity configuration simultaneously to the surface -seismic survey . \\n▪  Phase 2: in a much quieter  environment after the completion of the surface -seismic operation \\nwith a dedicated  source pattern (see Fig ure 1 in red) . The locations  of the  source s have been \\ndesigned to account for  the sensitivity of the fibre  cable, that favour s incident angles of \\nwavefronts parallel to the cable axis . \\n \\nS-DAS data  example  \\n \\nAn example of  a raw correlated seismic shot recorded for the entire 22,600 ft (6888 m) length of the \\nfibre  during the Phase 2 acquisition  is shown in Figure 2. The location of the shot is shown in green in \\nthe geometry layout sketch. The raw optical data have been realized w ith a spatial sampling of 10  ft (3 \\nm) and gauge  length  (GL)  of 20  ft (6 m) . In Figure 2, the locat ion of the shot is  very close to the first \\nturn of the cable, correspond ing to the point at which the cable from the segment on the surface is \\nburied. The strong dominating energy component clearly visible in the seismic record is ground -roll: \\nsurface -waves that travel cylindrically starting from the source point  with an elliptic particle motion \\ncontained in the  vertical plane (characterized by a vertical and a horizontal component ).  \\n \\n \\n \\nFigure  2 Single correlated shot  for the entire 22,600 ft (6888 m) of the recording fibre . The raw optical \\ndata have been realized with a spatial sampling of 10  ft (3 m)  and gauge  length of 20  ft (6 m) . Note the \\nmirroring effect caused by the back -loop configuration after the two fibres  inside the cable have been \\nspliced together , thus  doubling  the record ing active fibre . \\n \\nClosely observing the central part of the seismogram, it is noticeable that the dimming of the surface \\nwave amplitude corresponds to the portion of the fibre that lays orthogonally to the main direction of \\npropagation of the surface waves. This phenomenon is expected and peculiar to the DAS cable due to \\nthe combination of the directional sensitivity of the fibre to incident angle (theta) of the wavefront \\n(which  is a function of cos^2(theta ); see Sayed et al ., 2020 ) and t he sensitivity to the horizontal \\ncomponent of the elliptic particle motion , which is recorded by the S -DAS cabl e. This makes  the fibre \\nalmost transparent to ground -roll that comes from the crossline direction.  \\n\\n \\n \\n\\n \\n \\nSurface -wave analysis and inver sion \\n \\nIn this paper we will focus on the analysis of the results from the high -resolution spectral analysis of \\nthe ground -roll energy recorded by the fibre  cable . Dispersion curves for multiple modal events  are \\nautomatically picked  in the frequency -wavenum ber (FK)  space for each realized station along the cable \\n(spatial sampling at 10  ft (3 m) ), and the shear wave velocity ( Vs) depth  profile  is computed as a result \\nof their inversion  (Strobbia, 2010).  The same surface -wave analysis and inversion workflow has been \\napplied for comparison and validation to the corresponding segment of the horizontal (X) component \\nof the 3C sensors oriented in the same direction of the fibre  (see Figure 3 on the right).  \\n \\n \\n \\nFigure 3 Surface -wave analysis and inversion from co -located segment of fibre  optic cable (left) and \\n3C-X component geophone (right). Both the segments are spatially sampled every 10  ft (3 m) . A single \\nshot gather  is shown on the top; the high -resolution FK semblance for a single station is shown in the \\ncentre  and Vs multimodal inversion profile is shown at the bottom.  \\n \\nA single segment of the raw correlated shot gather is shown in Figure 3. The S -DAS seismic signa l \\n(Figure 3 on the left) realized from the heterodyne system is , in this case , proportional to the strain that \\nhas been demonstrated to be proportional to the displacement. The same segment from the co -located \\n3C-X component (Figure 3 on the right) of the geophone is instead proportional to the velocity. This \\nexpresses as a higher sensitivity of the DAS system towards the low -end of the frequency spectrum \\ncompared to conventional geophones. Both the segments are spatially sampled every 10  ft (3 m) . \\n \\nThe spectral analysis and the picking of the dispersion curves has been automated through a machine -\\nlearning (ML) enabled picker on both the S -DAS and 3C -X data. The high -resolution FK semblance \\n(see central pics in Figure 3) for a co -located location show s a higher sensitivity of the DAS cable \\n\\n \\n \\n\\ncompared to the 3C -X for a broad range of the frequency spectra . For this comparison the dispersion \\ncurves have  been picked for the fundamental and the first mode: the result  from the inversion is shown \\nat the bottom  of Figure 3.  \\n \\nDiscussion and conclusions  \\n \\nThe S -DAS data acquired from the fibre  optic shows a higher sensitivity compared to conventional \\nvertical geophone  (Figure  4). The direct comparison with the multicomponent co -located nodes shows \\nthat S -DAS data c an be compared to a single horizontal component oriented along the cable axis, but \\nwith the advantage of a higher sensitivity to the ground -roll and a denser spatial sampling at a much \\nlower cost. The dispersion curves spectra from the S -DAS data have been  inverted leading to a velocity \\nprofile that exhibit s a high resolution spatially and in depth.  The correlation of the velocity models \\nobtained from the different acquired components, both seismic and non -seismic, provided the best \\navailable framework to u nderstand the complexity of the near surface . This provides opportunities for  \\nnovel applications in the investigation of shallow targets .  \\n \\n \\nFigure 4 Vs multimodal inversion profile from conventional nodes acquisition , ERT resistivity profile  \\nand S -DAS are compared on a collocated section.  The lithological interpretation from the ERT profile \\nis overlaid.  \\n \\nAcknowledgments  \\n \\nWe would like to thank SLB WesternGec o Multiclient  for permission to use the S -DAS data , OptiQ \\nSLB fibre  optic division , the SLB Cambridge SCR R&E department for the knowledge support and \\nDawson for the acquisition and logistic s in the field.  \\n \\nReferences  \\n \\nBachrach, R., Busanello, G., Sayed, A., 2022, Applications of surface distributed acoustic sensing f or \\nCO2 storage and windfarms: Results from large scale experiment, GET 2022  \\n \\nBakulin, A., Silvestrov, I and Pevzner, R., 2020, Surface seismic  with DAS: An emerging alternative \\nto modern point -sensor acquisition, The Leading Edge  (2020),  39(11): 808  \\n \\nSayed, A., Shujat, A and Stewart, R., 2020, Distributed acoustic sensing (DAS) to velocity transform \\nand its benefits, SEG International Exposition and 90th Annual Meeting.  \\n \\nStrobbia C., Vermeer, P.L., Laake A., Glushchenko, A. and Re S. [2010] Surface wav es: processing, \\ninversion and removal. First Break, Vol. 28, 85 -91.\"}\n",
      "{\"text\": \"84th EAGE Annual Conference & Exhibition \\nDeep-salt: 3D salt segmentation from inaccurate migrated subsurface offset gathers using \\nconvolutional LSTM layers \\n \\nIntroduction \\n \\n An accurate seismic image depends directly on the velocity model used in migration. The \\nconventional geophysics tools, such as Tomography and Full-waveform inversion(FWI), recover the \\nvelocity model with a satisfactory resolution; however, they can not recover salt inclusions with high \\nresolution. As a result, salt inclusion over the velocity model is often a manual and interpretative task, \\nrequiring many tests and iterations to find a reasonable salt geometry (Dellinger et al. 2017).   \\n Deep learning (DL) has emerged as an alternative to conventional methods for many geophysical \\napplications (Yu and Ma, 2021). Recent studies investigated DL to solve tasks related to velocity model \\nbuilding (VMB) (AlAli and Anifowose 2022), where it was observed that salt inclusions are particularly \\nwell delimited over the predicted velocity models (Klatt et al. 2022). In order to reduce the data size \\nand the complexity of the task, some approaches addressed only the task of salt inclusions. Two \\nexamples of applications focused on salt are the segmentation of the bottom of the salt from the FWI \\nresponse by AlAli et al. (2022) and the segmentation of the complete salt geometry from the subsurface \\noffset gathers migrated with the sediment velocity by Muller et al. (2022).  \\n Despite the successful results, previous salt inclusion guided by DL relied on 2D simulations and \\n2D DL architectures. The extension to 3D simulations requires careful DL design to avoid memory \\nissues on the GPUs. In this work, we propose to extend the salt segmentation methodology proposed \\nby Muller et al. (2022) to 3D data,  using as DL architecture a U-Net with convolutional LSTM layers \\nsimilar to the one proposed by Arbelle and Raviv (2019). We tested this approach over a synthetic data set \\nand showed that the DL model could segment 3D salt geometries.  \\n \\nData generation \\n \\n Our training/validation/test set consists of 30 synthetic velocity models with 2 km of depth, 4,5 \\nkm of extension in the inline direction, and 3 km in the crossline direction. The spatial sampling rate is \\nequal to 12,5 m for the three directions. We constructed the 3D velocity models using real salt \\ngeometries from velocity models built with tomography and FWI in regions with complex salt structures \\nand high exploration interest in the past decade. To increase the complexity and diversity of the training \\nset, we applied over those geometries a 3D augmentation algorithm (Solovyev et al. 2022), which \\nrotates, flips, and resizes the original salt geometries. After the augmentation process, we resized the \\nsalt geometry to fit into the desired size and included it on a simple sediment velocity background. To \\ncheck for the generalisation ability of our approach, we also used the SEG/EAGE  3D  salt model \\n(Aminzadeh et al. 1996) in our tests because it presents sediment velocities and structures different from \\nthe ones in our training set. Figure 1 (a) shows one of the 30 generated models from the training set and \\n(b) the 3D SEG/EAGE salt model for comparison. \\n \\n \\nFigure 1: (a) shows one example of the velocity models used for the training/validation process. (b) \\nshows the SEG/EAGE 3d salt model used to test the generalisation ability of the DL predictions. \\n\\n \\n \\n84th EAGE Annual Conference & Exhibition \\n We simulated synthetic seismic shots for each model using a finite-difference wave propagator, \\nisotropic, acoustic, with second-order in time, eight-order in space, and exponential attenuation on the \\nabsorbing boundaries. The acquisition geometry defines 4800 shots, with 50 meters of increment in \\ninline and crossline directions. The simulated receivers are ten streamers 2000m long, spread with a \\nmaximum lateral distance of 500m from the source. We linearly extrapolated the original cubes in order \\nto accommodate the acquisition geometry. \\n We migrated the simulated shots using the smoothed sediment velocity, without any salt \\ninclusions, using an RTM cross-correlation extended imaging condition (Sava and Fomel 2003). The \\nRTM generates cubes for each desired subsurface offset value, which will then be used as channels of \\nthe input image to the neural network, defining the 3D salt geometry prediction in a pipeline similar to \\nthe one proposed by Muller et al. (2022). This kind of prediction relies on the fact that when using \\nextended imaging conditions, reflections migrated with an accurate velocity focalise on the zero \\nsubsurface offset. Otherwise, reflections migrated with a wrong velocity spread for the higher \\nsubsurface offsets. Thus, it is expected that the top of salt and sediment reflections to focalise at zero \\nsubsurface offset, and flanks and the base of the salt energy to be spread to higher subsurface offsets. \\n \\nDL architecture \\n \\n The proposed DL pipeline relies on supervised learning. The inputs are the cubes of migrated \\nsubsurface offsets, with each value of subsurface offset corresponding to one channel of the input image. \\nThe outputs are the cubes with the segmented salt geometry. One possibility to deal with 3D data is to \\nuse a DL architecture with 3D convolutional blocks. However, despite being successfully used for 3D \\ngeophysical tasks, e.g. (Shi et al. 2019), 3D convolutional blocks require much memory and present a \\nlonger training time.  \\n One possible alternative to handle the spatial information, and reduce the computational cost, is \\nthe use of Convolutional LSTM (C-LSTM). C-LSTM is a variant of LSTM (Long Short-Term \\nMemory), a special kind of recurrent neural network capable of learning long-term dependencies. C-\\nLSTM contains a convolution operation at each gate in the LSTM cell, capturing spatial features by \\nconvolution operations in multiple-dimensional data. Good examples of applications with C-LSTM are \\nfound in medical problems, especially the ones where it is possible to accommodate local spatial \\ninformation in 2D image sequences. For example, C-LSTM were used in volumetric data sets to \\nsegment 3D data represented as a stack of 2D slices (Chen et al. 2016). \\n In this work, we tested a network similar to the one proposed by Arbelle and Raviv (2019), where \\nthe C-LSTM blocks were incorporated in every scale of the encoder section of the U-Net architecture. \\nThe loss used for the supervised training is the Jaccard loss because of its capability to address the class \\nimbalance. We split the migrated cubes in sequences of five 2D slices along the crossline direction, and \\nthese five slices are the input for the training and prediction process. The label for the training process, \\ni.e., the output, is the central slice of the selected region with the segmented salt geometry. Thus, our \\ninput images present the dimensions [5,160,320,11] represented by the sequential 2D slices, vertical \\nsamples, horizontal samples, and subsurface offsets. The output is an image with [160,320], \\ncorresponding to the salt segmented of the central slice of the C-LSTM sequence. We run our DL \\nexperiments using TensorFlow in an NVIDIA RTX A5000. \\n \\nResults \\n \\n The proposed DL model was trained for eighty epochs, selecting the state with the smaller \\nvalidation loss. Then, the DL model was applied over unseen data, initially, two volumes not previously \\nused for training/validation but coming from the same data generation procedure. We used a sliding \\nwindow to accommodate the five samples for the C-LSTM layers to predict each slice of the volume. \\nThe results for two test volumes are in Figure 2, where we plot the predicted salt geometry in the first \\ncolumn and the difference between the predicted and the true salt geometry in the second column. It is \\npossible to observe that the salt geometry was recovered with high accuracy, with small errors in the \\ncontours and more significant errors in the salt flanks and bottoms. However,  the estimated salt masks \\nare not drastically off and can be used as an initial approach to salt inclusion over the velocity model to \\nbe posteriorly reinterpreted. The predictions present a Jaccard index for the test volumes equal to 0.88. \\n \\n \\n84th EAGE Annual Conference & Exhibition \\n  \\n \\nFigure 2: (a) and (c) show the predicted salt geometries of the test volumes. (b) and (d) show the \\ncorresponding difference between the predicted and the real geometry, where blue indicates a false \\nnegative and red indicates a false positive. \\n \\n We tested the generalisation ability of C-LSTM-U-Net by segmenting the salt geometry of the \\nSEG/EAGE 3D salt model. We preserved the original model structures and velocity values and just \\nrescaled the model to fit the model size used in this study. The predicted salt geometry is in Figure 3 \\n(a), and we show the prediction error in figure (b). When comparing with the test results, more mistakes \\nare visible for the salt geometry of the SEG/EAGE model, with some spurious inclusions in the deeper \\nregions and more significant errors in the contours. The contour errors can be due to the velocity model \\nused in the migration. Since there is no sediment model without salt available for use, we have chosen \\none slice without salt and applied a gaussian smooth over this slice which was then replicated to create \\nthe migration velocity volume. The Jaccard index obtained for the SEG/EAGE model was equal to 0.65. \\n \\n \\nFigure 3: (a) shows the salt geometry obtained for the SEG/EAGE model, and (b) shows the difference \\nbetween the predicted and the real geometry, where blue indicates a false negative and red false \\npositive. \\n \\n \\n \\n \\nConclusions \\n \\n\\n \\n \\n84th EAGE Annual Conference & Exhibition \\n Salt inclusion is particularly time-demanding over regions of allochthonous salt forming complex \\ngeometries, such as overhangs, teardrops and tongues. The method proposed here aims to solve the salt \\ninclusion in one step with DL, reducing the time of the iterative process of salt flooding. Our results \\nshow that RTM migration using the sediment velocity generates subsurface offset gathers, which serve \\nas the input for a DL model to identify the 3D true salt geometry, even for complex geometries.  \\n We showed that a C-LSTM-U-Net architecture is enough to solve the 3D problem in a simplified \\nway, using a sequence of 2D slices as input for the training and prediction process. The predictions \\nobtained present continuous salt geometries in both crossline and inline directions. Furthermore, the C-\\nLSTM-U-Net has small demands of memory compared with 3D convolutional neural networks and is \\nalso faster to be trained.  \\n This work is proof of the concept of salt segmentation over data migrated with 3D geometries and \\nstill requires further studies to be extended to real seismic data. In future studies, expanding the size of \\nthe volumes used for shot simulation and migration will be necessary, increasing the depth of the interest \\ngeometries. A higher number of training volumes will probably be necessary to reach an accurate result \\nwhen using larger volume sizes. Another improvement to make this technology viable for real data is \\nto apply a matching filter between the wavelet used for synthetic data generation and the wavelet from \\nreal data.  \\n \\n \\nReferences \\nAlAli, A. and Anifowose, F. [2022]. Seismic velocity modeling in the digital transformation era: a \\nreview of the role of machine learning. J Petrol Explor Prod Technol  12, 21–34. \\nAlAli, A., Kazei, V., Kalita, M. and Alkhalifah, T. [2022]. Deep learning unflooding for robust subsalt \\nwaveform inversion. Geophysical Prospecting. \\nAminzadeh, F., Burkhard, N., Long, J. Kunz, T., Duclos, P.[1996]. Three dimensional seg/eaeg models \\n- an update: The Leading Edge , 15, 131–134.  \\nArbelle, A. and Raviv, T. R.[2019]. Microscopy Cell Segmentation Via Convolutional LSTM \\nNetworks , 2019 IEEE 16th International Symposium on Biomedical Imaging, 1008-1012. \\nChen, J., Yang, L., Zhang, Y., Alber, M., Chen, D. Z. [2016]. Combining fully convolutional and \\nrecurrent neural networks for 3d biomedical image segmentation, Advances in Neural Information \\nProcessing Systems , 3036–3044. \\nDellinger, J., Brenders, A.J., Sandschaper, J.R., Regone, C., Etgen, J., Ahmed, I., Lee, K. J. [2017]. The \\ngarden banks model experience, The Leading Edge , 36, 151–158.  \\nKlatt, M., Faria, E.L., Muller, A.P.O., Coelho, J.M., González, J.L., de Albuquerque, M.P., Bom, C.R., \\nCorreia, M.D. [2022]. Deep learning strategy for salt model building, Geophysics  87, IM221-IM233.  \\nMuller, A.P.O., Costa, J.C., Bom, C.R., Faria, E.L., Klatt, M., Teixeira, G., de Albuquerque, M.P. \\n[2022]. Complete identification of complex salt geometries from inaccurate migrated subsurface offset \\ngathers using deep learning, Geophysics  87, R453-R463. \\nSava, P. C., and Fomel, S. [2003]. Angle-domain common-image gathers by wavefield continuation \\nmethods, Geophysics  68, 1065–1074 \\nShi, Y., Wu, X., Fomel, S. [2019]. SaltSeg: Automatic 3D salt segmentation using a deep convolutional \\nneural network, Interpretation  7, SE113-SE122 . \\nSolovyev, R., Kalinin, A.A., Gabruseva, T.[2022]. 3D convolutional neural networks for stalled brain \\ncapillary detection, Computers in Biology and Medicine  141, 105089. \\nYu, S. and Ma, J. [2021]. Deep learning for geophysics: Current and future trends,  Reviews of \\nGeophysics  59, e2021RG000742.\"}\n",
      "{\"text\": \"Construction of porous model using equivalent pore aspect ratio distribution function\\nIntroduction\\nResearch on wave-induced fluid flow (WIFF) phenomenon proved that the dispersion of seismic waves\\nat different scales are positively correlated with the physical properties, which is related to both the\\nfluid and pore structure in the pores. Given that the fluid in pores has a significant effect on velocity,\\nthe inherent influence of pore structure is often neglected. Therefore, understanding the propagation of\\nelastic waves in dry rocks is meaningful to evaluate the effect of pore structure in heterogeneous rocks.\\nAlthough it is very attractive to determine the distribution characteristics of pore structure by using con-\\nventional logging data (acoustic log or electrical log), its implementation is a prevailing challenge. As a\\nestablished technology, imaging logging is appropriate to estimate the characteristics of pore structure,\\nbut the corresponding interpretation is low-efficiency and empirical, which has great limitations. An\\nimportant research field of rock physics is to characterize the pore structure quantitatively in rocks by\\nusing the ideal geometric shape to simplify the actual complex structure.\\nMost rock physics models assume that rocks contain only one or two types of pores with fixed equiv-\\nalent pore aspect ratio (EPAR). This assumption can well explain the relation of velocity with porosity\\n(Kumar and Han, 2005), but there are some limitations in the experimental data about the variation of\\nvelocity with pressure. Accepting the fact that, the microcracks in the rock will gradually close with the\\nincrease of pressure, which has a great impact on the velocity. Importantly, the fixed EPAR assumption\\nmakes pore closure a discontinuous process, which results in that the microstructure porosity and the\\ncorresponding EPAR under different pressures can not be explained in a unified framework. In the rock\\nphysics model proposed by David and Zimmerman (2012), several sets of microcracks with different\\nEPAR exist within rock. The distribution of microstructures and the related elastic properties could be\\ndescribed with the help of microcrack density. On this basis, we carried out the analysis of measurement\\ndata and designed a 2D porous model using the obtained pore structure parameters. The effectiveness\\nof equivalent pore structure is verified by elastic wave propagation simulation based on finite element\\nmethod.\\nMethod\\nStiff pores usually do not \\\"close\\\" under experimental pressure conditions. Even for pores with EPAR\\nof 0.01, the differential pressure required to complete closure is 800 MPa, so the non-closable pores\\nhave little effect on the relationship between velocity and pressure in the experiment. We assume that\\nthe rock contains a set of stiff pores with EPAR αsan and several sets of microcracks with different\\nEPAR in contrast with the assumption of fixed EPAR. In the process of confining pressure decreasing,\\nthe change of elastic modulus can be considered to be produced by the addition of microstructure.\\nMoreover, the number of initial microcrack sets is related to the pressure interval range from 0 MPa to\\nthe closure pressure. The elastic modulus can be expressed as the function of microcrack density based\\non Mori-Tanaka's method \\n\\nKb\\nK=1+16(1−νb)(1+νb)Γ\\n9(1−2νb)\\nµb\\nµ=1+32(1−νb)(5−νb)Γ\\n45(2−νb). (1)\\nwhere Γis the microcrack density, Kbandµbare bulk modulus and shear modulus of dry rock at higher\\npressure, Kandµare elastic modulus which contain microcracks, and νbis Poisson's ratio.\\nBased on equation (1), we can estimate the related microcrack density Γ(P)in each pressure interval\\nduring the unloading process. Once the pressure drops to 0 MPa, the Γ(P)can be used to calculate the\\ncumulative microcrack density ¯Γ(α). Further to this, the initial crack aspect ratio distribution αican be\\nobtained as follows\\nαi=3\\n4π∫¯Γ(α)\\n¯Γi[\\nC(Γ)−Chp]\\nΓdP\\ndΓdΓ (2)\\nwhere ¯Γiis the initial microcrack density at 0 MPa, Cis the bulk compressibility, and Chpis the com-\\n84thEAGE Annual Conference & Exhibition\\npressibility at high pressures, which can be determined according the measurement range or nonlinear\\nleast square fitting.\\nFigure 1(a) shows the measured results of compressional and shear wave velocities of two sandstone\\nsamples under different confining pressures, in which sample 1 is shaly sandstone and sample 2 is clean\\nsandstone. It can be clearly observed that the response characteristics of the two samples to pressure\\nremain similar, notwithstanding the effects of mineral content on velocity. During the experiment, an\\nobvious increasing trend in velocity was observed until 20MPa. The reason for this phenomenon is\\nthat many microcracks are closed under the pressure higher than their closure pressure. As the pressure\\nincreased continually, the velocity variation trend is relatively smooth because of the amount of unclosed\\nmicrocracks in rock. The dotted line in Figure 1(a) describes the exponential law of rock elastic modulus\\nwith pressure (Zimmerman, 1991), and the fitting results are in good consistency with the measured data.\\nAccording to the above analysis, we can use equation (1) and (2) to calculate the initial pore aspect ratio\\ndistribution and the corresponding porosity of the microcracks in the two samples, as shown in Figure\\n1(b). It is proved that there are a large number of microcracks with small EPAR in samples, and the\\ndistribution range of EPAR in shaly sandstone (sample 1) is larger. By discretizing the corresponding\\ndistribution function, we can obtain the initial EPAR under 0MPa, as shown in Figure 1(c). As an aside,\\nthe EPAR of stiff pores is estimated from the high pressure measurement data by using rock physics\\nmodel.\\n0 20 40 60 80 100\\nDifferential pressure (MPa)1.52.53.54.55.56.5P-wave velocity (km/s)\\n1.52.53.54.55.56.5\\nS-wave velocity (km/s)Sandstone Sample 1\\nSandstone Sample 2\\nFitting Line\\n0 0.5 1 1.5 2 2.5\\nEquivalent microcrack aspect ratio 10-300.150.300.45Cumulative microcrack density\\n00.511.522.533.5\\nMicrocrack porosity×10-5\\nCumulative Density (Sample 1)\\nCumulative Density (Sample 2)\\nMicrocrack porosity (Sample 1)\\nMicrocrack porosity (Sample 2)\\n×-4 -3.5 -3 -2.5 -2 -1.5 -1 -0.5\\nLog equivalent pore aspect ratio10-810-610-410-2100PorosityEPAR distribution (Sample 1)\\nEPAR distribution (Sample 2)a) b) c)\\nFigure 1 Inversion of aspect ratio distribution of two sandstone samples: a) The velocity-pressure curve\\nin dry sandstone samples, b) The cumulative microcrack density and the corresponding microcrack\\nporosity, c) The initial EPAR distribution.\\nUsing the calculated porosity and EPAR of stiff pores and microcracks, we can construct an equivalent\\ntwo-dimensional porous geometrical model, and its elastic response can be solved by finite difference\\nmethod or finite element method. The similarity principle is the theoretical basis for studying the in-\\nfluence of pore structure on seismic wave propagation by using the forward modeling of porous model.\\nSimilar to the in-lab seismic physical modeling, the geomerty objects should keep equal proportions\\nwith the core at the real pore scale, and ensure that the product of the wave propagation distance and\\nits measured frequency is always consistent in a period, which is equivalent to the same propagation\\nvelocity in natural and simulated media. According to the elasticy theory, the differential equation of\\nmotion can be expressed\\nρm∂u\\n∂t2−∇·σ=F (3)\\nwhere ρmis the density of the solid matrix, u(x,y)is the displacement vector in 2D plane, σis the\\nelements of the stress tensor and Fis the stress vector caused by external source. The motion equation\\ncontrolling the pore fluid domain is\\n1\\nKf∂2p\\n∂t2+∇·[\\n−1\\nρf∇p]\\n=0 (4)\\nwhere ρfandKfis the pore fluid and the bulk modulus and pis the pressure. Importantly, the stress and\\nacceleration are continuous on the boundary between solid matrix and pore fluid. In similar 2D porous\\n84thEAGE Annual Conference & Exhibition\\ngeometrical models, some essential parameters, such as porosity, pore aspect ratio and pore distribution,\\nhave significant effects on elastic wave propagation. To exemplify this problem, we designed three\\ndiffernet models for numerical simulation. These models have the same porosity (6.8 %) and solid\\nmatrix properties, but there are obvious differences in the distribution of pores.\\na) b) c)\\nFigure 2 Differnet distribution of pores in 2D plane: a) Regular pore distribution, b) Regular pore\\ndistribution with different major axis orientation, c) Random pore distribution.\\nThe source and geophone in the numerical simulation are placed at the top and bottom of the model\\nrespectively. The forward modeling results show that different pore space distribution will make great\\neffect on the propagation velocity of elastic waves. Compared with Figure 2(b), the propagation velocity\\nof Figure 2(a) is significantly faster, which is related to the angle between the elastic wave propagation\\ndirection and the long axis of the pore. In addition, in the case of random distribution, the wave velocity\\nin the numerical simulation is close to that calculated by the DEM model (Wang et al., 2015).\\nExample\\nAccording to the above analysis, we can determine the porosity and EPAR of two different types of\\npores (stiff pore and microcrack) in the 2D model. Other properties including solid matrix properties\\ncan be referenced by David and Zimmerman's paper. We assume the spatial distribution of pores obey\\nrandom distribution, and the equivalent geometric model obtained is shown in Figure 3(a) and Figure\\n3(b). The geometric model can basically reveal the pore structure characteristics of rock. Moreover, the\\npores are independent of each other, and the fluid in the pores is gas. Simiarly, the source and geophone\\nin the numerical simulation are placed at the top and bottom of the model respectively.\\nAfter confirming the low reflection boundary, we can use FEM to simulate the elastic waves propagation\\nin the geometric model. Due to the existence of microcracks in the model, the relationship between\\nthe length of the minor axis and the minimum grid element should be determined in advance which\\ncould improve the computational efficiency. Even so, the number of mesh elements after division still\\nreaches 107. The details of part of the grid elements are shown in Figure 3(c) and Figure 3(d). The\\nvelocity of the sample can be calculated according to the length of the model and the first arrival time.\\nThe simulation results have a good correlation with the measured velocity, and the error of sample 2 is\\nsignificantly lower than that of sample 1, which may be related to its low porosity. But how the factors\\nsuch as distribution and pore size affect the wave velocity of the sample, further research are needed.\\n84thEAGE Annual Conference & Exhibition\\nFigure 3 The established porous geometric model and the corresponding meshing results: a) Geometric\\nmodel of sample 1, b) Geometric model of sample 2, c) Partial finite element mesh of sample 1, d) Partial\\nfinite element mesh of sample 2.\\nConclusions\\nIn this abstract, we analyzed the characteristics of non-closable pore and microcrack in dry rock samples\\nby David-Zimmerman model. The results show that there is an obvious difference in the distribution\\nof microfractures between pure sandstone and shaly sandstone, and the samples with high shale content\\nhave more developed microcracks and stronger heterogeneity. The results show that the random distri-\\nbution is more consistent with the pore distribution in the equivalent geometrical model. Therefore, the\\ncorresponding 2D porous model is designed in a random distribution. With the help of the finite element\\nmethod, we calculated the elastic wave response. The validity of the method is preliminarily proved, but\\nthe influence of many factors including pore distribution and pore size on elastic wave need to be further\\nstudied.\\nAcknowledgements\\nThis work is sponsored by National Natural Science Foundation of China (grant no.42174130), the\\nNational Key R & D Program of China (grant no.2018YFA0702501) and the Science and Technology\\nProject of CNPC (grant no.2019A-3310). It is published with the permission of the State Key Laboratory\\nof Petroleum Resources and Prospecting.\\nReferences\\nDavid, E.C. and Zimmerman, R.W. [2012] Pore Structure Model for Elastic Wave Velocities in Fluid-\\nSaturated Sandstones. Journal of Geophysical Research: Solid Earth ,117(B7).\\nKumar, M. and Han, D.H. [2005] Pore Shape Effect on Elastic Properties of Carbonate Rocks. In: SEG\\nTechnical Program Expanded Abstracts 2005 . Society of Exploration Geophysicists, 1477–1480.\\nWang, Z., Wang, R., Weger, R.J., Li, T. and Wang, F. [2015] Pore-Scale Modeling of Elastic Wave\\nPropagation in Carbonate Rocks. Geophysics ,80(1), D51–D63.\\nZimmerman, R.W. [1991] Compressibility of Sandstones . No. 29 in Developments in Petroleum Science.\\nElsevier, Amsterdam.\\n84thEAGE Annual Conference & Exhibition\"}\n",
      "{\"text\": \"Simultaneous deblending, deconvolution and Doppler-shift correction of marine vibrator data\\nIntroduction\\nNon-impulsive marine sources (NIMS), such as marine vibrators, appear as a viable option to fulfill pos-\\nsible new environmental constraints on marine acquisitions. They can also provide low frequencies and\\noffer opportunities for efficient acquisition strategies by blending many phase-encoded sources together.\\nHowever, they present a unique set of processing challenges, such as the handling of the Doppler effect,\\nwhich stems from having a moving boat (and possibly receivers), together with a long emitting source.\\nLong-emitting sources in a marine acquisition context can be blended to make them economically viable\\nalternatives to more conventional gun arrays. In this case, new sweeps can be started without waiting\\nfor others to finish. With blending, many sweeps can be started and overlapped in a way that make them\\namenable to deblending techniques. We propose and illustrate an inversion strategy to simultaneously\\ndeblend, Doppler-shift and deconvolve blended marine vibrator (MVib) data. Our approach assumes\\nthat the seismic signal is sparse in a given domain. While the blending operator acts on common-shot\\ngathers, the Doppler-shift correction and deconvolution happen on receiver gathers (CRG). A hyperbolic\\nfunctional (Li et al., 2012) in the model space yields the desired sparseness. A hyperbolic functional in\\nthe data space improves the inversion's robustness to non-Gaussian noise.\\nDoppler-shift correction and deconvolution of moving MVib data\\nFigure 1: Blended acquisition with three boats.We quickly review the approach presented in\\nGuitton et al. (2021) for the deconvolution and\\nDoppler-shift correction (DSC) of moving, ma-\\nrine vibrator data. The goal of this process is to\\nretrieve the data we would have acquired with-\\nout a moving source (DSC applied) and without a\\nsweep (deconvolved). This approach follows the\\nframework developed by Hampson and Jakubow-\\nicz (1995) to explain the relationships between\\nsweep, source motion and data. Assuming that\\na source boat moves at a constant speed while\\nemitting a repeatable sweep signal, with a non-\\nvarying sea surface, the relationship between an observed CRG dfrom a moving sweep signal and its\\ndeconvolved counterparts dibecomes\\nCdi≡d (1)\\nwhere Cis a slanted, multi-dimensional stationary convolution operator with the sweep. This convolu-\\ntion operator depends on the shot sampling which, in turn, depends on the boat speed and on the sweep\\nsignal. For a non-moving source, the convolution operator becomes 1D and no Doppler effect is present.\\nIn theory, the correlation of the 2D sweep with the recorded data should yield phase-corrected traces.\\nHowever, due to the shot sampling in CRGs, the 2D correlation will also generate aliasing artifacts. Gui-\\ntton et al. (2021) propose to interpolate the missing traces (shots) to a finer grid to mitigate the aliasing\\nartifacts, while deconvolving the sweep, by minimizing\\nf1(m) =|SCLm−d|h+ε|m|h, (2)\\nwhere|.|his the hyperbolic functional which varies between the ℓ1andℓ2norm. In equation 2, Lmare\\nthe deconvolved data on a fine grid where aliasing is mitigated and mis a data representation (dictionary)\\nwhere sparseness yields to data interpolation. In this work, the linear Radon domain is used. Other\\nrepresentations in the curvelet, wavelet, Fourier domains are also available. Finally, Sis a mapping\\noperator between the positions of the interpolated modeled data on a fine grid and the positions of the\\nobserved data.\\n84thEAGE Annual Conference & Exhibition\\nDeblending, deconvolution and DSC of MVib data\\nBlending could make the acquisition of MVib data time efficient by circumventing the issue of having\\nlong emitting sources. We propose to extend equation 2 by adding a blending operator Φ. Deblend-\\ning, together with interpolation and deconvolution, happens in common receiver gathers by forcing the\\nseismic data to be sparse in a given domain minimizing\\nf2(m) =|ΦSCLm−d|h+ε|m|h. (3)\\nEquation 3 shows that estimating msimultaneously interpolates, deconvolves and deblends the data.\\nThe core of this method is the regularization term (i.e. the sparseness constraint) that attenuates the\\nincoherent part of the signal in a domain of choice ( L′can be curvelet, Fourier wavelet, Radon, etc...\\ntransforms). In this work, the incoherency in the data representation comes from the blending of shot\\ngathers and the interpolation/reconstruction step needed to deconvolve the sweep. We use the linear\\nRadon transform in patches on CRGs as our dictionary ( Lmin equation 3 becomes PLm where Pis a\\npatching/stitching operator that makes the linear Radon transform local).\\nExamples\\nFigure 2: Blended acquisition with two sources\\n(blue and red).We first blend synthetic MVib data using a three\\nsource boats geometry (Figure 1). Here, 80 shots\\nrecorded on only 15 static nodes are used. Shot\\nand receiver spacing is 24 meters for a boat speed\\nof 4.8 m/s. Note that there is a small random\\nperturbation added to the firing times of the two\\nrightmost boats. The maximum frequency is\\n25 Hz to speed up computations, and there is lit-\\ntle to no Doppler shift in that case. The synthetic\\nMVib data without and with blending (i.e. Φ†d)\\nare shown in Figure 3. We apply equation 2 to\\nthe data in Figure 3 to obtain Figure 4: while the\\ndeconvolution works well on the unblended case,\\nit fails on the deblended data due to truncation and blending effects, as expected (we would never run\\nthis in practice). Figure 5 shows the whole deblending, deconvolution and DSC process applied to the\\nblended data. The difference panels in Figure 5 proves that our approach is able to perform three tasks\\nat once and yields data (left of Figure 5) very similar to those obtained without blending (left of Figure\\n4.)\\nFigure 3: Synthetic Mvib data acquired without (left) and with (right) blending (geometry of Figure 1\\nafter running Φ†d). 80 shots, recorded on 15 receivers are blended using the geometry of Figure 1.\\nNow, we look at the blending and deblending of MVib field data acquired in 2022 (Alfaro et al., 2023).\\n84thEAGE Annual Conference & Exhibition\\nFigure 4: Deconvolution and DSC of the synthetic data in Figure 3 using equation 2. Left: no blending.\\nRight: blending. This process is applied on CRGs. The artifacts on the right panel are due to the\\ntruncations of the sweep in the pseudo-deblended CRGs and the presence of blending noise. The sweep\\nneeds to be handled before deblending or during, as proposed in this work.\\nFigure 5: Left: Simultaneous deblending, deconvolution and DSC of the blended synthetic data with\\nequation 3. Right: difference with left panel of Figure 4. The proposed approach is quite succesful at\\nhandling three processing steps at once thanks to the sparseness assumption.\\nWe artificially blend 80 shots gathers (ds=12.5 m) recorded on 150 nodes (dr=50 m). The sweep is 5\\ns long and the blending sequence with two sources is shown in Figure 2: the boats fire on position but\\nwith a small random time delay within [−0.5,0.5]s. The boat speed is 1.8 m/s. Data are low-passed to\\n35 Hz to improve computational efficiency. This blending sequence optimizes the acquisition time by\\noverlapping sweeps from two sources (red and blue dots in Figure 2) on one booat. A similar blended\\nacquisition line was acquired in 2022. The field MVib data without and with blending (i.e. Φ†d) are\\nshown in Figure 6. Applying equation 2 to the individual CRGs of Figure 6 (i.e. without blending)\\nyields the left panel of Figure 7, while applying equation 3 to the blended shots yields the right panel of\\nFigure 7. The two panels of Figure 7 are similar, proving that the simultaneous processing (deblending,\\ndeconvolution, DSC) is feasible.\\nConclusion\\nWe show the first results of deblending MVib data and demonstrate that all pre-processing steps (de-\\nblending, DSC, deconvolution) can be achieved by enforcing a sparse representation of the signal in\\na given domain. Our goal is to use this approach on an actual blended MVib survey that we recently\\n84thEAGE Annual Conference & Exhibition\\nFigure 6: Field Mvib data acquired without (left) and with (right) blending (geometry in Figure 2 after\\nrunning Φ†d). 80 shots, recorded on 150 nodes are blended using the geometry of Figure 3.\\nFigure 7: Left: DSC and deconvolution of the data in the left panel of Figure 6 using equation 2. Right:\\nDeblending, DSC and deconvolution of the artificially blended data using equation 3. The two panels\\nare quite close, proving the efficiency of the simultaneous processing.\\nacquired. Other processing steps, such as deghosting, could be incorporated as well in our approach.\\nAcknowledgments\\nWe thank TotalEnergies and the MVJIP for permission to publish this work.\\nReferences\\nAlfaro, R., Secker, S., Zamboni, E., Cozzens, A., Henderson, N., Jenkerson, M., Nechayuk, V ., Johnson,\\nG. and Karran, J. [2023] Validation of an Alternative Seismic Source: The Integrated Projector Node\\nMarine Vibrator Pilot Seismic Survey. 84th EAGE Annual Conference and Exhibition , submitted.\\nGuitton, A., Duquet, B., Secker, S., Mascomere, J.P. and Feltham, A. [2021] A deconvolution-\\ninterpolation approach with sparse inversion to mitigate the Doppler effect in marine vibrators data.\\nFirst International Meeting for Applied Geoscience and Energy, Expanded Abstracts , 2555–2559.\\nHampson, G. and Jakubowicz, H. [1995] The effects of source and receiver motion on seismic data.\\nGeophysical Prospecting ,43(2), 221–244.\\nLi, Y ., Zhang, Y . and Claerbout, J. [2012] Hyperbolic estimation of sparse models from erratic data.\\nGeophysics ,77(1), V1–V9.\\n84thEAGE Annual Conference & Exhibition\"}\n",
      "{\"text\": \"\\nIntroduction  \\nIn this paper, we present a high-resolution FWI case study from the foothills of the Southern \\nCarpathians, in Romania. In this hilly area, the Spineni dataset was acquired in early 2022, composed \\nof approximately 85% dynamite shot points and 15% vibroseis shot po ints. It represents a total area of \\n500 km2 that fills and follows the reprocessing of the Getic Merge project for a total area of 3000 km2 \\n(Meffre et al., 2022). The large maximum offset x/y of 6 and 11 km, respectively, offers an optimal \\ndesign for wide-azimuth subsurface illumination, which is crucial in areas with complex structural \\ngeology as described by Krezsek et al. (2011). The reservoir, located below a thick overthrust at around \\n5 km depth, has a number of closures with uncertainties in size and crest positioning. This thrust \\ninclusion in the northern part of the survey, the so called Burdigalian wedge, is characterized by a very \\ncomplex folded structure with steep dips and strong lateral and vertical velocity variations with high \\nvelocities. In this context, a high-resolution FWI velocity model was used to help e nhance the imaging \\nand be\\ntter define the spill point of the prospect. \\nNear surface characterization \\nThe challenges identified in this area include the large variation of the topography and the presence of \\nvery slow velocities (700 m/s) in the weathering zone (WZ), as shown by the up-hole information. This \\ncreates strong imaging distortions from the very shallow down to the target depth. Ground roll \\ncontamination and irregular near offset distribution make access to near surface reflectivity very \\nchallenging. That is why multi-wave inversion (MWI) was used obtain an initial high vertical resolution \\nanisotropic velocity model for subsequent acoustic FWI updates. MWI, by minimizing the residuals of \\nthe first break (FB), ground roll dispersion curves (DC), and WZ base two-way times, allows for a \\nr\\neliable multi-parameter reconstruction of Vp, Vs, and  (Bardainne, 2018; Donno et al., 2021; Prieux et \\nal., 2020). Thanks to a dense acquisition design, we were able to clearly identify and pick the base of \\nthe WZ from the receiver-side operators of a surface consistent deconvolution (Retailleau, 2015 ) \\n(\\nFigure s 1c-d). In addition, the good low-frequency content coming from the broadband signal produced \\nby the dynamite shots coupled with long offsets of up t o 10 km, enabled us to further resolve the l ong-\\nto-mid wavelength velocity of the near surface model. The output QC (Figure 1) showed reliable and \\ngeological ly consistent results with a flatter base of the WZ and fewer structural undulations compared \\nto the FB tomography model.  \\nFigure 1:  (a) FB tomography model; (b) MWI + FWI  7 Hz near surface model; (c) and (d) The base \\nof the weathering zone interpreted on the deconvolution operators using models (a) and (b), \\nrespectively; (e) and (f) Stacks after Kirchhoff pre-stack depth migration (PSDM) with models (a) and \\n(b), respectively. \\n50Hz high resolution land FWI: a case study in the Carpathian foothills\\n \\n \\n\\nComplex imaging underneath overthrust folded structures  \\nThe velocity shown in Figure 2a has already been  through a complete near surface update as described \\nabove, plus a first pass of multilayer TTI tomography. In addition, a pass of FWI from 7 Hz up to 12Hz \\nusing diving and reflect ed waves for the deeper section update (Figure  2b) shows a global improvement \\nof focusing (Figure  2e). However, some residual structural distortions and poor focusing were still \\nvisible below the thick thrust composed of heavily folded structures with strong velocity contrasts. To \\ncapture more detail in the velocity mode l to resolve these imaging problems, high-resolution FWI was \\nused (Zhang et al., 2020 ). As the FWI frequency was increased from 12 Hz to 40 Hz , gradual imaging \\nimprovements were observed due to the additional details in the model . Beyond 40 Hz, no significant \\nimaging improvement s were observed. The 50  Hz FWI velocity model fully captured the strong \\nvariations and velocity contrasts (Figure 2c), and the structures exhibit ed nice details and improved \\nfocusing of the target area (Fig ure 2f).  \\n \\nFigure 2 : (a) Section of initial model after a complete near surface update , (b) FWI at 12  Hz and (c) \\n50 Hz illustrating the complexity of the thrust  and high -velocity contrast in the target area.  \\nCorresponding  zoomed section  of stacks after Kirchhoff pre -stack depth migration (PSDM) with (d) \\ninitial , (e) FWI 12  Hz and (f) 50 Hz models to illustrate  the impact on the imaging in the target area . \\n \\nIn addition, FWI  Imaging ( Zhang et al., 2020 ), was derived from the 50 Hz FWI velocity. FWI  Imaging \\nis seen to benefit from additional information provided by the full wavefield data and uses least -squares \\ndata fitting , which is essential to image below complex geological features ( Salaun et al., 2021 ). Figure \\n\\n \\n \\n\\n3c shows clearer fault definition, better amplitude continuity , and fewer  migration art ifacts, as \\nhighlighted by the white arrows.  \\n \\nFigure 3 : (a) 50 Hz FWI velocity  model , (b) Kirchhoff  PSDM  stack  and (c) FWI Imaging reflectivity  \\nderived from (a). The yellow and red  extrema color palette  on the reflectivity  underlines stronger \\nvelocity contrast s to help delineate potential reservoir spill points . \\n \\nDe-risking fault closure uncertainties in the prospect area  \\nThe main prospect sitting below the overthrust, shown in the  white polygon on the depth slice of Figure s \\n4b and 4e, has multidirectional fault -dependent closures. Figure 4b shows the PSDM depth slice cutting \\nthrough the prospect area, which has high uncertainties in both  size and crest position in depth. The 50 \\nHz FWI velocity  model  in Figure s 4d and 4f  exhibits very high  resolution , which  captur es the complex \\nvelocity contrast of the  upper folded structures and the underlying stratigraphy . As a result, the fault  \\n \\nFigure 4: (a-b) 12  Hz FWI model and PSDM stack; (d -e) 50  Hz FWI model and PSDM stack ; (c-f) \\nSonic log  comparison with 12  HZ and 50  Hz FWI model , respectively.  \\n \\n\\nclosures around the prospect area are better imaged (Figure 4e), allowing considerable de-risking of the \\nuncertainties in their interpretation. Moreover, the velocity contrasts are also well correlated with the \\nwell's sonic log velocities, providing further validation of the 50  Hz FWI model. \\nConclusions \\nIn this paper , we emphasiz ed the crucial importance of addi ng high-resolution details t o the velocity \\nfield using FWI i n a comple x foothill s geological settin g. The resulting model has greatly improved the \\nimagin g below th e overthrust an d help ed de-ri sk th e interpretation uncertainties of the reservoir \\nstructur e and fault closur e. In additi on, the associat ed high-resoluti on velocit y model c an al so be used \\nwith Kirchhoff PSDM conventional imagi ng to facilitate detail ed reservoir interpretation work.  \\nThese results, in reaching 50 Hz, which is high frequency for land imaging, were obtained also thanks \\nto the well-designed long offset WAZ acquisition, strong low-frequency content of the dynamite source, \\nand the  continuous  effort  made  on computation  technology  over last few years.  For the near  future, \\nelastic  FWI could potentially  offer  further  uplifts by  deriving improved  Vp and Vs  models  in such a \\nhigh velocity contrast medium. \\nAcknowledgements \\nWe are grateful to O MV PETRO M and CGG for their permission t o publish this study. We also thank \\nall those who were involved in the project. \\nReferences \\nBardainne, T. [2018] Joint inversion of refracted P-waves, surface waves and reflectivity. 80th EAGE \\nConference & Exhibition , Extended Abstracts, We K 02. \\nDonno, D., Farooqui, S., Mostafa, K., McCarthy, D., Solyga, D., Courbin, J., Prescott, A., Delmas, \\nL. and Le Meur, D. [2021] Multiwave inversion: A key step for depth model building - examples \\nfrom the Sultanate of Oman. The Leading Edge, 40(8), 610-618. \\nKrezsek, C., Oterdoom, H., Dzido, P., Barbu, V., Bland, S., Arnberger, K. and Lapadat, A. [2011] \\nNew insights into the hydrocarbon system of the getic Depression Romania: Implications for \\nExploration. 73rd EAGE Conference & Exhibition Workshops, Extended Abstracts. \\nMeffre, A., Prieux, V., Retailleau, M., Le Meur, D., Monteiro, A. A., Bouzouita, Z., Wang, F., Mestiri, \\nS., Markos, T., Vermeulen, J., Orosz, J. and Tyler, E. [2022] Revival of legacy land seismic surveys \\nusing advanced processing technologies - Example from the Carpathian foothills. First Break, 40(1), \\n45-52. \\nPrieux V., Bardainne T., Meffre A., Prigent H., Van Kleed F.J., Waqas M. and Hou L. [2020] \\nStructurally constrained anisotropic Multi-Wave Inversion utilizing Machine Learning and Big Data \\non a Middle East OBC project.  EAGE Annual Conference & Exhibition Online, Extended Abstracts. \\nRetailleau, M.G. [2 015], Imaging the Near Surface Using Surface-consistent Prediction Operators - \\nExamples from the Middle East. 77th EAGE Conference & Exhibition , Extended Abstracts, Th N116 \\n09. \\nSalaun N., Reinier M., Espin I.  and Gigou G. [2 021] FWI velocity and imaging: A case study in \\nthe Johan Castberg area. 82nd EAGE Annual Conference & Exhibition , Extended Abstracts. \\nZhang, Z., Wu, Z., Wei, Z., Mei, J., Huang, R. and Wang, P. [2 020] FWI Imaging: Full-wavefield \\nimaging through full-waveform inversion. 90th SEG International Meeting , Expanded Abstracts, \\n656-660\"}\n",
      "{\"text\": \"Dolomitization Model of Pelletoid of Arab C Formation in Southern Arabian  Gulf  \\n \\nIntroduction  \\n \\nThe Arabian Gulf in the late Jurassic is a semi -enclosed sea with shallow water  (Morad, 2012) . The \\nsouthern Arabian gulf presents a gentle landform, which is well suited for the deposition  of extensive \\ncarbonate sediments  (Figure 1) . With the closure of the ancient Tethys Ocean and the arid subtropical \\nclimate , it gradually changed from restricted platform to evaporative platform , and a large amount of \\nevaporite was produced, which also cal led Sabkha.  Among these evaporites, dolomite is the most \\ndeveloped  (DiLoreto , 2019) . From the remaining structural  and components, it can be seen that the \\noriginal dolomite sediments were mainly pelletoid.  Dolomitization  is the main geological process that \\nturn pelletoid into dolomites. However, there are still many unanswered questions . How does \\ndolomitization convert pelletoid  into dolomites?  Why do dolomites have different structures and \\ncrystals?  This paper mainly ans wers these two questions.  \\n \\n \\nFigure 1  Sedimentation and facies belts along the southern Arabian Gulf (cited in Kendall , 2011)  \\n \\nMethod and Data  \\n \\nTaking the Jurassic  Arab C  Formation of BH oilfield in United Arab Emirates  as an example, the study \\nwas carried out. The BH field is located in the south Persian Gulf, which is tectonically located in the \\nRubal -Khali Basin .  There are a total of 3 coring wells in the Arab C  Formation. The cumulative length \\nof the core is 120 m, there are 410 cast thin sections, there are 3263 physical property samples, there are \\n44 mercury injection  samples .  \\n \\nFirstly, according to the cast thin sections , the Pelletoid  types  are determined . Secondly,  the diagenesis \\nof the Pelletoid  is comprehensively recognized  based on the theory of petrology and geophysics, \\nincluding the diagenetic types, diageneti c environment  and diagenetic product.  Finally, the \\ndolomitization  model of Pelletoid  of Arab C Formation in Southern Persian Gulf  is established and \\ndiscussed based on various geological theories and data.   \\n \\nResults  \\n \\nThere are four  types dolomite in Arab C formation in BH oilfield, which were called type  Ⅰ, type Ⅱ, \\ntype Ⅲ, type Ⅳ. The type Ⅰ showed that the inner pelletoid  was replaced by dolomite  and the pelletoid  \\ncontour were preserved . The intergranular pore were filled with small calcite, locally dolomite filled \\n\\n \\n \\n(Figure 2a) . The type Ⅱ showed that the pelletoid  were almost dissolved, leaving moldic pores. However, \\nthe pelletoid  margin was still preserved, which is in a dark brown colour. The intergranular pore was \\nmainly filled with a large amounts of dolomite, as a result, the pores and particles are reversed  (Figure \\n2b). The type Ⅲ showed  that the pelletoid are completely repl aced by domomite, but the primitive \\nstructure was preserved and some dissolved pore are developed.  The dolomite size were pretty small \\nand large amounts of micropores were developed (Figure 2c) . The type Ⅳ  showed that the pelletoid  \\nwere completely replaced  by dolomite and intense dissolution also occurred. The pelletoid structure is \\nnot obvious and large amounts intercrystalline pore and vug pore developed (Figure 2d).  \\n \\n \\nFigure 2 Dolomite types of Arab C formation in BH oilfield . (a) W ell BH-3, 9696.0 ft , the remaining \\npelletoid is obvious, the dark brown part is calcite;(b) W ell BH-3, 9693.0 ft, the pores and particles \\nare reversed, the dolomite filled the pores, the dark brown part is calcite and around the moldic \\npore;(c)  Well BH-3, 9678.8 ft,  the pell etoid are completely replaced by domomite, but the primitive \\nstructure was preserved and some dissolved pore are developed; (d) Well BH-3, 9712.05 ft,  the \\npelletoid structure is not obvious, some vug pore is developed.  \\n \\nThe type  Ⅰ. The original rock of d olomite  of type  Ⅰ is pelletoid  limestone  (Figure 3a ). Diagenesis includes \\ngypsification , dolomitization, dissolution and cementation. In the penecontemporaneous  stage, some  \\npelletoid s were replaced by gypsum but the contour of the pe lletoid  were still recognizable. The \\nprecipitation of gypsum promoted dolomitization  due to consumption of Ca2+. As a result, the pelletoid \\nand gypsum were replaced by dolomite . Dolomite scatter  in the inner gypsum and pe lletoid.  (Figur e \\n3b). In marine environment, micrization  occurs at the margin  of pelletoid , forming micrite envelop . The \\nmicrite envelop has stable chemical property . The meteoric  water selectively dissolves the inner \\npelletoid , leaving the micrite envelop undissolved . The dissolution form pores in the grain s, but a small \\namount of calcite still  remains ( Figure 3c ). The cementation occurred because of the saturated fluid \\nformed by the dissolution , form ing of iso-rim blade  calcite at the margin  of pelletoid . With the \\ndevelopment int o the pores, the calcite develops into equiaxed granules, and the calcite occupy a large \\nnumber of intergranular pores  (Figure 3 d). Dolomitization occurred again during the burial period, \\nforming euhedral  fine-grained dolomite, which further filled the res idual pores ( Figure 3 d), and \\neventually formed the type  Ⅰ of dolomite as show in cast thin sections  (Figur e 2a). \\n \\n \\nFigure 3 The forming process of ty pe Ⅰ dolomite . (a) pelletoid limestone; (b) gypsification; (c) \\ndolom itization  in pelletoid; (d) mic ritization in marine environment and dissolution in  meteoric \\nenvironment; (e) iso -rim cementation in meteoric environment; (f) Dolomitization in burial \\nenvironment  \\n \\nThe type Ⅱ.  The original rock of dolomite of type  Ⅱ is pelletoid limestone  (Figur e 4a). In the \\npenecontemporaneous  stage, some  pelletoid were replaced by gypsum and the some intergranular pores  \\nwere filled with gypsum. The precipitation of gypsum consumes Ca2+, improves Mg2+/Ca2+, and \\npromotes dolomitization. The dolomite not only metasoma sis pelletoid , but also metasomasis  the \\n\\ngypsum formed in the early stage. The dolomite is discretely embedded in the gypsum and pelletoid  \\n(Figur e 4b). When the sea level decline, dissolution occurred in meteoric  envi ronment, and a large \\nnumber of  pelletoid  were intense dissolved , forming intragranular pores. Some particle dissolution was \\ninsufficient, and a small amount of micrite remained. Cementation occurred after the dissolution in \\nmeteoric environment because of the enrich of  Ca2+, form ing of iso- rim blade-like or equiaxed granular \\ncalcite at the margin  of pelletoid  (Figur e 4c). Mg2+/Ca2+ was enhanced  again by cementation. In the \\nburial environment, the temperature and pressure conditions promoted the occurrence of dolomitization. The newly precipitated  dolomite crystals filled the intergranular pores, resulting in the pores inversion \\n(Figur e 4d), and eventually formed the type  Ⅱ of dolomite as show in cast thin sections  (Figur e 2b). \\nFigure 4 The forming process of ty pe Ⅱ dolomite . (a) pelletoid  limestone; (b) gypsification  and \\ndolomiti zation  in pelletoid; ( c) micritization in marine environment , dissolution  and iso-rim \\ncementation in meteoric environment; ( d) Dolomitization in burial environment  \\nT\\nhe type Ⅲ. The original rock of dolomite of type  Ⅲ is pelletoid limestone  (Figur e 5a). Diagenesis \\nincludes gypsification , dolomitization and dissolution. In penecontemporaneous  stage , gypsification  \\nwas weak, while dolomitization , and pe lletoid  were fully metasomatized by dolomite  (Figur e 5b). \\nAlthough the structure of spheroids was well preserved, they almost changed from limestone  to \\ndolomite stone . In meteoric environment , the residual limestone was dissolved and  pelletoid  with weak \\ndolomitization dissolved to form mold ic pores . The rock was completely transformed into dolomite \\n(Figur e 5c). In the later burial process , compaction has little influence on porosity. Dolomitization in \\nburied environment forms a few coarse euhedral  dolomite filling pores  (Figur e 5d), forming the t ype Ⅲ \\nof dolomite as show in cast thin sections  (Figur e 2c). \\nFigure 5 The forming process of typ e Ⅲ dolomite . (a) pelletoid limestone; (b)  weak gypsification  \\nand intense dolomitization; (c ) dissolution in meteoric environment; (d) Dolomitization in burial \\nenvironment  \\nT\\nhe type Ⅳ. The original rock of dolomite of type  Ⅳ is pelletoid limestone  (Figur e 6a ). In \\npenecontemporaneous stage , the pe lletoid  underwent intense gypsification , and some intergranular \\npores were filled by  gypsum. And then, intense dolomitization occurred, and the pe lletoid  and gypsum \\nwere fully metasomatized by fine dolomite, in which the dolomites in the gypsum were discrete, and \\nthe dolomites in the pe lletoid  were in clusters (Figure 6b). In meteoric environment , the resid ual \\nlimestone was completely dissolved, and the pe lletoid  with low metasomatic degree of dolomite were \\nselectively dissolved to form mold ic pores, while the particles with high degree of dolomization were \\ndissolved to form intergranular pores (Figure 6c). As dissolution continues and burial depth increases, \\nthe pore was adjust ed, the cluster dolomite becomes loose, and the contour of the moldic pore  becomes \\nblurred. The distribution of the dolomite crystal is uniformized as a whole, the por osity d oes not ch ange, \\n\\nbut the connectivity of the inter crystalline  pore is significantly improved (Figure 6d), forming the t ype \\nⅣ of dolomite as show in cast thin sections  (Figur e 2d). \\nFigure 6 The forming process of type Ⅳ dolomite.  (a) pelletoid limestone; (b)  intense gypsification \\nand dolomiti zation ; (c) dissolution in meteoric environment; (d)  pore system ad justment  \\nDis\\ncussion  \\nThe \\nArab C Formation was in a tidal flat environment, in which facies include supratidal (Sab kha),  \\nintertidal and subtidal . Different facies have different water energy, lithology, exposure probability and \\ntime, as a result, the diagenesis such as gypsification, dolomitization, dissolution and cementation  in \\ndifferent area vary greatly. That's why there are four type of dolomite even the primitive particles are \\nthe same pelletoids.  The pellet oids were mainly in the intertidal facies, subtidal facies with relatively \\nhigh energy could also developed. I ntertidal facies was easy to exposure, and gypsification and \\ndolomitization was intense in penecontemporaneous stage . The type Ⅲ  and type Ⅳ probably deposited \\nin the intertidal facies. S ubtidal facies mainly under the water  where it is not easy to exposure . When \\nthe wave base decline, it would exposure for a short time . It's not only that the gypsification and \\ndolomitization have an important impact on the pelletoids , but also that dissolution and cementation  \\ntransform the pelletoids and pores.  The type Ⅰ, type Ⅱ, probably deposited in the intertidal facies.  \\n Co\\nnclusions  \\nTh\\ne pelletoids  limestone of Arab C  formation in BH oilfield is mainly developed in the intertidal and \\nsubtidal facies and has experienced intense diagenetic transformation intensity.  The most important \\ndiagenesis includes gyp sification, dolomitization, dissolution and cementation.  Due to the environment \\ndifference, response to sea level fluctuation and exposure time, the diagenesis, especially the extent of dolomitazition is different. As a result, the d olomite has  different c haracter in structure and crystal.  \\nAc\\nknowledgements  \\nThi\\ns paper was funded by China Petroleum Science and Technology Major Project \\\"Research on Key \\ntechnologies for effective development of ultra -low permeability carb onate reservoi r\\\", No.2021DJ3202 \\nand \\\" The Theory and Key Technology Research on Efficient Development of Abnormal High -Pressure, \\nSour, Light Oil Carbonate Reservoirs \\\", No.2022DJ3211.  \\nRe\\nferences  \\nMor\\nad S, Al-Aasm I , Nader F H. 2012. Impact of diagenesis on the spatial and temporal distribution of \\nreservoir quality in the Jurassic Arab D and C members, offshore Abu Dhabi oilfield, United Arab Emirates. GeoArabia, 17: 17 -56. \\nDiLoreto Z A, Bontognal i T R R, Al Disi Z A, et al. 2019. Microbial community composition and \\ndolomite formation in the hypersaline microbial mats of the  Khor Al -Adaid sabkhas, Qatar . \\nExtremophiles, 23: 201-218. \\nKendall C G, & Alsharhan A S. 2011. Coastal Holocene carbonates of Abu Dhabi, UAE: depositional \\nsetting, geomorphology, and role of cyanobacteria in micritization. Quaternary carbonate and evaporite \\nsedimentary facies and their ancient analogues. International Association of Sedimentologists Special Publication, 43, 205-220.\"}\n",
      "{\"text\": \"\\nCoupling Genetic algorithm and Random Forest for robust prediction of CO 2 storage efficiency \\nin underground formation s \\n \\nIntroduction  \\nSubsurface aquifers have long been acknowledged as the most significant storage areas worldwide. \\nAround 1000 -10000 Gt of CO 2 might be stored underground in saline deposits, which equals 99 percent \\nof the world's total storage capacity (IPCC, 2005; Zhang et al., 2020) . As a general rule, there are four \\nprimary ways in which CO 2 cannot escape from storage sites: s tructural, residual solubility, and mineral \\ntrappings (Al-khdheeawi et al., 2018) . These mechanisms have been different performances d ependent \\non saline aquifers and cap -rock properties (Sun et al., 2021) . It is observed that each of these processes \\nis engaged on various time frames. Nonethe less, the most significant strategy of assuring safe storage \\nefficiency is the trapping of residuals and solubility (Al-khdheeawi et al.,  2018) .  \\nSo far, several studies have applied the physics -based reservoir modeling approach for estimating the \\ntrapping efficiency at various storage sites around the globe. Deep saline aquifer CO2 storage \\nperformance may be estimated using a modeling t echnique if the available data is made accessible. \\nSubsurface data such as geological and geophysical must be acquired in order to employ the simulation \\nmethod (Vo Thanh et al., 2020; Vo Thanh and Lee, 2021) . Physical reservoir modeling, which \\nconsumes a lot of computer resources, is thus not always a viable choice for forecasting storage \\nperformance. As a res ult, running a single scenario through the full -field model takes an inordinate \\namount of time. Simulator work might last anything from a few days to several weeks (Ghassemzadeh \\net al., 2021; Jeong et al., 2018; Ma et al., 2020) . Due to this latter, the genetic algorithm is adopted for \\ncoupling with RF to  develop the robust the ML model for our research purpose . To the best of our \\nknowledge, no work has been completed using datasets from diversity reservoir characterization models \\nand GA -RF to forecast CO 2 trapping performance in saline aquifers. For start ers, the databank for this \\nresearch was compiled using various field observations and credible literature. In order to train the \\nrobust machine learning models from the databank, the GA is paired with the RF. A comparison with \\npast research will be used to  determine if the proposed ML model is better. After that, a quantitative \\nexamination of the features that affect CO 2 trapping performance will take place. In the end, the GA -\\nRF model will be used to estimate CO 2 storage in the Sleipner  field and compared to findings from \\ncommercial software reservoir simulations.  \\nMethod  \\nThe overview workflow constructing robust ML models to predict CO 2 trapping efficiency for saline \\nformations is depicted in  Figure 1.  Firstly, the data collection was co nducted to generate the inputs and \\noutputs for the databank. Then, RF and GA were used to build the robust ML models to evaluate residual \\ntrapping index (RTI) and solubility trapping index (STI) using selected variables in Table 1. Moreover, \\ntuning hyperpa rameter was conducted using the k -fold cross -validation (k -FCV) and genetic algorithm \\n(GA) process to optimize the robust ML models. Next, the superiority of robust ML models was \\ndemonstrated by comparing them with previous studies. Ultimately, the robust ML models have verified \\nthe accuracy and stability by using the 2019 Sleipner benchmark. Powerful ML models predicted the \\nCO 2 RTI and STI of Sleipner field. They were then compared with CO 2 trapping with commercial \\nsoftware  \\nResult and discussion  \\nFigure 2 shows the prediction performance of the GA -RF model for training and testing datasets. It is \\nexhibited that the data samples in the training RTI and STI set are close with the fitting line ( Figure 2a \\n& 2b). For the testing phase, the data samples are distri buted closely around the fitting line ( Figure 2c \\n& 2d), although the distribution presents several scatter points that are located not closely the fitting \\nline. These training and testing results prove that the GA -RF model can accurately reproduce the \\nsimulation CO 2 trapping data and has relative power and generalization capability to predict the blind \\ndata. As the excellent performance model that drives accurate evaluations of CO2 trapping efficiency \\n(RTI and STI) in deep underground saline aquifers, t he GA -RF model was further compared with \\nexisting ML study, namely Artificial Neural Network Geological CO 2 storage proposed by Kim et al. \\n(2017) . To ensure the fairness of the comparison with previous works, it should be recapped that we \\nonly  \\n \\n \\n\\nrecorded the RMSE and R2 in these works. The statistical indicator comparison between GA -RF and \\nANN models are denoted in Table 1 .  \\n \\nTable 1.  The c omparative of this work and previous model  \\nIn addition, Table 2  compares CO 2 trapping performance between our hybrid GA -RF model and \\nsimulation result of Sleipner storage site. This comparison reflected that that the prediction of the GA -\\nRF model is nearly perfect matching to the field simulation results for both RTI and STI. The field \\napplication demonstrated that the constructed GA -RF model can be adapted to provide the feasibility \\ninvestigation on CO 2 trapping efficiency in a large scale geological storage project as well as Sleipner \\nsite. Recap that it is essential to perform i nitial evaluation of potential trapping efficiency for CO 2 geo-\\nsequestration in underground saline formations. The GA -RF model can estimate the residual and \\nsolubility trapping efficiency with reasonable results by considering only nine variables, such as \\ninjection time, post injection period, injection rate, salinity, depth, residual gas saturation, permeability, \\nthickness, porosity. Thus, the proposed GA -RF based ML model can be utilized as a fast and robust \\noption to estimate the feasibility of geologica l CO 2 storage sites.  \\n \\nTable 2. The results are compared between commercial software and machine learning  \\n \\n \\n \\n \\n \\n \\n \\nConclusion  \\nThis study constructed a powerful machine learning technique, namely GA -RF model, to predict the \\nCO 2 trapping efficiency based on nine parameters: injection time, post injection period, injection rate, \\nsalinity, depth, residual gas saturation, permeability thickness, porosity. The reasonable and robustness \\nof the hybrid GA -RF model was checked by investi gating the statistical indicators. Also, the superiority \\nof the constructed model was pointed out by comparisons with previous studies. In addition, the \\nimportance of input features to the CO 2 trapping efficiency was explored using the PI. Moreover, the \\nGA-RF model was adapted for predicting CO 2 storage index in Sleipner sites with excellent agreement \\nbetween simulated and our ML model  \\n  Training set  Testing set  \\nSource  Model  R2 RMSE  R2 RMSE  \\nGA-RF 0.9977  0.0103  0.9929  0.0185  This work  \\nANN  0.9884  0.88 0.9888  1.26 Kim et al.2017  \\n RTI STI \\nTime  Simulated  ML predicted  Simulated  ML predicted  \\n20 0.3684  0.3856  0.2258  0.2565  \\n30 0.4038  0.3996  0.2764  0.2678  \\n40 0.4421  0.4277  0.2988  0.2898  \\n… .. …. …. ….. \\n1000  0.2027  0.2140  0.7302  0.7137  \\n \\n \\n\\nFigure 1. The entire workflow is proposed for this study  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nFigure 2. The graphical analysis is showing the correlation relationship between the predicted and \\nsimulated CO 2 trapping index using GA -RF model for training set ((a) & (b)) and testing set ((c)&(d)).  \\n \\n \\n\\n \\nReference  \\nAl-khdheeawi, E.A., Vialle, S., Barifcani, A., Sarmadivaleh, M., Iglauer, S., 2018. Impact of Injection \\nScenario on CO2 Leakage and CO2 Trapping Capacity in Homogeneous Reservoirs Model \\ndescription and initialization, in: Offshore Technology Conference A sia. Kuala Lumpur, \\nMalaysia.  \\nAltmann, A., Tolo, L., Sander, O., Lengauer, T., 2010. Permutation importance : a corrected feature \\nimportance measure. BIOINFORMATICS 26, 1340 –1347. \\nhttps://doi.org/10.1093/bioinformatics/btq134  \\nBreiman, L.E.O., 2001. Random Forests. Mach. Learn. 45, 5 –32. \\nGhassemzadeh, S., Perdomo, M.G., Haghigh, M., Abbasnejad, E., 2021. A data -driven reservoir \\nsimulation for natural gas reservoirs. Neural Comput. Appl. https://doi.org/10.1007/s00521 -021-\\n05886 -y \\nIPCC, 2005. IPCC special repo rt on carbon dioxide capture and storage. Cambridge University Press, \\nUK/New York, NY, USA.  \\nJeong, H., Sun, A.Y., Lee, J., Min, B., 2018. A learning -based data -driven forecast approach for \\npredicting future reservoir performance. Adv. Water Resour. 118, 95 –109. \\nhttps://doi.org/10.1016/j.advwatres.2018.05.015  \\nKim, Y., Jang, H., Kim, J., Lee, J., 2017. Prediction of storage efficiency on CO2 sequestration in \\ndeep saline aquifers using artificial neural network. Appl. Energy 185, 916 –928. \\nhttps://doi.org/10.10 16/j.apenergy.2016.10.012  \\nMa, S., Zhang, Y., Lv, J., Ge, Y., Yang, H., Li, L., 2020. Big data driven predictive production \\nplanning for energy -intensive manufacturing industries. Energy 211, 118320. \\nhttps://doi.org/10.1016/j.energy.2020.118320  \\nSong, Y., Su ng, W., Jang, Y., Jung, W., 2020. Application of an artificial neural network in predicting \\nthe effectiveness of trapping mechanisms on CO2 sequestration in saline aquifers. Int. J. Greenh. \\nGas Control 98, 103042. https://doi.org/10.1016/j.ijggc.2020.10304 2 \\nSun, X., Bi, Y., Guo, Y., Ghadiri, M., Mohammadinia, S., 2021. CO2 geo -sequestration modeling \\nstudy for contact angle estimation in ternary systems of brine , CO 2 , and mineral. J. Clean. \\nProd. 283. https://doi.org/10.1016/j.jclepro.2020.124662  \\nVo Thanh , H., Lee, K. -K., 2021. Application of machine learning to predict CO2 trapping \\nperformance in deep saline aquifers. Energy 122457. \\nhttps://doi.org/10.1016/j.energy.2021.122457  \\nVo Thanh, H., Sugai, Y., Sasaki, K., 2020. Application of artificial neural net work for predicting the \\nperformance of CO2 enhanced oil recovery and storage in residual oil zones. Sci. Rep. 10, \\n18204. https://doi.org/10.1038/s41598 -020-73931 -2 \\nZhang, J., Feng, Q., Zhang, X., Shu, C., Wang, S., Wu, K., 2020. A Supervised Learning Appro ach \\nfor Accurate Modeling of CO 2 − Brine Interfacial Tension with Application in Identifying the \\nOptimum Sequestration Depth in Saline Aquifers. \\nhttps://doi.org/10.1021/acs.energyfuels.0c00846\"}\n",
      "{\"text\": \"\\nMissing Seismic Trace Estimation using G enerative Adversarial Network: Image -to-Image \\nTranslation Method  \\n \\nIntroduction  \\n \\nGood quality seismic data is one of the key ele ments that contributes to better subsurface \\ncharacterization over the course of a field's life cycle. However, seismic data are typically distributed \\nsparsely or are often incomplete due to several factors such as obstruction due to constructions and oil \\nand gas platforms, constrained seismic survey design due to economics (Fu et al., 2018) or wipeout \\nzones on seismic that is caused by shallow gas. Missing and incomplete seismic traces often poses \\nchallenges to multi -trace processing algorithms in particula r and degrades processing that will impact \\nseismic interpretation and reservoir characterizations.  \\n \\nWith the advancement of processing capacity in recent years, Artificial Intelligence (AI) has sparked \\nsignificant interest and has been extensively applied to  address numerous problems in the field of \\ngeosciences. We have seen accelerating efforts in deep learning application to the prediction of missing \\nseismic traces that has exceeded the traditional interpolation techniques. These deep learning methods \\ndirec tly employ seismic data for training and learns how to interpolate as compared to the conventional \\ninterpolation techniques (Li et al., 2022). Wang et al. applied one of the deep learning methods known \\nas Convolutional Autoencoder (CAE) (Wang et al., 2020)  for the missing traces prediction.  \\n \\nThe performance of CAE, on the other hand, deteriorates as the number of traces that are missing \\nincreases. We introduce Image -to-image translation (I2I translation), a novel appr oach that has been \\napplied in this study to address this problem. The I2I approach uses a Generative Adversarial Network \\n(GAN) as the backbone of the method. In contrast to CAE, I2I helps the model to make a prediction \\nand enable the model to assess how cl osely the prediction matches the actual data by utilizing a \\ndiscriminator inside the model.  \\n \\nImage -to-Image (I2I) Translation Method  \\n \\nI2I establishes a zero -sum game between the generator G model and the discriminator D model, using \\nGAN as the algorithm's foundation. The g enerator G model attempts to generate a prediction image, \\nwhile the discriminator D model attempts to differentiate between the prediction and actual data. This \\nmethod aims to locate a Nash equilibrium between the generator G and discriminator D models (Pa ng \\net al., 2022). This objective can be written in the mathematical expression below:  \\n \\nℒ𝑐𝐺𝐴𝑁 (𝐺,𝐷)=𝔼𝑥,𝑦[log𝐷 (𝑥,𝑦)]+𝔼𝑥,𝑧[log (1−𝐷 (𝑥,𝐺(𝑥,𝑧))] \\n \\nwhere the observed image x and random noise vector z learn to mapping output image y. Thus, G tries \\nto minimize this objective against an adversarial D that tries to maximize it (Isola et al., 2017).  \\n \\nThe I2I technique makes use of the U-Net architecture as the generator model in order to supply \\nadequate predic tion data for the purpose of deceiving the discriminator model  (Ronneberger et \\nal., 2015 ). U-Net is an architecture of a Convolutional Neural Network (CNN) that was built \\nexpressly for the purpose of making p redictions about each data point. The L1 or Mean \\nAbsolute Error loss function is what the U-Net makes use of when it comes time to evaluate \\nthe generator model. However, if you only apply the L1 loss,  the Generator will produce \\noutputs that are unclear and  will have problems with its ability to anticipate. As a result, it has \\nbeen suggested to use PatchGAN as a discriminator in order to provide an additional loss \\nfunction to the generator in order to improve the results of predictions  (Larsen et al., 2016 ).  \\n \\nPatchGAN  is utilized to ensure outcome of the Generator model may be evaluated at the scale \\nof patches as opposed to the complete seismic section as is done for the Discriminator. To \\n \\n \\n\\nevaluate each seismic section patch, the discriminator performs inspections. Conv olutionally \\nexecuting the PatchGAN throughout the seismic segment is done first, and then the resulting \\nbinary cross entropy  loss is averaged before being used to evaluate both the generator and \\ndiscriminator models. (see Figure 1).  \\n \\n \\nFigure 1  Image -to-Image Translation general workflow scheme.  \\n \\nDataset Preparation  \\n \\nThe dataset used for this study is 2D post -stack Penobscot seismic data, located in the Scotian Basin, to \\nthe north of Sable Island and offshore of Nova Scotia (Campbell et al., 2015). The seismic dataset is \\nconditioned by slicing up into a patch with dime nsions of 256 pixels by 256 pixels so that the network \\ncan easily identify the correlation between the input and output. This ensures a more effective training \\nprocess and will require fewer computational resources. Additionally, the seismic data needs to be \\nnormalized (data normalization) prior input into the model to ensure each input parameter or pixel has \\na data distribution that is consistent with itself. The process of normalizing the data involves taking the \\nmean and subtracting it from each data poi nt, then dividing the value that is obtained by the standard \\ndeviation. These speeds up the convergence process while the network is being trained.  \\n \\nThe generation of input and output for the training method of the model is the focus of the second phase. \\nAs shown in Figure 2, a portion of the original seismic traces is removed, and as an output, we \\nreconstruct the original seismic with the remaining seismic traces in their entirety. As a result, the I2I \\nmodel will learn the correlation and make an effort to e stimate the missing traces in order to bring them \\ninto line with the actual one. The model was made more reliable through the use of this research by \\nselecting every 25 increments of inline and crossline seismic data.  \\n \\n \\nInput (Trace Removal)Sample number\\nSample number\\nTrace number Trace numberOutput (Original Data)\\n \\n \\n\\nFigure 2 The step of generating input and output data for I2I model training process. Input data is \\nobtained by removing several seismic traces form original seismic data, while the output is the original \\nfull seismic data.  \\n \\nResult and Discussion  \\n \\nThe predicted results for the part with its traces removed can be observed in the right figure of Figure \\n3, which  is juxtaposed with the original seismic data on the left side of Figure 3. The outcomes of the \\npredicte d seismic section appear to be geological, and they have a high degree of similarity to the \\nprimary data in  general.  \\n \\n \\nFigure 3 Comparison between the original and predicted seismic data I2I mode within the red box \\nsection. Note that the fault is preserved,  and migration artifact is excluded in prediction result.  \\n \\nIt is interesting to note that geological  elements have been preserved quite well, such as the overall \\ndipping  angles of the reflectors and the significant fault that was present in the original data, and that \\nthese aspects  have been recreated by the forecast. In addition to this, it has been not ed that artifacts such \\nas migration  smiles are not included in the prediction. Figure 4 shows the amplitude spectrum of original \\nand predicted  that is superimposed to observe the consistency, and the cross -correlation coefficient \\nbetween the amplitude  of the signal.  \\n \\n \\nFigure 4  The amplitude spectrum analysis (left) and cross -correlation coefficient b etween original \\nseismic data and seismic trace (right). This analysis shows that the amplitude spectrum is highly \\npreserved, and the cross -correlation achieved overall higher than 0.8 cross -correlation coefficient.  \\nMigrationartifact\\nisexcludedInput PredictedSample number\\nSample numberFaultispreserved\\nOriginalSample number\\nTrace number\\n\\n \\n \\n\\n \\nIt can be observed from the amplitude spe ctrum plot that the projected seismic results show very good \\nconsistency to the actual data across the missing traces area. This indicates that the I2I translation \\napproach preserves the amplitude of the signal. The cross -correlation between the anticipate d and \\noriginal seismic data shows that average values over the deleted portion are more than 0.8. Based on \\nthe amplitude spectrum and the cross correlation coefficient between original seismic and the seismic \\ntrace, we can validate that the prediction is a cceptable and reliable for this dataset.  \\n \\nConc lusions \\n \\nIn this study, we introduced Image -to-Image (I2I) translation method that can be successfully applied \\nfor the prediction of missing seismic traces in post -stack 2D seismic data. With a cross -correlation \\ncoefficient t hat is larger than 0.8 and an amplitude spectrum that is substantially conserved, the I2I \\nmodel can accurately estimate the missing trace on the Penobscot seismic data with approximately 25 \\nminutes of training time or 1500 epochs of data to use as a basis.  In addition, the findings of the forecast \\nkept the fault delineation while removing the artifact impact that was present in the initial data. This \\nwas achieved by maintaining the fault delineation. Furthermore, the pre -trained model can be used to \\ninterpo late seismic data with similar geomorphological structures through transfer learning , a machine \\nlearning approach that allows to reapply the knowledge from the model to a different dataset . We \\nconclude that I2I translation approach is applicable to the recovery of seismic data with missing traces \\nand further studies are being carried out to test the applicat ion on 3D seismic volumes.   \\n \\nReferences  \\n \\nCampbell, T. J., Richards, F. W. B., Silva, R. L., Wach, G., and Eliuk, L. [2015]. Interpretation of the \\nPenobscot 3D seismic volume using constrained sparse spike inversion, Sable sub -Basin, offshore \\nNova Scotia. Marine and Petroleum Geology , 68. https:// doi.org/10.1016/j.marpetgeo.2015.08.009  \\n \\nFu, L., Zhang, M., Liu, Z., and Li, H. [2018]. Reconstruction of seismic data with missing traces using \\nnormalized Gaussian weighted filter. Journal of Geophysics and Engineering , 15(5). \\nhttps://doi.org/10.1088/1742 -2140/aac31c  \\n \\nIsola, P., Zhu, J. Y., Zhou, T., and Efros, A. A. [2017]. Image -to-image translation with conditional \\nadversarial networks. Proceedings - 30th IEEE Conference on Computer Vision and Pattern \\nRecognition, CVPR 2017 , 2017 -January . https://doi.or g/10.1109/CVPR.2017.632  \\n \\nLarsen, A. B. L., Sønderby, S. K., Larochelle, H., and Winther, O. [2016]. Autoencoding beyond \\npixels using a learned similarity metric. 33rd International Conference on Machine Learning, ICML \\n2016 , 4. \\n \\nLi, X., Wu, B., Zhu, X., and Yang, H. [2022]. Consecutively Missing Seismic Data Interpolation \\nBased on Coordinate Attention Unet. IEEE Geoscience and Remote Sensing Letters , 19. \\nhttps://doi.org/10.1109/LGRS.2021.3128511  \\n \\nPang, Y., Lin, J., Qin, T., and Ch en, Z. [2022]. Image -to-Image Translation: Methods and \\nApplications. IEEE Transactions on Multimedia , Vol. 24. \\nhttps://doi.org/10.1109/TMM.2021.3109419  \\n \\nRonneberger, O., Fischer, P., and Brox, T. [2015]. Unet. MICCAI2015 . https://doi.org/10.1007/978 -3-\\n319-24574 -4_28  \\n \\nWang, Y., Wang, B., Tu, N., and Geng, J. [2020]. Seismic trace interpolation for irregularly spatial \\nsampled data using convolutional autoencoder. Geophysics , 85(2). https://doi.org/10.1190/geo2018 -\\n0699.1\"}\n"
     ]
    }
   ],
   "source": [
    "!head /workspace/data/clean/documents.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing data for continuous pretraining\n",
    "Creating `*.idx` and `*.bin` files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-06-03 11:20:47 tokenizer_utils:185] Getting SentencePiece with model: /workspace/models/Llama-2-7b-chat-hf/tokenizer.model\n",
      "Vocab size: 32000\n",
      "Output prefix: /workspace/data/clean/hackathon\n",
      "Time to startup: 0.033612966537475586\n",
      "Processing file /workspace/data/clean/documents.jsonl 1/1\n",
      "[NeMo I 2024-06-03 11:20:47 tokenizer_utils:185] Getting SentencePiece with model: /workspace/models/Llama-2-7b-chat-hf/tokenizer.model\n",
      "Processed 100 documents (74.32878124697075 docs/s, 0.8518148535852177 MB/s).\n",
      "Processed 200 documents (75.22731791273134 docs/s, 0.8709619475826994 MB/s).\n",
      "Processed 300 documents (75.90809837632872 docs/s, 0.8812034476742628 MB/s).\n",
      "Processed 400 documents (77.02688587987312 docs/s, 0.885677219310585 MB/s).\n",
      "Processed 500 documents (77.18279159749733 docs/s, 0.885471173599083 MB/s).\n",
      "Processed 600 documents (77.9499979758134 docs/s, 0.8895413917972526 MB/s).\n",
      "Processed 700 documents (77.33576908335414 docs/s, 0.8886260467578556 MB/s).\n",
      "Processed 800 documents (77.67416083575895 docs/s, 0.8908259209833347 MB/s).\n"
     ]
    }
   ],
   "source": [
    "!python /opt/NeMo/scripts/nlp_language_modeling/preprocess_data_for_megatron.py \\\n",
    "    --input /workspace/data/clean/documents.jsonl \\\n",
    "    --json-keys text \\\n",
    "    --tokenizer-library sentencepiece \\\n",
    "    --tokenizer-model /workspace/models/Llama-2-7b-chat-hf/tokenizer.model \\\n",
    "    --output-prefix /workspace/data/clean/hackathon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 17M\n",
      "-rw-r--r-- 1 0 0  11M Jun  3 11:09 documents.jsonl\n",
      "-rw-r--r-- 1 0 0 6.2M Jun  3 11:20 hackathon_text_document.bin\n",
      "-rw-r--r-- 1 0 0  18K Jun  3 11:20 hackathon_text_document.idx\n"
     ]
    }
   ],
   "source": [
    "!ls -nh /workspace/data/clean/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing data for LLM tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split text into overlapping chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5551 chunks extracted out of 879 pdf documents\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=3000, chunk_overlap=1500)\n",
    "\n",
    "with open(\"/workspace/data/clean/documents.jsonl\", \"r\") as f:\n",
    "    documents = [json.loads(line)[\"text\"] for line in f.readlines()]\n",
    "document_chunks = [text_splitter.split_text(document) for document in documents]\n",
    "document_chunks_flat = [chunk for chunks in document_chunks for chunk in chunks]\n",
    "print(f'{len(document_chunks_flat)} chunks extracted out of {len(documents)} pdf documents')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define LLM and prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/langchain_nvidia_ai_endpoints/_statics.py:313: UserWarning: Model ai-llama3-70b is deprecated. Using meta/llama3-70b-instruct instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "INSTRUCTION_PROMPT = \"\"\"Given the paragraph after <INPUT_START> tag, create a very good geoscience-related question and answer pair. Your output should be in a .json format containing the following fields: ['question', 'answer']\n",
    "Restrict the question to the context information provided. The questions should use information from passage, but should not refer to the originating text implicitly (you can not use 'according to', 'based on', and similar).\n",
    "Respond only with .json output, add no other comments. If generating a good question and answer pair is not possible, output <skip> instead.\n",
    "<INPUT_START>\"\"\"\n",
    "# CHUNKS_TO_PROCESS = 10\n",
    "CHUNKS_TO_PROCESS = None # means all\n",
    "\n",
    "llm = ChatNVIDIA(\n",
    "    model=\"ai-llama3-70b\",\n",
    "    temperature=0.2,\n",
    "    max_tokens=256\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submit batched requests to the LLM\n",
    "\n",
    "**WARNING! It will take ±30 min to generate QA paris**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# qa_pairs = await llm.abatch(['\\n'.join([INSTRUCTION_PROMPT, chunk]) for chunk in document_chunks_flat[:CHUNKS_TO_PROCESS]], \n",
    "#                             config={\"max_concurrency\": 10})\n",
    "# qa_pairs = [qa_pair.content for qa_pair in qa_pairs if qa_pair.content != \"<skip>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('{\\n'\n",
      " '\"question\": \"What type of climate is characterized by large amounts of '\n",
      " 'precipitation and a large influence of snow wind transport on avalanche '\n",
      " 'risk?\",\\n'\n",
      " '\"answer\": \"Coastal climate\"\\n'\n",
      " '}')\n"
     ]
    }
   ],
   "source": [
    "pprint(qa_pairs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse generated QA pairs in tovalid document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: [    {        \"input\": \"What is the primary goal of the SEIS experiment on the InSight mission?\",        \"output\": \"To study the internal structure of Mars\"    }]\n",
      "WARNING: [    {        \"input\": \"What is the primary objective of diffraction modelling in the context of diffraction imaging?\",        \"output\": \"To validate diffraction imaging at target locations and to obtain a better understanding of the diffraction response in relation to interpretation impact.\"    }]\n",
      "WARNING: [    {        \"input\": \"What is a significant limitation of using (Ultra-) High-Resolution streamers in high-frequency seismic surveys?\",        \"output\": \"The positioning accuracy of the source and receivers in the lateral and vertical planes.\"    }]\n",
      "WARNING: [    {        \"input\": \"What is the primary difference in terms of accuracy between satellite-based and fixed-point-based global positioning methods?\",        \"output\": \"Satellite-based methods provide good coverage around the globe but may suffer from lower accuracy, while fixed-point-based systems provide centimeter-level accuracy but require near-by reference stations.\"    }]\n",
      "WARNING: [    {        \"input\": \"What is the primary goal of FOSs in the context of seismic surveys?\",        \"output\": \"To measure the strain caused by the bending of a fiber cable and reduce/eliminate the effects of external wave fields and environmental noise.\"    }]\n",
      "WARNING: [    {        \"input\": \"What is the primary function of the core in a fiber optic strain sensor?\",        \"output\": \"The core acts as a sensor and an interrogator unit, which emits and records light.\"    }]\n",
      "WARNING: [    {        \"input\": \"What is the principle behind the detection of a specific gauge along a fiber in distributed fiber optic sensing systems?\",        \"output\": \"The principle is based on the backscattered light presenting a shift in wavelength, which can be detected and attributed to a specific gauge along the fiber.\"    }]\n",
      "WARNING: [    {        \"input\": \"What is the relationship between strain and curvature in shape sensing?\",        \"output\": \"The strain ε at a given point can be directly related to the curvature 𝛫 through the distance between the core and the center of the fiber and the angle between the core and the neutral plane.\"    }]\n",
      "WARNING: [    {        \"input\": \"What is the maximum shape reconstruction error achieved with a simple tricore sensor for a large bending radius?\",        \"output\": \"8 mm over 2 m\"    }]\n",
      "WARNING: [    {        \"input\": \"What is the primary role-playing clay mineral in the fine migration damage mechanism?\",        \"output\": \"Kaolinite\"    }]\n",
      "WARNING: [    {        \"input\": \"What type of neural network is used for automatic salt-body classification in seismic data?\",        \"output\": \"Deep convolutional neural network\"    }]\n",
      "WARNING: [    {        \"input\": \"What is the primary goal of the CNN-based seismic inversion method?\",        \"output\": \"To predict seismic attributes from seismic data\"    }]\n",
      "WARNING: [    {        \"input\": \"What is the primary application of CNN-based seismic impedance inversion methods?\",        \"output\": \"Seismic structural interpretation\"    }]\n",
      "WARNING: [    {        \"input\": \"What is the relationship between the dip angle and the predicted fracture density value when the density and azimuth are the same?\",        \"output\": \"The greater the dip angle, the higher the predicted fracture density value.\"    }]\n",
      "WARNING: [    {        \"input\": \"What is the range of CO2 storage efficiency in an open system condition?\",        \"output\": \"0.55% to 1.67%\"    }]\n",
      "WARNING: [    {        \"input\": \"What is the purpose of introducing auxiliary variables in the frequency domain, as described in the given equations?\",        \"output\": \"The purpose of introducing auxiliary variables is to transform the frequency-domain equation into a time-domain equation, making it possible to derive the time-domain expression of the equation.\"    }]\n",
      "WARNING: [    {        \"input\": \"What is the purpose of transforming auxiliary variables from frequency to time domain in the context of the PML scheme?\",        \"output\": \"To enable the calculation of coefficients just once and then use them during iterations over time steps, leading to a speed up of computations.\"    }]\n",
      "WARNING: [    {        \"input\": \"At what depth might the production of hot geothermal fluid occur in the granodiorite intrusion?\",        \"output\": \"2 km\"    }]\n",
      "WARNING: [    {        \"input\": \"What is a key advantage of the TPMR method for automated shut-in transient identification?\",        \"output\": \"It does not require data pre-processing and manual threshold selection.\"    }]\n",
      "WARNING: [    {        \"input\": \"What is the effect of far-field strain on crack propagation when the inner layer thickness is 1m and 15m?\",        \"output\": \"The far-field strain has a significant impact on crack propagation, with different constant strains and linearly increasing strains over time affecting the crack length and time to failure.\"    }]\n",
      "WARNING: [    {        \"input\": \"What is the primary cause of induced seismicity in the Ruhr area, Germany?\",        \"output\": \"Coal mining\"    }]\n",
      "WARNING: [    {        \"input\": \"What happens to the average bubble size and bubble count when the foam quality of CO2 foam is increased from 50% to 80%?\",        \"output\": \"The average bubble size is reduced and the bubble count is increased.\"    }]\n",
      "WARNING: [    {        \"input\": \"What is the minimum elastic computational depth during the wavefield backward propagation denoted by?\",        \"output\": \"M- = 1/0ηη\"    }]\n",
      "WARNING: [    {        \"input\": \"What is the purpose of in-situ carbon dioxide generation in oil recovery?\",        \"output\": \"To increase oil recovery\"    }]\n",
      "WARNING: [    {        \"input\": \"What is the primary goal of deghosting in marine seismic imaging?\",        \"output\": \"To recover the broadband signal by removing the source and/or receiver ghost wavefields from the data.\"    }]\n",
      "WARNING: [    {        \"input\": \"What is the relationship between pressure data P and vertical velocity data Vz in the f-p domain?\",        \"output\": \"P = ρwvwVz / (1 - px2vw2 - py2vw2)\"    }]\n",
      "WARNING: [    {        \"input\": \"What is the primary advantage of firing a seismic source multiple times in the same location in a full-scale setup?\",        \"output\": \"To increase the signal-to-noise ratio for each spot.\"    }]\n",
      "WARNING: [    {        \"input\": \"What is the primary application of the predicted acoustic wave curve in the context of drilling a horizontal well?\",        \"output\": \"To obtain the time-depth relationship of the well.\"    }]\n",
      "WARNING: [    {        \"input\": \"What is the primary purpose of fluid substitution in seismic rock physics analysis?\",        \"output\": \"To provide a tool for fluid identification and quantification in the reservoir.\"    }]\n",
      "WARNING: [    {        \"input\": \"What is the relationship between the amplitude along the straight line Δt(ζ) at fixed x and h=0, and the square root of the absolute value of the determinant of the Hessian with respect to h and x/a?\",        \"output\": \"The amplitude is inverse proportional to the square root of the absolute value of the determinant of the Hessian.\"    }]\n",
      "WARNING: [    {        \"input\": \"What is the approximate depth at which the shape of the tunnel changes significantly?\",        \"output\": \"600 m\"    }]\n",
      "WARNING: [    {        \"input\": \"What is the velocity of c0?\",        \"output\": \"1600 m/s\"    }]\n",
      "WARNING: [    {        \"input\": \"What is the value of c4 in meters per second?\",        \"output\": \"2800 m/s\"    }]\n",
      "WARNING: [    {        \"input\": \"What is the depth range of the data points shown in the diagram?\",        \"output\": \"1000 m to 1200 m\"    }]\n",
      "WARNING: [    {        \"input\": \"What is the approximate depth at which the shape of the graph changes significantly?\",        \"output\": \"Around 600-760 m\"    }]\n",
      "WARNING: [    {        \"input\": \"What is the depth at which the tunnelling process is likely to occur?\",        \"output\": \"Between 200 m and 800 m\"    }]\n",
      "WARNING: [    {        \"input\": \"What type of fault is the result of tensile stress, according to the Anderson fault model?\",        \"output\": \"A normal fault\"    }]\n",
      "WARNING: [    {        \"input\": \"What is the expected duration for the supercritical stage of CO2 to occur after the first CO2 injection into the depleted sandstone gas reservoir?\",        \"output\": \"2-4 years\"    }]\n",
      "WARNING: [    {        \"input\": \"What type of changes can PS time-shifts detect with high sensitivity?\",        \"output\": \"Zones of pore pressure changes\"    }]\n",
      "WARNING: [    {        \"input\": \"What is the relationship between cumulative gas production and gas pressure gradient in a reservoir?\",        \"output\": \"The gas pressure gradient changes gradually with cumulative gas production, while the water pressure gradient remains relatively constant.\"    }]\n",
      "WARNING: [    {        \"input\": \"What is the primary goal of facies classification in seismic interpretation?\",        \"output\": \"To analyze and map a seismic section into different facies classes according to their geological similarities.\"    }]\n",
      "WARNING: [    {        \"input\": \"What is the primary purpose of gas injection in gas condensate reservoirs?\",        \"output\": \"To enhance condensate recovery.\"    }]\n",
      "WARNING: [    {        \"input\": \"What is the range of azimuthal resolution that can be achieved in constructing a three-dimensional slowness distribution?\",        \"output\": \"360 °/N, where N is the number of azimuthal receivers.\"    }]\n",
      "Done\n",
      "Failed\t0 / 5518\n",
      "Warnings\t43 / 5518\n"
     ]
    }
   ],
   "source": [
    "failed_count = 0\n",
    "warning_count = 0\n",
    "with open(\"/workspace/data/clean/documents_sft.jsonl\", \"w\") as f:\n",
    "    for qa_pair in qa_pairs:\n",
    "        # Checking if json is correct\n",
    "        try:\n",
    "            json.loads(qa_pair)\n",
    "        except json.JSONDecodeError:\n",
    "            print(f'Failed to read {qa_pair} as a valid JSON')\n",
    "            failed_count += 1\n",
    "            continue\n",
    "        jsonl_line = qa_pair.replace(\"\\n\", \"\").replace('\"question\":', '\"input\":').replace('\"answer\":', '\"output\":').strip()\n",
    "        json_line_obj = json.loads(jsonl_line)\n",
    "        if isinstance(json_line_obj, list):\n",
    "            print(f'WARNING: {jsonl_line}')\n",
    "            jsonl_line = json.dumps(json_line_obj[0])\n",
    "            warning_count += 1\n",
    "        f.write(jsonl_line + \"\\n\")\n",
    "\n",
    "print('Done')\n",
    "print(f'Failed\\t{failed_count} / {len(qa_pairs)}')\n",
    "print(f'Warnings\\t{warning_count} / {len(qa_pairs)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('{\"input\": \"What is the primary advantage of using the SPAFSM method over the '\n",
      " 'standard FSM method in complex media?\", \"output\": \"The accuracy of travel '\n",
      " 'time calculation is effectively improved.\"}')\n"
     ]
    }
   ],
   "source": [
    "pprint(jsonl_line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split dataset into train / val / test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "\n",
    "def read_and_split(fname: str, out_dir: str):\n",
    "    # Open the original file\n",
    "    with open(fname, 'r') as original_file:\n",
    "        lines = original_file.readlines()\n",
    "\n",
    "    # Calculate partition sizes\n",
    "    total_lines = len(lines)\n",
    "    test_size = int(total_lines * 0.1)\n",
    "    val_size = int(total_lines * 0.1)\n",
    "    # The rest goes to the train partition\n",
    "\n",
    "    print(f'There are {total_lines}--> {test_size}, {val_size}, {total_lines - test_size - val_size}')\n",
    "    print(f'Iterate over {len(lines)} lines in {fname}')\n",
    "\n",
    "    with open(os.path.join(out_dir, 'data_test.jsonl'), 'w') as test_file, \\\n",
    "         open(os.path.join(out_dir, 'data_val.jsonl'), 'w') as val_file, \\\n",
    "         open(os.path.join(out_dir, 'data_train.jsonl'), 'w') as train_file:\n",
    "\n",
    "        # Iterate over each line in the original file\n",
    "        for i, line in enumerate(lines):\n",
    "            # Parse JSON data (optional, if you need to manipulate the data)\n",
    "            json_data = json.loads(line)\n",
    "\n",
    "            # Convert JSON back to string (if manipulated) or use original line\n",
    "            # json_line = json.dumps(json_data) if 'manipulate' in locals() else line\n",
    "            # json_line = str(json.dumps(json_data))\n",
    "            json_line = line\n",
    "\n",
    "            # Write to appropriate file based on index\n",
    "            if i < test_size:\n",
    "                test_file.write(json_line)\n",
    "            elif i < test_size + val_size:\n",
    "                val_file.write(json_line)\n",
    "            else:\n",
    "                train_file.write(json_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_and_split('/workspace/data/clean/documents_sft.jsonl', '/workspace/data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"input\": \"What is a major challenge in exploration and production that can be addressed by improving the predictive power of subsurface models?\", \"output\": \"Significant risks associated with exploration and production\"}\n",
      "{\"input\": \"What is a key requirement for producing subsurface models with a higher predictive value in Stratigraphic Forward Models?\",\"output\": \"Calibration of SFMs, which involves defining initial parameters and conditions leading to plausible models.\"}\n",
      "{\"input\": \"What is a major challenge in automatically finding the best-fit subsurface model using forward models?\", \"output\": \"The complexity of typical input parameters, such as subsidence history, makes it difficult to test them in an automated way.\"}\n",
      "{\"input\": \"What are the two paths to achieve more predictive subsurface models?\",\"output\": \"Improving the way SFMs are formulated and calibrated, or through the combination of different numerical techniques (hybrid models)\"}\n",
      "{\"input\": \"What can improve the calibration workflow of Sedimentary Forward Models (SFMs) and ensure consistency with global and regional processes?\", \"output\": \"Using Paleo Earth Models\"}\n",
      "{\"input\": \"How can stratigraphic forward models help in exploring scenarios and measuring uncertainty in subsurface modeling?\", \"output\": \"By improving the way we explore scenarios and measure uncertainty, SFMs can help us be less dependent on data and modeller bias.\"}\n",
      "{\"input\": \"What is the purpose of integrating Earth Models and Forward Stratigraphic Simulations in petroleum exploration?\",\"output\": \"To predict petroleum system elements in frontier areas.\"}\n",
      "{\"input\": \"What is the primary application of forward modeling in sequence stratigraphy and diagenesis?\", \"output\": \"Rapid, cost-effective carbonate reservoir characterization\"}\n",
      "{\"input\": \"What type of fluids are non-Newtonian fluids with incompressible or weakly-compressible assumptions regarded as in the context of debris flow simulations?\",\"output\": \"Non-Newtonian fluids with incompressible or weakly-compressible assumptions are regarded as granular material in the context of debris flow simulations.\"}\n",
      "{\"input\": \"What is the physical property that the yielding stress must be exceeded before the flow starts in the Bingham model?\", \"output\": \"The flow\"}\n"
     ]
    }
   ],
   "source": [
    "!head /workspace/data/data_train.jsonl"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
